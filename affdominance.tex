%!TEX TS-program =  pdflatex 

%%!TEX TS-program =  arara 
%% arara: pdflatex
%% arara: bibtex

%Changes since the initial arXiv submission are marked:
%SinceArXiv:  

\documentclass{amsart}
\usepackage{array}
\newcolumntype{P}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{graphicx,verbatim, amsmath, amssymb, amsthm, amsfonts, epsfig, amsxtra,ifthen,mathtools,epstopdf,caption,enumerate,hhline,bbm,capt-of,longtable}	
\usepackage[bookmarks=true, bookmarksopen=false,%
    colorlinks=true,%
    linkcolor=darkblue,%
    citecolor=darkblue,%
    filecolor=darkblue,%
    menucolor=darkblue,%
%    linktoc=page,%
    linktoc=all,%
    urlcolor=darkblue
]{hyperref}
\usepackage[usenames]{xcolor}
\definecolor{darkblue}{cmyk}{1,0.3,0,0.1}  %blue
\usepackage[capitalize]{cleveref}
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widebar}{0}{mathx}{"73}
\epstopdfsetup{suffix=}
\DeclareGraphicsExtensions{.ps}
\DeclareGraphicsRule{.ps}{pdf}{.pdf}{`ps2pdf -dEPSCrop -dNOSAFER #1 \noexpand\OutputFile}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{thm}[proposition]{Theorem}
\newtheorem{conj}[proposition]{Conjecture}
\newtheorem{phen}{Phenomenon}

\renewcommand\thephen{\Roman{phen}}

\theoremstyle{definition}
\newtheorem{example}[proposition]{Example}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{question}[proposition]{Question}

\theoremstyle{remark}
\newtheorem{remark}[proposition]{Remark}
\newtheorem{problem}[proposition]{Problem}

\numberwithin{equation}{section}


% This is for setting off words we define in a separate typeface.
\newcommand{\newword}[1]{\textbf{\emph{#1}}}

\newcommand{\integers}{\mathbb Z}
\newcommand{\rationals}{\mathbb Q}
\newcommand{\naturals}{\mathbb N}
\newcommand{\reals}{\mathbb R}

\newcommand{\edge}{\,\,\rule[2.7pt]{20pt}{0.5pt}\,\,}

\newcommand{\ep}{\varepsilon}
\newcommand{\thet}{\vartheta}
\newcommand{\col}{\operatorname{col}}
\newcommand{\cut}{\operatorname{cut}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\op}{\operatorname{op}}
\newcommand{\JIrr}{\operatorname{JIrr}}
\newcommand{\ji}{\operatorname{ji}}
\newcommand{\Sh}{\operatorname{Sh}}
\newcommand{\Wall}{\operatorname{Wall}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\cw}{\operatorname{cw}}
\newcommand{\ccw}{\operatorname{ccw}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\vsgn}{\mathbf{sgn}}
\newcommand{\Seed}{\operatorname{Seed}}
%\newcommand{\Sh}{{\mathcal Sh}}
\newcommand{\posspan}{\!\!\tiny\begin{array}{c}\mathbf{pos}\\\mathbf{span}\end{array}\!\!}
%\newcommand{\posspan}{\underset{\text{span}}{\overset{\text{pos}}{}}}
%\newcommand{\posspan}{\mathbf{span_+}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Int}{\operatorname{Int}}
\newcommand{\Fix}{\operatorname{Fix}}
\newcommand{\Stab}{\operatorname{Stab}}
\newcommand{\geom}{{\operatorname{geom}}}
\newcommand{\mon}{{\operatorname{mon}}}
\newcommand{\Ray}{{\operatorname{Ray}}}
\newcommand{\Ram}{{\operatorname{Ram}}}
\newcommand{\uf}{{\operatorname{uf}}}
\newcommand{\fr}{{\operatorname{fr}}}
\newcommand{\Geom}{{\operatorname{\textbf{Geom}}}}
\newcommand{\gFan}{\g\!\operatorname{Fan}}
\newcommand{\Cg}{\mbox{{\rm Cg}}}
\newcommand{\Con}{\mbox{{\rm Con}}}
\newcommand{\Irr}{\mbox{{\rm Irr}}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\fs}{\mathrm{fs}}
\newcommand{\ufs}{\mathrm{ufs}}
\newcommand{\covers}{{\,\,\,\cdot\!\!\!\! >\,\,}}
\newcommand{\covered}{{\,\,<\!\!\!\!\cdot\,\,\,}}
\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace}}
\newcommand{\pidown}{\pi_\downarrow}
\newcommand{\piup}{\pi^\uparrow}
\newcommand{\br}[1]{{\langle #1 \rangle}}
\newcommand{\brr}[1]{{\bigl\langle #1 \bigr\rangle}}
\newcommand{\brrr}[1]{{\Bigl\langle #1 \Bigr\rangle}}
\newcommand{\brrrr}[1]{{\biggl\langle #1 \biggr\rangle}}
\newcommand{\brrrrr}[1]{{\Biggl\langle #1 \Biggr\rangle}}
\newcommand{\A}{{\mathcal A}}
\newcommand{\I}{{\mathcal I}}
\newcommand{\GG}{{\mathbf G}}
\newcommand{\CC}{{\mathbf C}}
\newcommand{\EL}{{\mathcal L}}
\newcommand{\F}{{\mathcal F}}
\newcommand{\D}{{\mathfrak D}}
\newcommand{\N}{{\mathcal N}}
\newcommand{\p}{{\mathfrak p}}
\newcommand{\X}{{\mathcal X}}
\newcommand{\W}{{\mathcal W}}
\newcommand{\join}{\vee}
\newcommand{\meet}{\wedge}
\renewcommand{\Join}{\bigvee}
\newcommand{\Meet}{\bigwedge}
\newcommand{\bigmeet}{\Meet}
\newcommand{\bigjoin}{\Join}
\newcommand{\leftq}[2]{\!\!\phantom{.}^{#1} {#2}}
\newcommand{\closeleftq}[2]{\!\!\phantom{.}^{#1}\! {#2}}
\newcommand{\Pge}{{\Phi_{\ge -1}}}
%\newcommand{\ck}{^\vee}
%\newcommand{\ck}{^{\scalebox{0.5}[0.5]{$\vee$}}}
\newcommand{\ck}{\spcheck}
\newcommand{\letw}{\le_{\mathrm{tw}}}
\newcommand{\Alg}{\mathrm{Alg}}
\newcommand{\toname}[1]{\overset{#1}{\longrightarrow}}
\newcommand{\dashname}[1]{\overset{#1}{\mbox{---\!---}}}
\newcommand{\st}{^\mathrm{st}}
\renewcommand{\th}{^\text{th}}
\newcommand{\nd}{^\text{nd}}
\newcommand{\rd}{^\text{rd}}
\newcommand{\0}{{\mathbf{0}}}
\newcommand{\Vol}{\mathrm{Vol}}
\newcommand{\lleq}{\le\!\!\!\le}
\newcommand{\notlleq}{\le\!\!\!\!\not\,\le}
\newcommand{\ggeq}{\ge\!\!\!\ge}
\newcommand{\Cone}{\mathrm{Cone}}
\newcommand{\Comp}{\mathrm{Comp}_C}
\newcommand{\Star}{\mathrm{Star}}
\newcommand{\Lin}{\mathrm{Lin}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Proj}{\mathrm{Proj}}
\newcommand{\relint}{\mathrm{relint}}
\newcommand{\Clust}{\mathrm{Clust}}
\newcommand{\into}{\hookrightarrow}
\newcommand{\equivalent}{\Longleftrightarrow}
\newcommand{\onto}{\twoheadrightarrow}
\newcommand{\isomorph}{\cong}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Asym}{A_{\mathrm{sym}}}
\newcommand{\Cox}{\mathrm{Cox}}
\newcommand{\Des}{\mathrm{Des}}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\inv}{inv}
\newcommand{\odd}{\mathrm{odd}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\newcommand{\m}{\mathbf{m}}
\renewcommand{\c}{\mathbf{c}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\k}{\mathbbm{k}}
\newcommand{\kk}{\mathbf{k}}
\renewcommand{\ll}{{\boldsymbol\ell}}
\newcommand{\ks}{\mathbf{k}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\renewcommand{\t}{\mathbf{t}}
\renewcommand{\v}{\mathbf{v}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\tB}{{\tilde{B}}}
\newcommand{\tM}{{\widetilde{M}}}
\newcommand{\tN}{{\tilde{N}}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\ZP}{\mathbb{ZP}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\BB}{\mathbf{B}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\U}{\mathcal{U}}
\renewcommand{\O}{\mathcal{O}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Rel}{\operatorname{Rel}}
\newcommand{\Trop}{\operatorname{Trop}}
\newcommand{\pr}{{\operatorname{pr}}}
\newcommand{\bB}{\widebar{B}}
\renewcommand{\S}{\mathbf{S}}
\newcommand{\Clear}{\operatorname{Clear}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\Scat}{\operatorname{Scat}}
\newcommand{\Fan}{\operatorname{Fan}}
\newcommand{\ScatFan}{\operatorname{ScatFan}}
\newcommand{\ClusFan}{\operatorname{ClusFan}}
\newcommand{\CambScat}{\operatorname{CambScat}}
\newcommand{\ChamberFan}{\operatorname{ChamberFan}}
\newcommand{\Nar}{\operatorname{Nar}}
\newcommand{\can}{\operatorname{can}}
\renewcommand{\mid}{\operatorname{mid}}
\newcommand{\re}{\mathrm{re}}
\newcommand{\im}{\mathrm{im}}
%\newcommand{\init}{\mathrm{in}}
\renewcommand{\d}{{\mathfrak d}}
\newcommand{\bfd}{{\mathbf d}}
%\newcommand{\f}{{\mathfrak f}}
\newcommand{\seg}[1]{\overline{#1}}
\newcommand{\hy}{\hat{y}}
\newcommand{\ab}{\uparrow}
\newcommand{\bel}{\downarrow}

\newcommand{\complexes}{\mathbb{C}}
\newcommand{\Up}{\Upsilon}
\newcommand{\tUp}{\widetilde\Upsilon}
\newcommand{\cm}[3]{(#1\Vert#2)_{#3}}
\newcommand{\Cm}[3]{\bigl(#1\big\Vert#2\bigr)_{#3}}
\newcommand{\CM}[3]{\Bigl(#1\Big\Vert#2\Bigr)_{#3}}
\newcommand{\cmrarrow}{{\small{\rightarrow}}}
\newcommand{\cmlarrow}{{\small{\leftarrow}}}
\newcommand{\cmcircarrow}{{\small{\circlearrowright}}}
\newcommand{\cmr}[3]{(#1\cmrarrow#2)_{#3}}
\newcommand{\Cmr}[3]{\bigl(#1\cmrarrow#2\bigr)_{#3}}
\newcommand{\CMr}[3]{\Bigl(#1\cmrarrow#2\Bigr)_{#3}}
\newcommand{\cml}[3]{(#1\cmlarrow#2)_{#3}}
\newcommand{\Cml}[3]{\bigl(#1\cmlarrow#2\bigr)_{#3}}
\newcommand{\CMl}[3]{\Bigl(#1\cmltarrow#2\Bigr)_{#3}}
\newcommand{\cmcirc}[3]{(#1\,\cmcircarrow\,#2)_{#3}}
\newcommand{\Cmcirc}[3]{\bigl(#1\,\cmcircarrow\,#2\bigr)_{#3}}
\newcommand{\CMcirc}[3]{\Bigl(#1\,\cmcircarrow\,#2\Bigr)_{#3}}
\newcommand{\intnum}[2]{(#1\,|\,#2)}
\newcommand{\Phire}{\Phi^{\operatorname{re}}}
\newcommand{\dist}{\operatorname{dist}}
\renewcommand{\c}{{\mathbf c}}
\newcommand{\dd}{{\mathbf d}}
\newcommand{\aff}{\mathrm{aff}}
\newcommand{\fin}{\mathrm{fin}}
\renewcommand{\th}{^\text{th}}
%\newcommand{\laff}{<}
%\newcommand{\gaff}{>}
\newcommand{\laff}{\triangleleft}
\newcommand{\gaff}{\triangleright}
\newcommand{\DF}{{\mathcal {DF}}}
\newcommand{\DCScat}{{\operatorname{DCScat}}}
\newcommand{\adj}[2]{\operatorname{adj}_{#1}(#2)}
\newcommand{\Lower}{\operatorname{Lower}}
\newcommand{\Upper}{\operatorname{Upper}}


% Notation mess
% the space of eigenvectors of c
\newcommand{\eigenspace}[1]{U^{#1}}
% the full root system
\newcommand{\RSChar}{\Phi}
\newcommand{\RS}{\RSChar}
\newcommand{\RSre}{\RS^\re}
\newcommand{\RSpos}{\RS^+}
\newcommand{\RSneg}{\RS^-}
\newcommand{\RSfin}{\RS_\fin}
\newcommand{\RSfinpos}{\RSfin^+}
\newcommand{\RSfinneg}{\RSfin^-}
% simples in \RS
\newcommand{\SimplesChar}{\Pi}
\newcommand{\Simples}{\SimplesChar}
\newcommand{\simple}{\alpha}
% the root subsystem in \eigenspace (T is for "tubes")
\newcommand{\RSTChar}{\Upsilon}
\newcommand{\RST}[1]{\RSTChar^{#1}}
\newcommand{\RSTfin}[1]{\RST{#1}_\fin}
% simples in \RST
\newcommand{\SimplesTChar}{\Xi}
\newcommand{\SimplesT}[1]{\SimplesTChar^{#1}}
\newcommand{\simpleT}{\beta}
% Supports
\newcommand{\Supp}{\operatorname{Supp}_\SimplesChar}
\newcommand{\SuppT}{\operatorname{Supp}_\SimplesTChar}
% traversals for \tau-orbits
\newcommand{\TravInfChar}{\Psi}
\newcommand{\TravInf}[1]{\TravInfChar^{#1}}
\newcommand{\proj}{\to}
\newcommand{\TravProj}[1]{\overrightarrow{\TravInfChar}^{#1}}
\newcommand{\inj}{\leftarrow}
\newcommand{\TravInj}[1]{\overleftarrow{\TravInfChar}^{#1}}
%\newcommand{\proj}{\medvertdot}
%\newcommand{\TravProj}[1]{\TravInfChar^{#1}_\proj}
%\newcommand{\inj}{\dotmedvert}
%\newcommand{\TravInj}[1]{\TravInfChar^{#1}_\inj}
\newcommand{\TravRegChar}{\Omega}
\newcommand{\TravReg}[1]{\TravRegChar^{#1}}
% Schur roots
\newcommand{\AP}[1]{\RS_{#1}}
\newcommand{\APre}[1]{\AP{#1}^\re}
%\newcommand{\APT}[1]{\RST{#1}_{#1}}           % THIS NEEDS A BETTER NOTATION possibly \Lambda_c
%\newcommand{\APTre}[1]{\RST{#1;\re}_{#1}}     % THIS NEEDS A BETTER NOTATION possibly \Lambda_c^\re
\newcommand{\APTChar}{\Lambda}
\newcommand{\APT}[1]{\APTChar_{#1}}      
\newcommand{\APTre}[1]{\APT{#1}^\re}     


\newcommand{\fakesubsec}[1]{\medskip\noindent\textbf{#1.}}  %unnumbered

%\allowdisplaybreaks

%  Uncomment the following to remove all figures (useful for checking how many pages are taken up by figures)
%\usepackage{comment}
%\excludecomment{figure}
%\let\endfigure\relax



\newcommand{\afftype}[1]{{\widetilde{\raisebox{0pt}[6pt][0pt]{#1}}}}




% Commands for marginal notes below
\usepackage[draft]{say}
\newcommand{\saySS}[1]{\say[S]{#1}}
\newcommand{\sayS}[1]{\say[S]{#1}}
\newcommand{\sayN}[1]{\say[N]{#1}}
\newcommand{\sayD}[1]{\say[D]{#1}}
\newcommand{\sayDR}[1]{\say[D]{#1}}
\newcommand{\margin}[1]{\say[N]{#1}}
% control the width of your comments
\addtolength{\marginparwidth}{3mm}

%  If you want to switch which margin you're using, do the command  \switchmargin before your marginal comment.
% But it won't let you switch which margin you use in the middle of a paragraph of the main text.
% Also, you can only switch if there is room on the other margin.  (I.e. if you switch too often, things may overlap
\makeatletter
\newcommand{\switchmargin}{
\if@reversemargin
\normalmarginpar
\else
\reversemarginpar
\fi
}
\makeatother

%\newcommand{\response}[2]{{\color{red}#1:--}#2{\color{red}--:#1}}
\newcommand{\response}[2]{ {\color{red}#1:}~#2}
\newcommand{\rn}[1]{\response{NR}{#1}}
\newcommand{\rs}[1]{\response{SS}{#1}}

\newcommand{\ok}[1]{ {\color{blue}#1: OK }}
\newcommand{\okn}{\ok{NR}}
\newcommand{\oks}{\ok{SS}}

% A quick way to get rid of the red text.
%\renewcommand{\textcolor}[2]{}


\author{Nathan Reading}
\author{Dylan Rupel}
\author{Salvatore Stella}
\title{Dominance Regions for Affine Cluster Algebras}
\address[N. Reading]{Department of Mathematics, North Carolina State University, Raleigh, NC, USA}
\address[D. Rupel]{NEED THIS}
\address[S. Stella]{NEED THIS}
%\keywords{}
\thanks{Nathan Reading was partially supported by the Simons Foundation under award number 581608 and by the National Science Foundation under award number DMS-2054489.
Dylan Rupel was partially supported by ????.  
Salvatore Stella was partially supported by ????.  }
%\received{}
%\revised{}
%\accepted{}

\allowdisplaybreaks


\begin{document}

\begin{abstract}
NEED THIS
\end{abstract}

\maketitle

\vspace{-8pt}

\setcounter{tocdepth}{2}
\tableofcontents


\section{Introduction}

...

The first part of this paper contains a short proof of the first key fact about dominance regions due to Fan Qin:
Under certain hypotheses, if $\lambda$ is the $\g$-vector of a cluster monomial with respect to an initial extended exchange matrix $\tB$, then the dominance region $\P^\tB_\lambda$ is $\set\lambda$.
(Qin's result is an immediate consequence of combining parts (1) and (3) of \cite[Theorem~1.2.1]{FanQin}.)
Our proof takes stronger hypotheses than Qin's, \sayN{UPDATE:  Some versions of our proof take *weaker* hypotheses, specifically in finite type, we don't need $B$ to be full rank to get the coefficient-free result!}
but is sufficient for the purposes of the paper, uses only the basic discrete geometry of matrix mutation, and is short and self-contained.
The first part of the paper also introduces much of the background that will be used later.

\section{Dominance regions}


\subsection{Definitions and background}\label{def sec}
We assume the basic definitions of exchange matrices and of matrix mutation.
Given a sequence $\kk=k_m\cdots k_1$ of indices in $\set{1,\ldots,n}$, we read the sequence from right to left for the purposes of matrix mutation.
That is, $\mu_\kk(B)$ means $\mu_{k_m}(\mu_{k_{m-1}}(\cdots(\mu_{k_1}(B))\cdots))$.
We write $\kk^{-1}$ for $k_1\cdots k_m$, the reverse of $\kk$.
Throughout, we will use without comment the fact that matrix mutation commutes with the maps $B\mapsto-B$ and $B\mapsto B^T$.

Given an exchange matrix $B$, the \newword{mutation map} $\eta^B_\kk:\reals^n\to\reals^n$ takes the input vector in $\reals^n$, places it as an additional row below $B$, mutates the resulting matrix according to the sequence $\kk$, and outputs the bottom row of the mutated matrix.
In this paper, it is convenient to think of vectors in $\reals^n$ as column vectors, and also, the mutation maps we need use transposes $B^T$ of exchange matrices.
Thus we write maps $\eta_\kk^{B^T}$.
This map takes a vector, places it as an additional \emph{column} to the right of $B$ (not $B^T$), does mutations according to $\kk$, and reads the rightmost column of the mutated matrix.

Given a vector $\lambda\in\reals^n$, define $\P^B_{\lambda,\kk}=\bigl(\eta_{\kk}^{B^T}\bigr)^{-1}\set{\eta_\kk^{B^T}(\lambda)+B_t\alpha:\alpha\in\reals^n,\alpha\ge0}$, where the symbol $\ge$ denotes componentwise comparison.
(Throughout the paper, we will define sets indexed by vectors $\alpha\in\reals^n$ with $\alpha\ge0$, or sometimes $\alpha\in\reals^m$ with $\alpha\ge0$.
When we can do so without confusion, we will omit the explicit statement that $\alpha\in\reals^n$ or $\alpha\in\reals^m$.)
Define the \newword{dominance region} of $\lambda$ with respect to $B$ to be $\P^B_\lambda=\bigcap_\kk\P^B_{\lambda,\kk}$, where the intersection is over all sequences~$\kk$.
\begin{lemma}\label{shift}
If $\lambda'=\eta^{B^T}_\kk(\lambda)$ and $B'=\mu_\kk(B)$, then 
\begin{enumerate}[\quad\bf1.]
\item \label{shift all}
$\eta^{B^T}_\kk\!\!(\P^B_\lambda)=\P^{B'}_{\lambda'}$.
\item \label{shift one}
$\eta^{B^T}_\kk\!\!(\P^B_{\lambda,\ll})=\P^{B'}_{\lambda',\ll\kk^{-1}}$ for any $\ll$.
\end{enumerate}
\end{lemma}
\begin{proof}
For any $\ll$,
\begin{align*}
\eta^{B^T}_\kk\!\!(\P^B_{\lambda,\ll})
&=\eta^{B^T}_\kk\!\!\left(\bigl(\eta_{\ll}^{B^T}\bigr)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\right)\\
%&=\left(\eta^{B^T}_\ll\!\!\left(\eta_{\kk}^{B^T}\right)^{-1}\right)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\\
&=\left(\eta^{B^T}_\ll\!\!\eta_{\kk^{-1}}^{\mu_\kk(B)^T}\right)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\\
%&=\left(\eta^{\mu_\kk(B)^T}_{\ll\kk^{-1}}\right)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\\
&=\left(\eta^{\mu_\kk(B)^T}_{\ll\kk^{-1}}\right)^{-1}\set{\eta^{\mu_\kk(B)^T}_{\ll\kk^{-1}}\left(\eta^{B^T}_\kk(\lambda)\right)+B_t\alpha:\alpha\ge0}\\
&=\P^{B'}_{\lambda',\ll\kk^{-1}}.
\end{align*}
Thus $\eta^{B^T}_\kk\!\!(\P^B_\lambda)=\bigcap_\ll\P^{B'}_{\lambda',\ll\kk^{-1}}=\P^{B'}_{\lambda'}$.
\end{proof}


For seeds $t_0$ and $t$ and an exchange matrix $B$, let $C_t^{B;t_0}$ be the matrix whose columns are the $C$-vectors at $t$ relative to the initial seed $t_0$ with exchange matrix~$B$.
Each column of $C_t^{B;t_0}$ is nonzero and all of its nonzero entries have the same sign.
(This is ``sign-coherence of $C$-vectors'', which was implicitly conjectured in \cite{FZ07} and proved as \cite[Corollary~5.5]{GHKK18}.)
Thus we will refer to the \newword{sign} of a column of $C_t^{B;t_0}$.
For $\kk=k_m\cdots k_1$, define seeds $t_1,\ldots,t_m$ by $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m$.
The sequence $\kk$ is a \newword{green sequence} for an exchange matrix $B$ if column $k_\ell$ of $C_{t_{\ell-1}}^{B;t_0}$ is \emph{positive} for all $\ell$ with $1\le\ell<m$.
A \newword{maximal green sequence} for $B$ is a green sequence that cannot be extended.
That is, the sequence $\kk$ is a maximal green sequence if every column of $C_{t_m}^{B;t_0}$ is \emph{negative}.
We will call $\kk$ a \newword{red sequence} for~$B$ if it is a green sequence for $-B$.
A \newword{maximal red sequence} is a red sequence that cannot be extended.
(A red sequence relates to antiprincipal coefficients: 
If we were to define the $C$-vectors recursively starting with the negative of the identity matrix, the requirement for a red sequence is that the $k_\ell$ column is negative at every step.)

Let $G_t^{B;t_0}$ be the matrix whose columns are the $\g$-vectors at $t$ relative to the initial seed $t_0$ with exchange matrix~$B$.
Let $\Cone^{B;t_0}_t$ be the nonnegative linear span of the columns of $G_t^{B;t_0}$.
For each $k\in\set{1,\ldots,n}$, the entries in the $k\th$ row of $G_t^{B;t_0}$ are not all zero and the nonzero entries have the same sign.
(This is ``sign-coherence of $\g$-vectors'', conjectured as \cite[Conjecture~6.13]{FZ07} and proved as \cite[Theorem 5.11]{GHKK18}.)
Thus all vectors in $\Cone^{B;t_0}_t$ all have weakly the same sign in the $k\th$ position.
The inverse of $G_t^{B;t_0}$ is $\bigl(C_t^{-B^T;t_0}\bigr)^T$.
(This is \cite[Theorem~1.2]{NZ12} or \cite[Theorem~1.1]{RS16} and \cite[Theorem~3.30]{RS16}.)
Thus $\Cone^{B;t_0}_t=\set{x\in\reals^n:x^TC_t^{-B^T;t_0}\ge0}$, where $0$ is a row vector and ``$\ge$'' means componentwise comparison. 

Given $\kk$ with $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m$, let $B_i$ be the exchange matrix at~$t_i$, so that in particular, $B_0=B$.
The map $\eta_{\kk}^{B^T}$ is ${\eta_{k_m}^{B_{m-1}^T}\circ\cdots\circ\eta_{k_2}^{B_1^T}\circ\eta_{k_1}^{B_0^T}}$.
The definition of each $\eta_{k_i}^{B_{i-1}^T}$ has two cases, separated by the hyperplane $x_{k_i}=0$.
Two vectors are in the same \newword{domain of definition} of $\eta_\kk^{B^T}$ if, at every step, the same case applies for the two vectors.
(Both cases apply on the hyperplane, so domains of definition are closed.)
In particular, $\eta_\kk^{B^T}$ is linear in each of its domains of definition, but the domains of linearity of $\eta_\kk^{B^T}$ can be larger than its domains of definition.

There is a fan $\F_{B^T}$ called the \newword{mutation fan} for $B^T$ \cite[Definition~5.12]{universal}.
We will not need the details of the definition, but roughly, the cones of $\F_{B^T}$ are the intersections of domains of definition of all mutation maps $\eta_\kk^{B^T}$, as $\kk$ varies.
Thus for each $\kk$, each cone of $\F_{B^T}$ is contained in a domain of definition of $\eta_\kk^{B^T}$, and the mutation map $\eta_\kk^{B^T}$ is linear on every cone of $\F_{B^T}$ \cite[Proposition~5.3]{universal}.
Every cone $\Cone^{B;t_0}_t$ is a maximal cone in the mutation fan $\F_{B^T}$ \cite[Proposition~8.13]{universal}.
Thus in particular, the mutation map $\eta_\kk^{B^T}$ is linear on every cone $\Cone^{B;t_0}_t$.
Furthermore, $\Cone_t^{B_m;t_m}=\eta_\kk^{B^T}\bigl(\Cone_t^{B;t_0}\bigr)$ for every seed~$t$.
(This amounts to the initial seed mutation formula for $\g$-vectors, conjectured as \cite[Conjecture~7.12]{FZ07} and shown in \cite[Proposition~4.2(v)]{NZ12} to follow from sign-coherence of $C$-vectors.
The restatement in terms of mutation maps is \cite[Conjecture~8.11]{universal}.)
A similar fact applies to the entire mutation fan:
For any sequence $\kk$, the map $\eta_\kk^{B^T}$ induces an isomorphism of fans from $\F_{B^T}$ to $\F_{\mu_\kk(B)^T}$.

\begin{remark}\label{conditional}
As written, \cite[Proposition~8.13]{universal} is conditional on ``sign-coherence of $C$-vectors'', which was a conjecture but is now a theorem \cite[Corollary~5.5]{GHKK18}.
\end{remark}

We will need to relate the cones $\Cone^{B;t_0}_t$ and $\Cone^{-B^T;t_0}_t$.
It is immediate from \cite[Proposition~7.5]{universal} and the skew-symmetry of $B$ that $-B^T$ is a \newword{rescaling} of $B$, meaning that there is a diagonal matrix $\Sigma$ with positive entries on the diagonal such that $-B^T=\Sigma^{-1}B\Sigma$.
Therefore, \cite[Proposition~8.20]{universal} says that the $i\th$ column of $G_t^{-B^T;t_0}$ is a positive scalar multiple of the $i\th$ column of $\Sigma G_t^{B;t_0}$.
(In the statement of \cite[Proposition~8.20]{universal}, $\Sigma$ is multiplied on the right, because there $\g$-vectors are row vectors rather than column vectors.)
Thus we have the following fact.
\begin{lemma}\label{B or -BT}
The $k\th$ entries of vectors in $\Cone^{B;t_0}_t$ have the same sign as the $k\th$ entries of vectors in $\Cone^{-B^T;t_0}_t$.
\end{lemma}
%Therefore, \cite[Proposition~8.20]{universal} implies the following lemma.
%(In the statement of \cite[Proposition~8.20]{universal}, $\Sigma$ is multiplied on the right, because there $\g$-vectors are row vectors rather than column vectors.)
%\begin{lemma}\label{SigmaG}
%For $i=1,\ldots,n$, the $i\th$ column of $G_t^{-B^T;t_0}$ is a positive scalar multiple of the $i\th$ column of $\Sigma G_t^{B;t_0}$.
%\end{lemma}
%The following lemma is an immediate consequence.
%\begin{lemma}\label{B or -BT}
%The $k\th$ entries of vectors in $\Cone^{B;t_0}_t$ have the same sign as the $k\th$ entries of vectors in $\Cone^{-B^T;t_0}_t$.
%\end{lemma}

For $k\in\set{1,\ldots,n}$, let $J_k$ be the $n\times n$ matrix that agrees with the identity matrix except that $J_k$ has $-1$ in position $kk$.
For an $n\times n$ matrix $M$ and $k\in\set{1,\ldots,n}$, let $M^{\bullet k}$ be the matrix that agrees with $M$ in column $k$ and has zeros everywhere outside of column $k$.
Let $M^{k\bullet}$ be the matrix that agrees with $M$ in row $k$ and has zeros everywhere outside of row $k$.

Given a real number $a$, let $[a]_+$ denote $\max(a,0)$.
Given a matrix $M=[m_{ij}]$, define $[M]_+$ to be the matrix whose $ij$-entry is $[m_{ij}]_+$.
Given an exchange matrix~$B$, an index $k\in\set{1,\ldots,n}$ and a sign $\ep\in\set{\pm1}$, define matrices
\begin{align*}
E_{\ep,k}^B&=J_k+[\ep B]_+^{\bullet k}\\
F_{\ep,k}^B&=J_k+[-\ep B]_+^{k\bullet}.
\end{align*}
Each matrix $E_{\ep,k}^B$ is its own inverse, and each $F_{\ep,k}^B$ is its own inverse.
We note that $(E_{\ep,k}^B)^T=J_k+[\ep B^T]_+^{k\bullet}=F_{\ep,k}^{-B^T}$.
The following is essentially a result of \cite{NZ12}, although it is not stated there in this form.  \margin{Do I have this attribution right?}
\begin{lemma}\label{EBF trick}
For $k\in\set{1,\ldots,n}$ and either choice of $\ep\in\set{\pm1}$, the mutation of $B$ at $k$ is $\mu_k(B)=E_{\ep,k}^BBF_{\ep,k}^B$.
\end{lemma}
\begin{proof}
We expand the product $(J_k+[\ep B]_+^{\bullet k})B(J_k+[-\ep B]_+^{k\bullet})$ to four terms.
The term $[\ep B]_+^{\bullet k}B[-\ep B]_+^{k\bullet}$ is zero because $b_{kk}=0$.
The term $[\ep B]_+^{\bullet k}BJ_k$ is $[\ep B]_+^{\bullet k}B^{k\bullet}J_k$, which equals $[\ep B]_+^{\bullet k}B^{k\bullet}$.
Similarly, the term $J_kB[-\ep B]_+^{k\bullet}$ equals $B^{\bullet k}[-\ep B]_+^{k\bullet}$
%\begin{align*}
%(J_k+[\ep B]_+^{\bullet k})B(J_k+[-\ep B]_+^{k\bullet})
%&=J_kBJ_k+J_kB[-\ep B]_+^{k\bullet}+[\ep B]_+^{\bullet k}BJ_k+[\ep B]_+^{\bullet k}B[-\ep B]_+^{k\bullet}.
%\end{align*}
Both 
Thus the $ij$-entry of $E_{\ep,k}^BBF_{\ep,k}^B$ is 
\[
\left\{\begin{aligned}
-b_{ij}&\quad\text{if }k\in\set{i,j}\\
b_{ij}&\quad\text{otherwise}
\end{aligned}\right\}
+
\left\{\begin{aligned}
|b_{ik}|b_{kj}&\quad\text{if }\sgn b_{ik}=\ep\\
0&\quad\text{otherwise}
\end{aligned}\right\}
+
\left\{\begin{aligned}
b_{ik}|b_{kj}|&\quad\text{if }\sgn b_{kj}=-\ep\\
0&\quad\text{otherwise}
\end{aligned}\right\}.
\]
This coincides with the $ij$-entry of $\mu_k(B)$.
\end{proof}

Given a matrix $M$, write $M_{\col(i)}$ for the $i\th$ column of $M$.
We observe that $(MN)_{\col i}=M(N)_{\col i}$.
\begin{lemma}\label{columns lem}
Suppose $B=[b_{ij}]$ is an exchange matrix, let $k\in\set{1,\ldots,n}$, and choose a sign $\ep\in\set{\pm1}$.
\begin{enumerate}[\quad\bf1.]
\item \label{col i}
$(E_{\ep,k}^BB)_{\col i}=J_k(B)_{\col i}+b_{ki}([\ep B]_+)_{\col k}$.
\item \label{col k}
$(E_{\ep,k}^BB)_{\col k}=(E_{-\ep,k}^BB)_{\col k}=B_{\col k}$.
\item \label{cols k}
$(E_{-\ep,k}^BB)_{\col i}=(E_{\ep,k}^BB)_{\col i}-\ep b_{ki}B_{\col k}$.
\end{enumerate}
\end{lemma}
\begin{proof}
The first two assertions follow immediately from the fact that $(MN)_{\col i}=M(N)_{\col i}$ and the fact that $b_{kk}=0$.
The first assertion (for $\ep$ and $-\ep$) implies that $(E_{-\ep,k}^BB)_{\col i}=(E_{\ep,k}^BB)_{\col i}-b_{ki}([\ep B]_+-[-\ep B]_+)_{\col k}$.  
The third assertion follows.
\end{proof}

We will also need the following simple fact about nonnegative linear spans.
Given a set $S$ of vectors, let $\posspan(S)$ denote the nonnegative linear span of $S$.
For $k\in\set{1,\ldots,n}$ and $\ep\in\set{\pm1}$, let $S_{k,\ep}$ be the set of vectors in $S$ whose $k\th$ entry has sign strictly agreeing with $\ep$.

\begin{lemma}\label{ps lemma}
Suppose $\lambda$ is a vector in $\reals^n$ whose $k\th$ $\lambda_k$ has $\ep\lambda_k\le0$.
Then %any vector in $\set{\lambda+\posspan(S)}\cap\set{x\in\reals^n:\sgn x_k=\ep}$ can be written as a vector in $\set{\lambda+\posspan(S)}\cap\set{x\in\reals^n:x_k=0}$ plus a vector in $posspan(S_{k,\ep})$.
\begin{multline*}
\set{\lambda+\posspan(S)}\cap\set{x\in\reals^n:\ep x_k\ge0}\\
=\set{\lambda+\posspan(S)}\cap\set{x\in\reals^n:x_k=0}+\posspan(S_{k,\ep}).
\end{multline*}
\end{lemma}
\begin{proof}
The set on the right side is certainly contained in the set on the right side.
If $x$ is an element of the left side, then $x$ is $\lambda$ plus a nonzero element $y$ of $\posspan(S_{k,\ep})$ plus an element $z$ of $\posspan(S\setminus S_{k,\ep})$.
Since the sign of $\ep x\ge0$ and $\ep\lambda\le0$, there exists~$t$ with $0\le t\le1$ such that $\lambda+ty+z$ has $k\th$ entry $0$.
We see that ${x=(\lambda+ty+z)+(1-t)y}$ is an element of the right side.
\end{proof}

\subsection{Linearizing the dominance region}\label{lin sec}
The difficult thing about computing the regions $\P^B_{\lambda,\kk}$ whose intersection is the dominance region $\P^B_\lambda$ is applying the piecewise linear map $\bigl(\eta^{B^T}_\kk\bigr)^{-1}$, because the number of its domains of linearity may grow without bound as the length of $\kk$ increases.
In this section, we will describe, in some cases, the cones (pointed at $\lambda$) that coincide with $\P^B_{\lambda,\kk}$ inside a domain of linearity of $\bigl(\eta^{B^T}_\kk\bigr)^{-1}$ containing~$\lambda$.
For convenience, we work here with square exchange matrices.
In \cref{ext sec}, we extend the result to tall extended exchange matrices.
%As an application, we prove in \cref{ext sec} that (when $B$ is replaced by an extended exchange matrix with linearly independent columns) that the dominance region of a point $\lambda$ in the $\g$-vector fan... hypotheses... never mind

Let $B_0$ be an exchange matrix.
For a sequence $\kk=k_m\cdots k_1$ of indices, define seeds $t_1,\ldots,t_m=t$ by $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
We will prove the following theorem.
%
%\noindent
%START ALTERNATIVE WORDING
%
%There may be more than one sequence connecting $t_0$ to $t$.
%
%\noindent
%NOT TRUE:\\
%The mutation map $\eta_\kk^{B_0^T}$ depends only on $t$, not on the choice of $\kk$.
%
%\noindent
%HOWEVER, we can rescue the following (by showing that different $\kk$s with the same $t$ are related by a global permutation of rows/coumns):\\
%AFTER MORE THOUGHT:  I think that's probably true, but there are some issues.
%It would come down to showing that there is only one automorphism of the mutation fan that fixes the positive cone pointwise, and that's probably true, but would take some thought, and it's not really necessary for what we're trying to accomplish this spring.
%(And actually, it's not true as I just stated it... Think about Markov.  
%But for piecewise linear automorphisms of the mutation fan with the pieces separated by codim-1 cones that contain codim-1 faces of the fan, it's probable true.
%That's the kind of complication we would need to deal with.
%
%Thus we define $\P^{B_0}_{\lambda,t}$ to be $\P^{B_0}_{\lambda,\kk}$ for any sequence $\kk=k_m\cdots k_1$ with $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
%Our first main result is about $\P^{B_0}_{\lambda,t}$ in the case where $\lambda$ is in $\Cone^{B_0;t_0}_t$.
%
%Our first main result is about $\P^{B_0}_{\lambda,\kk}$ in the case where $\lambda$ is in $\Cone^{B_0;t_0}_t$ for some sequence $\kk$.
%
%
%\begin{theorem}\label{P in B0C}
%Fix an exchange pattern with $B_0$ at $t_0$.
%For some vertex $t$, suppose there exists a red sequence for $B_t$ that ends at $t_0$.
%Then for $\lambda\in\Cone^{B_0;t_0}_t$,
%\[\P^{B_0}_{\lambda,t}\subseteq\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}.\]
%\end{theorem}
%
%\noindent
%END ALTERNATIVE WORDING
%

\begin{theorem}\label{P in B0C}
Suppose $\kk=k_m\cdots k_1$ and $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
If $\kk^{-1}=k_1\cdots k_m$ is a red sequence for $B_t$, then for any~$\lambda$ in the domain of definition of $\eta_\kk^{B_0^T}$ that contains $\Cone^{B_0;t_0}_t$,
\[\P^{B_0}_{\lambda,\kk}\subseteq\set{\lambda+G_t^{B_0;t_0}B_t\alpha:\alpha\in\reals^n,\alpha\ge0}=\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}.\]
\end{theorem}

Since $\bigl(\eta_{\kk}^{B_0^T}\bigr)^{-1}=\eta_{\kk^{-1}}^{B_t^T}$, we have $\P^{B_0}_{\lambda,\kk}=\eta_{\kk^{-1}}^{B_t^T}\set{\eta_\kk^{B_0^T}(\lambda)+B_t\alpha:\alpha\ge0}$.
Let $D$ be the domain of definition of $\eta_{\kk}^{B_0^T}$  that contains $\Cone^{B_0;t_0}_t$.
Then $\eta_{\kk^{-1}}^{B_t^T}$ is linear on $\eta_{\kk}^{B_0^T}(D)$.
Let~$\L_{\kk^{-1}}^{B_t^T}$ be the linear map that agrees with $\eta_{\kk^{-1}}^{B_t^T}$ on~$\eta_{\kk}^{B_0^T}(D)$.

\begin{proposition}\label{L mat}
The matrix for $\L_{\kk^{-1}}^{B_t^T}$, acting on column vectors, is $G_t^{B_0;t_0}$.
\end{proposition}
\begin{proof}
By \cite[Proposition~8.13]{universal}, $\Cone^{B_0;t_0}_t=\eta_{\kk^{-1}}^{B_t^T}\left(\left(\reals_{\ge0}\right)^n\right)$, and therefore also ${\eta_\kk^{B_0^T}\bigl(\Cone^{B_0;t_0}_t\bigr)=\left(\reals_{\ge0}\right)^n}$.
The proof of \cite[Proposition~8.13]{universal} shows not only an equality of cones, but also that $\eta_{\kk^{-1}}^{B_t^T}$ takes the extreme ray of $\left(\reals_{\ge0}\right)^n$ spanned by $e_i$ to the extreme ray of $\Cone^{B_0;t_0}_t$ spanned by the $i\th$ $\g$-vector at $t$ relative to $B_0;t_0$, where the total order on these $\g$-vectors at $t$ is obtained from the order $e_1,\ldots,e_n$ on $\g$-vectors at $t_0$ by the sequence $\kk$ of mutations.
\end{proof}

The following is a result of \cite{NZ12}.
It follows from the proof of \cite[Proposition~1.3]{NZ12}, or from \cite[(6.14)]{FZ07}, as explained in \cite[Remark~2.1]{NZ12}.

\begin{proposition}\label{GBBC}
$G_t^{B_0;t_0}B_t=B_0C_t^{B_0;t_0}$.
\end{proposition}

We can rewrite \cref{GBBC} as follows.

\begin{proposition}\label{BGCB}
$B_t\bigl(G_t^{-B_0^T;t_0}\bigr)^T=\bigl(C_t^{-B_0^T;t_0}\bigr)^TB_0$.
\end{proposition}
\begin{proof}
Starting with \cref{GBBC}, we use \cite[Theorem~1.2]{NZ} to rewrite $G_t^{B_0;t_0}$ as $\bigl(C_t^{-B_0^T;t_0}\bigr)^{-T}$ and $C_t^{B_0;t_0}$ as $\bigl(G_t^{-B_0^T;t_0}\bigr)^{-T}$, so $\bigl(C_t^{-B_0^T;t_0}\bigr)^{-T}B_t=B_0\bigl(G_t^{-B_0^T;t_0}\bigr)^{-T}$.
\end{proof}

Since $G_t^{B_0;t_0}$ is the matrix for~$\L_{\kk^{-1}}^{B_t^T}$ and since $\L_{\kk^{-1}}^{B_t^T}\eta_\kk^{B_0^T}(\lambda)=\lambda$, the following result is immediate from \cref{GBBC}.

\begin{proposition}\label{B0C}
\begin{align*}
\L_{\kk^{-1}}^{B_t^T}\set{\eta_\kk^{B_0^T}(\lambda)+B_t\alpha:\alpha\in\reals^n,\alpha\ge0}
&=\set{\lambda+G_t^{B_0;t_0}B_t\alpha:\alpha\in\reals^n,\alpha\ge0}\\
&=\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}.
\end{align*}
\end{proposition}

In light of \cref{B0C}, the conclusion of \cref{P in B0C} is equivalent to
\[\P^{B_0}_{\lambda,\kk}\subseteq\L_{\kk^{-1}}^{B_t^T}\set{\eta_\kk^{B_0^T}(\lambda)+B_t\alpha:\alpha\ge0}.\]

%% Don't really care about this any more.  Also, if we keep it, the meaning of $D$ has changed, so we would have to rephrase it.
%\cref{B0C} also immediately implies the following statement that is weaker than \cref{P in B0C}.
%
%\begin{proposition}\label{on domain}
%$\P^{B_0}_{\lambda,\kk}\cap D=\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}\cap D$.
%\end{proposition}


\begin{proof}[Proof of \cref{P in B0C}]
We will prove that $P^{B_0}_{\lambda,\kk}\subseteq\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}$, by induction on $m$ (the length of $\kk$).
The base case, where $\kk=\emptyset$, is true because $C_{t_0}^{B_0;t_0}$ is the identity matrix and $\P_{\lambda,\emptyset}=\set{\lambda+B_0\alpha:\alpha\ge0}$.
 
\cite[Proposition~1.4]{NZ12} says that $C_t^{B_0;t_0}=F^{B_1}_{\ep,k_1}C_t^{B_1;t_1}$, where $\ep$ is the sign of the $k_1$-column of $C_{t_1}^{-B_t;t}$.  
(The hypothesis that $\kk^{-1}$ is a red sequence for $B_t$ determines $\ep$, but we leave $\ep$ unspecified for now in order to highlight later where this hypothesis is relevant.)
By \cref{EBF trick} and because $E^{B_1}_{\ep,k_1}$ and $F^{B_1}_{\ep,k_1}$ are their own inverses,
\begin{equation}\label{ind B0C}\begin{aligned}
\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}
&=\set{\lambda+B_0F^{B_1}_{\ep,k_1}C_t^{B_1;t_1}\alpha:\alpha\ge0}\\
&=\set{\lambda+E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}\\
&=E^{B_1}_{\ep,k_1}\set{E^{B_1}_{\ep,k_1}\lambda+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.
\end{aligned}\end{equation}

The map $\eta_{\kk}^{B_0^T}$ is linear on $\Cone_t^{B_0;t_0}$.  
This map is $\eta_{\kk}^{B_0^T}={\eta_{k_m}^{B_{m-1}^T}\circ\cdots\circ\eta_{k_2}^{B_1^T}\circ\eta_{k_1}^{B_0^T}}$.
The map $\eta_{k_1}^{B_0^T}$ restricts to a linear map from $\Cone_t^{B_0;t_0}$ to $\Cone_t^{B_1;t_1}$.
The inverse of $\eta_{k_1}^{B_0^T}$ is $\eta_{k_1}^{B_1^T}$.
We claim that $E^{B_1}_{\ep,k_1}$ is the matrix for the linear map on column vectors that agrees with $\eta_{k_1}^{B_1^T}$ on $\Cone_t^{B_1;t_1}$.
Since $E^{B_1}_{\ep,k_1}$ is its own inverse, the claim is equivalent to saying that implies that $E^{B_1}_{\ep,k_1}$ is the linear map that agrees with $\eta_{k_1}^{B_0^T}$ on $\Cone_t^{B_0;t_0}$.

By \cite[(1.13)]{NZ12}, $\ep$ is the sign of the $k_1$-column of $\bigl(G_t^{-B_1^T;t_1}\bigr)^T$.
That is, $\ep$ is the sign of the $k_1$-row of $G_t^{-B_1^T;t_1}$, or in other words, the sign of the $k_1$-entry of vectors in $\Cone_t^{-B^T_1;t_1}$.
By \cref{B or -BT}, $\ep$ is the sign of the $k_1$-entry of vectors in $\Cone_t^{B_1;t_1}$, which is the sign that determines how $\eta_{k_1}^{B_1^T}$ acts on $\Cone_t^{B_1;t_1}$.
One easily checks that the action of $\eta_{k_1}^{B_1^T}$ on vectors whose $k_1$-entry has sign $\ep$ is precisely the action of $E^{B_1}_{\ep,k_1}$.
%It negates the $k_1$ entry.
%The $i\th$ ($i\neq k$) entry gets sent to itself plus the $k_1$ entry times $(B_1)_{ik}$ times $\ep$, if $(B_1)_{ik}$ also has sign $\ep$.

Let $\lambda'=\eta_{k_1}^{B_0^T}(\lambda)$, so that $\lambda'$ is in the same domain of definition of $\eta_{k_m\cdots k_2}^{B_1^T}$ as $\Cone_t^{B_1;t_1}$ and so that $\lambda'=E^{B_1}_{\ep,k_1}\lambda$.
By induction on $m$, 
\[\eta_{k_2\cdots k_m}^{B_t^T}\set{\eta_{k_m\cdots k_2}^{B_1^T}(\lambda')+B_t\alpha:\alpha\ge0}\subseteq\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.\]
Applying the homeomorphism $\eta_{k_1}^{B_1^T}$ to both sides, we obtain
\[\eta_{\kk^{-1}}^{B_t^T}\set{\eta_\kk^{B_0^T}(\lambda')+B_t\alpha:\alpha\ge0}\subseteq\eta_{k_1}^{B_1^T}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.\]
In light of \eqref{ind B0C}, we can complete the proof by showing that
\[\eta_{k_1}^{B_1^T}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}\subseteq E^{B_1}_{\ep,k_1}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.\]

We have seen that $E^{B_1}_{\ep,k_1}$ is the linear map that agrees with $\eta_{k_1}^{B_1^T}$ on the set $\set{x\in\reals^n:\sgn x_{k_1}=\ep}$.
We can similarly check that $E^{B_1}_{-\ep,k_1}$ is the linear map that agrees with $\eta_{k_1}^{B_1^T}$ on $\set{x\in\reals^n:\sgn x_{k_1}=-\ep}$.
Thus $\eta_{k_1}^{B_1^T}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}$ is
\[(U\cap\set{x\in\reals^n:\sgn x_{k_1}=-\ep})\cup(V\cap\set{x\in\reals^n:\sgn x_{k_1}=\ep}),\]
where 
{\small
\begin{align*}
U&=E^{B_1}_{\ep,k_1}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}=E^{B_1}_{\ep,k_1}\lambda'+\posspan\set{\left(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}}_{i=1}^n\\
V&=E^{B_1}_{-\ep,k_1}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}=E^{B_1}_{-\ep,k_1}\lambda'+\posspan\set{\left(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}}_{i=1}^n,
\end{align*}
}
where $\posspan$ denotes the nonnegative linear span of a set of vectors.

We need to show that $V\cap\set{x\in\reals^n:\sgn x_{k_1}=\ep}\subseteq U$.
Since $\eta_{k_1}^{B_1^T}$ is a homeomorphism, $U\cap\set{x\in\reals^n:x_{k_1}=0}=V\cap\set{x\in\reals^n:x_{k_1}=0}$.
By \cref{ps lemma}, any vector in $V\cap\set{x\in\reals^n:\sgn x_{k_1}=\ep}$ equals a vector in $V\cap\set{x\in\reals^n:x_{k_1}=0}$ plus a positive combination of vectors $\bigl(E^{B_1}_{-\ep,k_1}B_1C_t^{B_1;t_1}\bigr)_{\col i}$ whose $k_1$-entry has sign $\ep$.
Therefore, it suffices to show that every vector $\bigl(E^{B_1}_{-\ep,k_1}B_1C_t^{B_1;t_1}\bigr)_{\col i}$ whose $k_1$-entry has sign~$\ep$ is in $\posspan\set{\left(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}}_{i=1}^n$.

As a temporary shorthand, write $b_{ij}$ for the entries of $B_1$ and write $k$ for $k_1$.
Suppose $v_i=\bigl(E^{B_1}_{-\ep,k}B_1C_t^{B_1;t_1}\bigr)_{\col i}$ for some~$i$ and suppose the $k$-entry of $v_i$ has sign $\ep$.
Write $M$ for $E^{B_1}_{-\ep,k}B_1$ and write $N$ for $E^{B_1}_{\ep,k}B_1$.
\cref{columns lem}.\ref{col i} implies that $M_{kj}=-b_{kj}$ for all~$j$.
\cref{columns lem}.\ref{cols k} implies that if $\ep M_{kj}\ge0$, then $M_{\col j}=N_{\col j}+|b_{kj}|N_{\col k}$.
Similarly, if $\ep M_{kj}\le0$, then $M_{\col j}=N_{\col j}-|b_{kj}|N_{\col k}$.

Now $v_i=E^{B_1}_{-\ep,k}B_1\bigl(C_t^{B_1;t_1}\bigr)_{\col i}$, and $\bigl(C_t^{B_1;t_1}\bigr)_{\col i}$ has a sign $\delta\in\set{\pm1}$, meaning that it is not zero and all of its nonzero entries have sign $\delta$.
(This is ``sign-coherence of $C$-vectors''.  
See Remark~\ref{conditional}.)
Thus there are nonnegative numbers $\gamma_j$ such that $v_i=\delta\sum_{j=1}^n\gamma_jM_{\col j}$.
Write $\set{1,\ldots,n}=S\cup T$ with $S\cup T=\emptyset$ such that $\ep M_{kj}\ge0$ for all $j\in S$ and $\ep M_{kj}\le0$ for all $j\in T$.
Then
\begin{align*}
v_i
&=\delta\sum_{j\in S}\gamma_jM_{\col j}+\delta\sum_{j\in T}\gamma_jM_{\col j}\\
&=\delta\sum_{j\in S}\gamma_j(N_{\col j}+|b_{kj}|N_{\col k})+\delta\sum_{j\in T}\gamma_j(N_{\col j}-|b_{kj}|N_{\col k})\\
&=\delta\sum_{j=1}^n\gamma_jN_{\col j}-\delta\sum_{j=1}^n\ep\gamma_jb_{kj}N_{\col k}\\
&=N\bigl(C_t^{B_1;t_1}\bigr)_{\col i}+\delta\sum_{j=1}^n\ep\gamma_jM_{kj}N_{\col k}\\
&=N\bigl(C_t^{B_1;t_1}\bigr)_{\col i}+\sigma N_{\col k},
\end{align*}
where $\sigma=\ep\delta\sum_{j=1}^n\gamma_jM_{kj}$ is a positive scalar, because $\delta\sum_{j=1}^n\gamma_jM_{kj}$ is the $k$-entry of $v_i$, which has sign $\ep$.

As noted above, $\ep$ is the sign of the $k_1$-entry of vectors in $\Cone_t^{-B_1^T;t_1}$.
Since $\Cone^{-B_1^T;t_1}_t=\set{x\in\reals^n:x^TC_t^{B_1;t_1}\ge0}$, the rows of $\bigl(C_t^{B_1;t_1}\bigr)^{-1}$ span the extreme rays of $\Cone_t^{-B_1^T;t_1}$.
In particular $\bigl(C_t^{B_1;t_1}\bigr)^{-1}(\ep e_k)$ has nonnegative entries.
Thus $C_t^{B_1;t_1}\bigl(C_t^{B_1;t_1}\bigr)^{-1}(\ep e_k)=\ep e_k$ is a nonnegative linear combination of columns of~$C_t^{B_1;t_1}$.

Now, the hypothesis that $\kk^{-1}$ is a red sequence for $B_t$, or equivalently a green sequence for $-B_t$, says that $\ep=+1$, so that $e_k$ is a nonnegative linear combination of columns of~$C_t^{B_1;t_1}$.
Thus $N_{\col k}=Ne_k$ is a nonnegative linear combination of columns of~$NC_t^{B_1;t_1}$.
We have shown that $v_i=N\bigl(C_t^{B_1;t_1}\bigr)_{\col i}+\sigma N_{\col k}$ is a nonnegative linear combination of columns of~$NC_t^{B_1;t_1}$.
In other words, $v_i$ is in $\posspan\set{\left(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}}_{i=1}^n$, as desired.
\end{proof}

\subsection{Extended exchange matrices}\label{ext sec}
We follow \cite{FZ07} in considering $m\times n$ extended exchange matrices $\tB$ that are ``tall'', in the sense that $m\ge n$.
We will also consider $m\times m$ matrices related to $\tB$:
Writing $\tB$ in block form $\begin{bsmallmatrix}B\\E\end{bsmallmatrix}$, let $\BB$ be the matrix with block form $\begin{bsmallmatrix}B&-E^T\\E&0\end{bsmallmatrix}$.
Most importantly, $\BB$ is skew-symmetrizable and agrees with $\tB$ in columns $1$ to $n$.
Throughout, if we have defined an extended exchange matrix $\tB$, without comment we will take $B$ to be the underlying exchange matrix and $\BB$ to be the associated $m\times m$ matrix.

The matrix $\BB$ defines mutation maps $\eta_\kk^{\BB^T}$ that act on $\reals^m$ rather than $\reals^n$, but without exception we will only consider mutations in positions $1,\ldots,n$.
Also, given $\BB$, a sequence $\kk=k_m\cdots k_1$ of indices in $\set{1,\ldots,n}$, and seeds $t_1,\ldots,t_m$ by $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$, there are associated matrices of $\g$-vectors and $C$-vectors, which we write as $\GG_t^{\BB;t_0}$ and $\CC_t^{\BB;t_0}$.
Since $\kk$ only contains indices in $\set{1,\ldots,n}$, these matrices have block forms
\[
\GG_t^{\BB;t_0}=\begin{bsmallmatrix}G_t^{B;t_0}&0\\H_t^{\tB;t_0}&I_{m-n}\end{bsmallmatrix}
\quad\text{ and }\quad
\CC_t^{\BB;t_0}=\begin{bsmallmatrix}C_t^{B;t_0}&D_t^{\tB;t_0}\\0&I_{m-n}\end{bsmallmatrix},
\]
where $H_t^{\tB;t_0}$ is an $(m-n)\times n$ matrix, $D_t^{\tB;t_0}$ is an $n\times(m-n)$ matrix, and $I_{m-n}$ is the identity matrix.

Given a vector $\lambda\in\reals^m$, define $\P^\tB_{\lambda,\kk}=\bigl(\eta_{\kk}^{\BB^T}\bigr)^{-1}\set{\eta_\kk^{\BB^T}(\lambda)+\tB_t\alpha:\alpha\in\reals^n,\alpha\ge0}$.
Define the \newword{dominance region} $\P^\tB_\lambda$ of $\lambda$ with respect to $\tB$ to be the intersection $\bigcap_\kk\P^\tB_{\lambda,\kk}$ over all sequences~$\kk$ of indices in $\set{1,\ldots,n}$.

We relate the dominance region $\P_\lambda^\tB$ with respect to the extended exchange matrix $\tB$ to the dominance region $\P_\lambda^B$ with respect to the (non-extended) exchange matrix $B$.
Let $\Proj_n$ be the projection from $\reals^m$ to $\reals^n$ that ignores the last $m-n$ coordinates.

\begin{proposition}\label{contains proj}
Let $\tB$ be an extended exchange matrix with underlying exchange matrix $B$.
If $\lambda\in\reals^m$ and $\lambda'=\Proj_n\lambda\in\reals^n$, then $\Proj_n(\P_\lambda^\tB)\subseteq\P_{\lambda'}^B$.
\end{proposition}
\begin{proof}
It is apparent that $\Proj_n\circ\eta_\kk^{\BB^T}=\eta_\kk^{B^T}\circ\Proj_n$ as maps on $\reals^n$, for any sequence $\kk$ of indices in $\set{1,\ldots,n}$.
Since $\bigl(\eta_\kk^{\BB^T}\bigr)^{-1}=\eta_{\kk^{-1}}^{\mu_\kk(\BB)^T}$, the analogous fact is true for $\bigl(\eta_\kk^{\BB^T}\bigr)^{-1}$.
Also, $\Proj_n\set{\tB_t\alpha:\alpha\in\reals^n,\alpha\ge0}=\set{B_t\alpha:\alpha\in\reals^n,\alpha\ge0}$.
The \lcnamecref{contains proj} follows.
\end{proof}

Since $\kk$ consists only of indices in $\set{1,\ldots,n}$, the domains of definition of $\eta_\kk^{\BB^T}$ are determined by the domains of definition of $\eta_\kk^{B^T}$.
Specifically, each domain of definition of $\eta_\kk^{\BB^T}$ is $\Proj_n^{-1}D$ for some domain $D$ of definition of $\eta_\kk^{B^T}$.
Accordingly, we define $\Cone_t^{\tB;t_0}$ to be %the nonnegative span of the columns of $\GG_t^{\BB;t_0}$ and the columns of $\begin{bsmallmatrix}0\\-I_{m-n}\end{bsmallmatrix}$.
$\Proj_n^{-1}\Cone_t^{B;t_0}$.
Since $\Cone_t^{B_m;t_m}=\eta_\kk^{B^T}\bigl(\Cone_t^{B;t_0}\bigr)$ for every seed~$t$, also $\Cone_t^{\tB_m;t_m}=\eta_\kk^{\BB^T}\bigl(\Cone_t^{\tB;t_0}\bigr)$ for every seed~$t$.

Similarly, we define the mutation fan for $\tB^T$ to be the set $\F_{\tB^T}$ of cones $\Proj_n^{-1}C$ such that $C$ is a cone in the mutation fan $\F_{B^T}$.
Since $\Proj_n\circ\eta_\kk^{\BB^T}=\eta_\kk^{B^T}\circ\Proj_n$ for sequences $\kk$ of indices in $\set{1,\ldots,n}$, the map $\eta_\kk^{\BB^T}$ is linear on every cone of $\F_{\tB^T}$ and acts as an isomorphism of fans from $\F_{\tB^T}$ to $\F_{\mu_\kk(\tB)^T}$.

To understand dominance regions $\P^\tB_\lambda$, it is enough to consider the case where $\lambda$ has nonzero entries only in positions $1,\ldots,n$.
Other dominance regions are obtained by translation, as explained in the following lemma.
The lemma is an immediate consequence of the fact that domains of definition of $\eta_\kk^{\BB^T}$ depend only on the first $n$ coordinates.
\begin{lemma}\label{after all coefficients are just coefficients}
If $\lambda$ and $\lambda'$ are vectors in $\reals^m$ that agree in the first $n$ coordinates, then $\P^\tB_{\lambda'}=\P^\tB_\lambda-\lambda+\lambda'$.
\end{lemma}

\cref{shift} immediately implies the following lemma.
\begin{lemma}\label{shift extended}
If $\lambda'=\eta^{\BB^T}_\kk$ and $\tB'=\mu_\kk(\tB)$, then 
\begin{enumerate}[\quad\bf1.]
\item \label{shift all}
$\eta^{\BB^T}_\kk\!\!(\P^\tB_\lambda)=\P^{\tB'}_{\lambda'}$.
\item \label{shift one}
$\eta^{\BB^T}_\kk\!\!(\P^\tB_{\lambda,\ll})=\P^{\tB'}_{\lambda',\ll\kk^{-1}}$ for any $\ll$.
\end{enumerate}
\end{lemma}



We will prove the following extension of \cref{P in B0C} and an important corollary.

\begin{theorem}\label{P in B0C extended}
Suppose $\kk=k_m\cdots k_1$ is a sequence of indices in $\set{1,\ldots, n}$ and $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
If $\kk^{-1}=k_1\cdots k_m$ is a red sequence for $B_t$, then for any~$\lambda$ in the domain of definition of $\eta_\kk^{\BB_0^T}$ that contains $\Cone^{B_0;t_0}_t$,
\[\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda+\GG_t^{\BB_0;t_0}\tB_t\alpha:\alpha\in\reals^n,\alpha\ge0}=\set{\lambda+\tB_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}.\]
\end{theorem}
\begin{proof}
First, we notice that $\kk^{-1}=k_1\cdots k_m$ is a red sequence for $\BB_t$, or in other words, $\kk$ is a green sequence for $-\BB_t$.
Indeed, since $\CC_{t_{\ell-1}}^{-\BB;t_0}=\begin{bsmallmatrix}C_{t_{\ell-1}}^{-\BB;t_0}&*\\0&I_{m-n}\end{bsmallmatrix}$, the sign of column $k_\ell$ of $\CC_{t_{\ell-1}}^{-\BB;t_0}$ equals the sign of column $k_\ell$ of $C_{t_{\ell-1}}^{-\BB;t_0}$ whenever $1\le\ell<k$.
Thus \cref{P in B0C} says that
\[\P^{\BB_0}_{\lambda,\kk}\subseteq\set{\lambda+\GG_t^{\BB_0;t_0}\BB_t\alpha:\alpha\in\reals^m,\alpha\ge0}=\set{\lambda+\BB_0\CC_t^{\BB_0;t_0}\alpha:\alpha\in\reals^m,\alpha\ge0}.\]
The assertion of \cref{P in B0C extended} is that the same holds even when, in each term, the conditions $\alpha\in\reals^m,\alpha\ge0$ are strengthened by requiring that $\alpha$ is zero in coordinates $n+1,\ldots,m$.

Thus we run through the proof of \cref{P in B0C} with $\BB$ replacing $B$ and $m$ replacing $n$ throughout and these additional conditions on $\alpha$ in all relevant expressions.
There is no effect on the argument until the point of showing that $V\cap\set{x\in\reals^m:\sgn x_{k_1}=\ep}\subseteq U$.
Here, we need to show that every vector $v_i=\bigl(E^{\BB_1}_{-\ep,k_1}\BB_1\CC_t^{\BB_1;t_1}\bigr)_{\col i}$ with $i\in\set{1,\ldots,n}$ whose $k_1$-entry has sign~$\ep$ is contained in $\posspan\set{\left(E^{\BB_1}_{\ep,k_1}\BB_1\CC_t^{\BB_1;t_1}\right)_{\col i}}_{i=1}^n$.
We argue as in the proof of \cref{P in B0C} that $v_i=N\bigl(\CC_t^{\BB_1;t_1}\bigr)_{\col i}+\sigma N_{\col k}$ and that $\ep e_k$ is a nonnegative linear combination of columns of~$\CC_t^{\BB_1;t_1}$.
Since $\CC_t^{\BB;t_0}=\begin{bsmallmatrix}C_t^{B;t_0}&*\\0&I_{m-n}\end{bsmallmatrix}$, we conclude that $\ep e_k$ is a nonnegative linear combination of columns $1$ through $n$ of~$\CC_t^{\BB_1;t_1}$.
Thus $v_i$ is a nonnegative linear combination of columns $1$ through $n$ of~$N\CC_t^{\BB_1;t_1}$ as desired.
\end{proof}

\begin{corollary}\label{P point}  
Suppose $\tB_0$ is an extended exchange matrix and suppose that the nonnegative linear span of the columns of $\tB_0$ contains no line.
Suppose $t$ is a seed in the exchange graph for $\tB_0;t_0$ and take $\lambda\in\Cone^{\tB_0;t_0}_t$.
If there exists a maximal red sequence for $B_t$, then $\P^{\tB_0}_\lambda=\set{\lambda}$.
\end{corollary}

\begin{proof}%[Proof of \cref{P point}]
Let $t'$ be the seed at the end of the maximal red sequence for $B_t$.
There exists $\ll=\ell_q\ell_{q-1}\cdots\ell_1$ with $t_0=t'_0\overset{\ell_1}{\edge}t'_1\overset{\ell_2}{\edge}\,\cdots\,\overset{\ell_q}{\edge}t'_q=t'$.
Let $\lambda'=\eta^{\BB_0^T}_\ll\!(\lambda)$.
\cref{shift extended} says $\eta^{\BB_0^T}_\ll\!(\P^{\tB_0}_\lambda)=\P^{\tB_{t'}}_{\lambda'}$.
Thus it is enough to prove that $\P^{\tB_{t'}}_{\lambda'}=\set{\lambda'}$.
Since $\eta_\ll^{\BB_0^T}\bigl(\Cone_t^{\tB_0;t_0}\bigr)=\Cone_t^{\tB_{t'};t'}$, we have reduced the proof to the case where there is a maximal red sequence for $B_t$ starting from $t$ and ending at $t_0$.

Working in that reduction, let $\kk=k_m\cdots k_1$ be the reverse of the maximal red sequence and define seeds $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
Then \cref{P in B0C extended} says that $\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda+\tB_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}$.

Since $\kk^{-1}$ is a maximal red sequence for $B_t$, or in other words a maximal green sequence for $-B_t$, every column of $C_{t_0}^{-B_t;t}$ has negative sign, so $\Cone_{t_0}^{B_t^T;t}=\set{x\in\reals^n:x^TC_{t_0}^{-B_t;t}\ge0}$ consists of vectors with nonpositive entries.
Since $\left(\reals_{\le0}\right)^n$ is a cone in the mutation fan $\F_{-B_t}$ (for example, combining \mbox{\cite[Proposition~7.1]{universal}}, \mbox{\cite[Proposition~8.9]{universal}}, and sign-coherence of $C$-vectors) and also $\Cone_{t_0}^{B_t^T;t}$ is a cone in $\F_{-B_t}$, we see that $\Cone_{t_0}^{B_t^T;t}=\left(\reals_{\le0}\right)^n$.
Thus, up to permuting columns, $C_{t_0}^{-B_t;t}$ is the negative of the identity matrix.
We see that $\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda-\tB_0\alpha:\alpha\in\reals^n,\alpha\ge0}$.

Since also $\P^{\tB_0}_{\lambda,\emptyset}=\set{\lambda+\tB_0\alpha:\alpha\in\reals^n,\alpha\ge0}$, and since the nonnegative linear span of the columns of $\tB_0$ contains no line, we conclude that $\P^{\tB_0}_\lambda=\set{\lambda}$.
\end{proof}

The hypothesis that the nonnegative linear span of the columns of $\tB_0$ contains no line is strictly weaker than requiring that the columns of $\tB_0$ are linearly independent.
We have the following further corollaries.

An exchange matrix $B=[b_{ij}]$ determines a directed graph with vertices $\set{1,\ldots,n}$ and edges $i\to j$ whenever $b_{ij}>0$.
We say that $B$ is \newword{bipartite} if there is a bipartition $\set{1,\ldots,n}=P\cup N$ such that $i\to j$ implies $i\in P$ and $j\in N$.
If $B$ is bipartite, then the nonnegative linear span of the columns of $B$ contains no line, because every vector $\begin{bsmallmatrix}x_1\\\vdots\\x_n\end{bsmallmatrix}$ in the nonnegative linear span has $x_i\ge0$ if $i\in P$ and $x_i\le0$ if $i\in N$.

\begin{corollary}\label{finite P point}  
Suppose $B$ is an exchange matrix of finite type.
Then $\P^B_\lambda=\set{\lambda}$ for all $\lambda\in\reals^n$.
\end{corollary}
\begin{proof}
Since $B$ is of finite type, it is mutation-equivalent to an exchange matrix whose graph is an orientation of a Dynkin diagram of finite type and in particular an oriented tree.
Source-sink mutations on this tree give a bipartite exchange matrix $B_0$, so that the nonnegative linear span of the columns of $B_0$ contains no line.
Suppose $\kk$ is a sequence of indices such that $\mu_\kk(B_0)=B$.
For any $\lambda\in\reals$, the dominance region $\P_\lambda^B$ is $\eta_\kk^{B_0^T}(\P_{\lambda'}^{B_0})$ for $\lambda'=\eta_\kk^{B_0^T}(\lambda)$.
Every exchange matrix of finite type admits a maximal red sequence, so \cref{P point} applies to say that $\P_{\lambda'}^{B_0}=\set{\lambda'}$, so $\P_\lambda^B=\set{\lambda}$.
\end{proof}

An exchange matrix is of \newword{affine type} if it is mutation-equivalent to an acyclic exchange matrix whose underlying Cartan matrix is of affine type.

\begin{corollary}\label{affine P point}  
Suppose $B$ is an exchange matrix of affine type.
If $t$ is a seed in the exchange graph for $B;t_0$ and $\lambda\in\Cone^{B;t_0}_t$, then $\P^B_\lambda=\set{\lambda}$.
\end{corollary}
\begin{proof}
We first construct an exchange matrix $B_0$ mutation equivalent to $B$ such that the nonnegative linear span of the columns of $B_0$ contains no line.
Since $B$ is of affine type, it is mutation-equivalent to an exchange matrix whose graph is an orientation of a Dynkin diagram of affine type.
When this graph is an oriented tree, source-sink moves make a bipartite exchange matrix $B_0$ mutation equivalent to $B$.

Otherwise, the graph is a cycle, and source-sink moves lead to an exchange matrix $B_0$ whose oriented graph has exactly one source and exactly one sink.
For convenience, we can reindex $B_0$ so that the source is $1$, the sink is $n$, and the directed graph for $B_0$ has $1\to2\to\cdots\to k\to n$ and $1\to k+1\to k+2\to\cdots\to n$ for some $k$ with $1\le k<n$.
Then any vector $x$ in the nonnegative span of the columns of $B$ has 



I THINK we can show that for this $B_0$ also, the nonnegative linear span of the columns of $B_0$ contains no line.
Assuming we can, finish this like the finite type case.

NO, this doesn't work.
$B_0=\begin{bsmallmatrix*}[r]0&1&1&0\\-1&0&0&1\\-1&0&0&1\\0&-1&-1&0\end{bsmallmatrix*}$


CAN I USE the neighboring seed thing to get the right orientation?

Computations show that the nonnegative linear span of the columns of $B_0$ contains no line in all cases except where $n$ and $k$ are both even.  
Only one of those cases is bipartite (the case $k=\frac n2$) and I don't know right now what to do in the other cases.
\end{proof}

\subsection{Exchange relations in the principal coefficients case}\label{exch rel sec}
We pause to prove a necessary fact about exchange relations, assuming basic background from \cite[Sections~2 \&~3]{ca4}.
%For a moment we work in the most general setting of (normalized) cluster algebras in the sense of \cite[Definition~2.3]{ca4}.
%(See also \cite[Remark~2.7]{ca4}.)
Two cluster variables $x$ and $x'$ in a cluster pattern of rank $n$  are \newword{exchangeable} if there is no cluster containing both $x$ and $x'$ but there exists a set $\Gamma$ of $n-1$ cluster variables such that $\Gamma\cup\set{x}$ is a cluster and $\Gamma\cup\set{x'}$ is a cluster.
The exchange relation relating $x$ and $x'$ might, in principle, depend on $\Gamma$, but we show that, in the case of principal coefficients, it only depends on $x$ and $x'$.

\begin{lemma}\label{exch ind}  \sayN{More generally, we may as well state this for signed nondegenerating coefficients, if we define that term.}
Suppose $x$ and $x'$ are exchangeable cluster variables in a cluster pattern with principal coefficients.
Then every exchange relation for $x$ and $x'$ is the same:
It involves the same two cluster monomials with the same coefficients.
\end{lemma}
\begin{proof}
Let $\g(x)$ and $\g(x')$ be the $\g$-vectors of $x$ and $x'$.
Sign-coherence of $C$-vectors (explained earlier in \cref{def sec}) implies that every exchange relation for $x$ and $x'$ has a monomial $M$, involving cluster variables but not coefficient variables, whose $\g$-vector is $\g(x)+\g(x')$.
This is a cluster monomial, and it is known that different cluster monomials have different $\g$-vectors (as conjectured in \cite[Conjecture~7.10]{ca4} and proved as \cite[Theorem~2.13]{GHKK}).
We see that $M$ is the same cluster monomial in 
Thus any exchange relation for $x$ and $x'$ writes $xx'$ as $M$ plus another monomial $N$.
But then $N=xx'-M$, so every exchange relation for $x$ and $x'$ also has the same~$N$.
But $N$ is a monomial in the coefficient variables times a cluster monomial, and again, it is the same cluster monomial in any exchange relation and the same coefficient monomial.
\end{proof}

\begin{remark}\label{4.3}
It appears that, to extend \cref{exch ind} to arbitrary coefficients, one needs \cite[Conjecture~4.3]{ca4}, which says that the exchange graph does not depend on the choice of coefficients.
\sayN{As of 2012 (and maybe 2019), this was known for skew-symmetric geometric type (Linear independence of cluster monomials for skew-symmetric cluster algebras, by Giovanni Cerulli Irelli, Bernhard Keller, Daniel Labardini-Fragoso, Pierre-Guy Plamondon).   Also, for nondegenerate $B$ (M. Gekhtman, M. Shapiro, A. Vainshtein, On the properties of the exchange graph of a cluster algebra, Math. Res. Lett. 15 (2) (2008) 321--330.) and of course finite type (ca2).  Is it known more generally now? (Is it worth trying to prove it for affine type??}
\end{remark}

\subsection{Mutation-finite exchange matrices}\label{mut fin sec}
An exchange matrix $B$ is \newword{mutation-finite} if only finitely matrices can be obtained by $B$ arbitrary sequences of mutations.
The following \lcnamecref{mut fin 2x2} is \cite[Theorem~2.8]{FeShTu}.  \sayN{Cluster algebras of finite mutation type via unfoldings}

\begin{theorem}\label{mut fin 2x2}
An $n\times n$ exchange matrix $B$ with $n\ge3$ is mutation finite if and only if, for every sequence $\kk$ of indices in $\set{1,\ldots,n}$, the exchange matrix $B'=\mu_\kk(B)$ satisfies $b'_{ij}b'_{ji}\ge-4$ for all indices $i$ and $j$.
\end{theorem}

This theorem is useful to us because all cluster algebras of affine type are mutation-finite.
In fact, the following stronger statement holds \cite[Theorem~3.5]{Seven} \sayN{Cluster algebras and semipositive symmetrizable matrices}

\begin{theorem}\label{acyc mut fin}
An acyclic $n\times n$ exchange matrix with $n\ge3$ is mutation finite if and only if its underlying Cartan matrix is of finite or affine type.
\end{theorem}

We also point out a well-known fact.  \sayN{Surely this is well known!  Reference?}
The \lcnamecref{mut fin sub} holds because, for any subset $I$ of $\set{1,\ldots,n}$, and any sequence $\kk$ of indices in $I$, if $B'=[b'_{ij}]=\mu_\kk(B)$, then $\mu_\kk([b_{ij}]_{i,j\in I})=[b'_{ij}]_{i,j\in I}$.
(The analogous fact for 

\begin{proposition}\label{mut fin sub}
Suppose $B$ is an $n\times n$ exchange matrix.
If $B$ is mutation-finite then, for every subset $I$ of $\set{1,\ldots,n}$, the submatrix $[b_{ij}]_{i,j\in I}$ is mutation-finite.
\end{proposition}
%\begin{proof}
%Suppose $B'=[b_{ij}]_{i,j\in I}$ is not mutation-finite.
%Then \cref{mut fin 2x2} says that there is some sequence $\kk$ of indices in $I$ such that $\mu_\kk(B')$ has a $2\times2$ submatrix failing the condition of \cref{mut fin 2x2}.
%Then the same submatrix occurs in $\mu_\kk(B)$, and thus $B$ is not mutation-finite.
%\end{proof}

\subsection{Growth of cluster algebras}\label{growth sec}
The \newword{growth} of a cluster algebra is the asymptotic behavior of the function that counts the number of seeds within a fixed mutation distance from some initial seed.
Some facts about growth will be useful.

First, beyond rank $2$, the notion of affine type has the following intrinsic characterization.
Combining \cite[Theorem~3.5]{Seven} and \cite[Theorem~1.1]{FeShThTu12}, we see that a cluster algebra of rank at least $3$ is of affine type if and only if it is not of finite type but has linear growth.
%Growth rate of cluster algebras
%Anna Felikson, Michael Shapiro, Hugh Thomas and Pavel Tumarkin

Second, given an $n\times n$ exchange matrix $B=[b_{ij}]$ and a subset $I\subseteq\set{1,\ldots,n}$, if the submatrix $[b_{ij}]_{i,j\in I}$ has exponential growth, it is immediate that that $B$ has exponential growth as well.

Finally, we will need that fact that non-acyclic skew-symmetrizable $3\times3$ exchange matrices with $b_{12}b_{21}=b_{13}b_{31}=b_{23}b_{32}=-4$ have exponential growth.
One can check that such matrices have the property that every single step mutation coincides with negation of the matrix, and thereby apply \cite[Theorem~1.1]{FeShThTu12}.



%\section{Affine type}
%Let $B_0$ be acyclic of affine type, indexed so that entries above the diagonal are nonnegative.
%%Or is this backwards?:
%%Then $n(n-1)\cdots1$ is a maximal green sequence for $B_0$ and $12\cdots n$ is a maximal red sequence for $B_0$.
%Take $\lambda$ in the imaginary cone.
%Let $t$ be any seed such that $\Cone_t^{B_0;t_0}$ has $n-2$ rays on the boundary of the imaginary wall $\d_\infty$ such that $\lambda$ is in the imaginary cone spanned by those $n-2$ rays and the imaginary ray.
%Let $\kk=k_m\cdots k_1$ be a sequence such that $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
%%(Probably we need to address greenness or redness of this sequence, which we can presumably do pretty easily with the sortable elements stuff.)
%
%Let $\tB_0$ be an extension of $B_0$ that has linearly independent columns.
%Computations show that  \margin{Probably need more specific notation about $\d_\infty$.}
%
%\[\P_{\lambda,\kk}^{\tB_0}\cap\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0}\cap\d_\infty=\set{\lambda+x\tB_0\delta:x\in\reals}\cap\d_\infty.\]
%
%Let $u$ be the seed reached from $t_0$ by the sequence $n(n-1)\cdots1$ and let $\lambda_u$ be $\eta_{n(n-1)\cdots1}^{\BB_0^T}(\lambda)$.
%\cref{shift extended} says that $\eta_{n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk}^{\tB_0})=\P_{\lambda_u,\kk12\cdots n}^{\tB_u}$ and
%\[\eta_{n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0})=\P_{\lambda_u,\kk n\cdots11\cdots n}^{\tB_u}=\P_{\lambda_u,\kk}^{\tB_u}.\]
%The map $\eta_{n(n-1)\cdots1}^{\BB_0^T}$ is linear on $\d_\infty$ and maps $-\tB_0\delta$ to $-\tB_u\delta$, so it maps the set $\set{\lambda+x\tB_0\delta:x\in\reals}\cap\d_\infty$ to $\set{\lambda_u+x\tB_u\delta:x\in\reals}\cap\d_\infty$.
%Thus we will show that 
%\[\P_{\lambda_u,\kk}^{\tB_u}\cap\P_{\lambda_u,\kk12\cdots n}^{\tB_u}\cap\d_\infty=\set{\lambda_u+x\tB_u\delta:x\in\reals}\cap\d_\infty.\]
%
%Let $t'$ be the seed reached from $t_0$ by the sequence $\kk n(n-1)\cdots1$ or in other words, the seed reached from $u$ by the sequence $\kk$.
%Leaving out repetitions of ``$\alpha\ge0$'' for reasons of space,
%\begin{multline*}
%\P_{\lambda_u,\kk}^{\tB_u}
%\cap
%\P_{\lambda_u,\kk12\cdots n}^{\tB_u}
%\cap
%\d_\infty\\
%\begin{aligned}
%&=
%\left(\eta_{\kk}^{\BB_u^T}\right)^{-1}\!\!\set{\eta_{\kk}^{\BB_u^T}(\lambda_u)+\tB_{t'}\alpha}
%\cap
%\left(\eta_{\kk1\cdots n}^{\BB_u^T}\right)^{-1}\!\!\set{\eta_{\kk1\cdots n}^{\BB_u^T}(\lambda)+\tB_t\alpha}
%\cap
%\d_\infty\\&=
%\eta_{\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk}^{\BB_u^T}(\lambda_u)+\tB_{t'}\alpha}
%\cap
%\eta_{n\cdots1\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk1\cdots n}^{\BB_u^T}(\lambda_u)+\tB_t\alpha}
%\cap
%\d_\infty\\&=
%\eta_{\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk}^{\BB_u^T}(\lambda_u)+\tB_{t'}\alpha}
%\cap
%\eta_{n\cdots1}^{\BB_0^T}\left(\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk1\cdots n}^{\BB_u^T}(\lambda_u)+\tB_t\alpha}\right)
%\cap
%\d_\infty %\\&=
%\end{aligned}
%\end{multline*}
%
%
%
%
%
%Now, writing $\tB_u=\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}$, we have 
%\margin{If we use this, we need to put the $\GG B C$ thing in the background.}
%\begin{align*}
%\tB_t=\mu_\kk(\mu_{1\cdots n}(\tB_u))=\mu_\kk(\tB_0)
%&=\mu_\kk\left((\GG_{t_0}^{B_u;u})^{-1}\tB_uC_{t_0}^{B_u;u}\right)\\
%&=\mu_\kk\left(\begin{bsmallmatrix}G_{t_0}^{B_u;i}&0\\ H_{t_0}^{\tB_u;u}&I_{m-n}\end{bsmallmatrix}^{-1}\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}C_{t_0}^{B_u;u}\right)\\
%&=\mu_\kk\left(\begin{bsmallmatrix}(G_{t_0}^{B_u;u})^{-1}&0\\-H_{t_0}^{\tB_u;u}(G_{t_0}^{B_u;u})^{-1}&I_{m-n}\end{bsmallmatrix}\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}C_{t_0}^{B_u;u}\right)\\
%&=\mu_\kk\left(\begin{bsmallmatrix}B_0\\-H_{t_0}^{\tB_u;u}B_0+E_uC_{t_0}^{B_u;u}\end{bsmallmatrix}\right)\\
%\end{align*}
%We compute that $B_0=B_u$ and that $C_{t_0}^{B_u;u}=-I_n$.
%So $\tB_t=\mu_\kk\left(\begin{bsmallmatrix}B_u\\-H_{t_0}^{\tB_u;u}B_u-E_u\end{bsmallmatrix}\right)$.
%On the other hand, $\tB_{t'}=\mu_\kk(\tB_u)=\mu_\kk\left(\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}\right)$.
%
%
%KEY POINT:  On $\d_\infty$, the map $\eta_{n\cdots1}^{\BB_0^T}$ is linear, and agrees with $c$ or $c^{-1}$ or something.
%So there is just a chance that we know something.
%
%
%
%\subsection{Other ideas}
%
%%Let $t'$ be the seed reached from $t$ by the sequence $\kk n(n-1)\cdots1$ and let $u$ be the seed reached from $t_0$ by the sequence $n(n-1)\cdots1$.
%%Leaving out repetitions of ``$\alpha\ge0$'' for reasons of space,
%%\begin{multline*}
%%\P_{\lambda,\kk}^{\tB_0}
%%\cap
%%\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0}\\
%%\begin{aligned}
%%&=
%%\left(\eta_{\kk}^{\BB_0^T}\right)^{-1}\!\!\set{\eta_{\kk}^{\BB_0^T}(\lambda)+\tB_t\alpha}
%%\cap
%%\left(\eta_{\kk n\cdots1}^{\BB_0^T}\right)^{-1}\!\!\set{\eta_{\kk n\cdots1}^{\BB_0^T}(\lambda)+\tB_{t'}\alpha}
%%\\&=
%%\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk}^{\BB_0^T}(\lambda)+\tB_t\alpha}
%%\cap
%%\eta_{1\cdots n\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1}^{\BB_0^T}(\lambda)+\tB_{t'}\alpha}
%%\\&=
%%\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk}^{\BB_0^T}(\lambda)+\tB_t\alpha}
%%\cap
%%\eta_{1\cdots n\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1}^{\BB_0^T}(\lambda)+\tB_{t'}\alpha}
%%\end{aligned}
%%\end{multline*}
%%
%%Now, writing $\tB_0=\begin{bsmallmatrix}B_0\\E_0\end{bsmallmatrix}$, we have 
%%\margin{If we use this, we need to put the $\GG B C$ thing in the background.}
%%\begin{align*}
%%\tB_{t'}=\mu_\kk(\mu_{n\cdots1}(\tB_0))=\mu_\kk(\tB_u)
%%&=\mu_\kk\left((\GG_u^{B_0;t_0})^{-1}\tB_0C_u^{B_0;t_0}\right)\\
%%&=\mu_\kk\left(\begin{bsmallmatrix}G_u^{B_0;t_0}&0\\ H_u^{\tB;t_0}&I_{m-n}\end{bsmallmatrix}^{-1}\begin{bsmallmatrix}B_0\\E_0\end{bsmallmatrix}C_u^{B_0;t_0}\right)\\
%%&=\mu_\kk\left(\begin{bsmallmatrix}(G_u^{B_0;t_0})^{-1}&0\\-H_u^{\tB;t_0}(G_u^{B;t_0})^{-1}&I_{m-n}\end{bsmallmatrix}\begin{bsmallmatrix}B_0\\E_0\end{bsmallmatrix}C_u^{B_0;t_0}\right)\\
%%&=\mu_\kk\left(\begin{bsmallmatrix}B_u\\-H_u^{\tB;t_0}B_u+E_0C_u^{B_0;t_0}\end{bsmallmatrix}\right)\\
%%\end{align*}
%%We compute that $C $
%
%
%
%
%
%
%%What does \cref{P in B0C extended} say?
%%Assuming the appropriate sequences are red, and that $\lambda$ is in the right domain of definition, 
%%\[\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda+\GG_t^{\BB_0;t_0}\tB_t\alpha:\alpha\ge0}=\set{\lambda+\tB_0C_t^{B_0;t_0}\alpha:\alpha\ge0}\]
%%\[\P^{\tB_0}_{\lambda,\kk n(n-1)\cdots1}\subseteq\set{\lambda+\GG_{t'}^{\BB_0;t_0}\tB_{t'}\alpha:\alpha\ge0}=\set{\lambda+\tB_0C_{t'}^{B_0;t_0}\alpha:\alpha\ge0},\]
%%where $t'$ is the seed obtained from $t_0$ by $\kk n(n-1)\cdots1$.
%
%%Let $\lambda'=\eta_{\kk n(n-1)\cdots1}^{\BB_0^T}$.
%%\cref{shift extended} says that $\eta_{\kk n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk}^{\tB_0})=\P_{\lambda',\kk12\cdots n\kk^{-1}}^{\tB_{t'}}$ and
%%\[\eta_{\kk n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0})=\P_{\lambda',\kk n\cdots11\cdots n \kk^{-1}}^{\tB_{t'}}=\P_{\lambda',\emptyset}^{\tB_{t'}}=\set{\lambda'+\tB_{t'}\alpha:\alpha\ge0}.\]
%
%Let $\lambda_0=\eta_\kk^{\BB_0^T}$.
%\cref{shift extended} says that  
%\[\eta_\kk^{\BB_0^T}(\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0})=\P_{\lambda_0,\kk n(n-1)\cdots1\kk^{-1}}^{\tB_t}=\left(\eta^{\BB_t}_{\kk n\cdots1\kk^{-1}}\right)^{-1}\set{\eta^{\BB_t}_{\kk n\cdots1\kk^{-1}}(\lambda_0)+\tB_{t'}\alpha:\alpha\ge0}.\]
%Also, 
%\[\eta_\kk^{\BB_0^T}(\P_{\lambda,\kk}^{\tB_0})=\P_{\lambda_0,\kk\kk^{-1}}^{\tB_t}=\P_{\lambda_0,\emptyset}^{\tB_t}=\set{\lambda_0+\tB_t\alpha:\alpha\ge0}.\]
%
%We believe we could prove that $\P_{\lambda,\kk}$ only depends on the seed that $\kk$ leads to, not the specific $\kk$.
%So does it make sense to consider the sequence (maximal red or maximal green) that connects $t$ and $t'$.
%
%
%%Let $t'$ be the seed reached from $t$ by the sequence $\kk n(n-1)\cdots1\kk^{-1}$ and let $u$ be the seed reached from $t_0$ by the sequence $n(n-1)\cdots1$.
%%Now, leaving out repetitions of ``$\alpha\ge0$'' for reasons of space,
%%\begin{multline*}
%%\P_{\lambda_0,\kk n(n-1)\cdots1\kk^{-1}}^{\tB_t}\cap\P_{\lambda_0,\kk\kk^{-1}}^{\tB_t}\\
%%\begin{aligned}
%%&=\left(\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}\right)^{-1}\!\!\set{\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_{t'}\alpha}
%%\cap
%%\left(\eta_{\kk\kk^{-1}}^{\BB_t^T}\right)^{-1}\!\!\set{\eta_{\kk\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_t\alpha}\\
%%&=\eta_{\kk1\cdots n\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_{t'}\alpha}
%%\cap
%%\eta_{\kk\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_t\alpha}\\
%%&=\eta_{\kk1\cdots n}^{\BB_{u}^T}\left(\eta_{\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_{t'}\alpha}\right)
%%\cap
%%\eta_{\kk}^{\BB_0^T}\left(\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_t\alpha}\right)\\
%%\end{aligned}
%%\end{multline*}
%


\section{Dominance regions in affine type}
As before, an exchange matrix is of \newword{affine type} if it is mutation-equivalent to an acyclic exchange matrix whose underlying Cartan matrix is of affine type.
The main results of this paper are the following theorem and a version (\cref{affine main extended}) for extended exchange matrices. \sayN{hopefully}

\begin{theorem}\label{affine main}
Suppose $B$ is an exchange matrix of affine type.
If $\lambda$ is contained in the imaginary wall~$\d^B_\infty$, then the dominance region $\P^B_\lambda$ is the line segment parallel to the imaginary ray, with one endpoint at $\lambda$ and the other endpoint on the relative boundary of $\d^B_\infty$.
\end{theorem}

In preparation for the proof of \cref{affine main}, we quote some known results about the affine case and prove some new ones.
It is convenient to separate these results according to classes of affine seeds.

\subsection{Acyclic seeds}\label{acyc sec}
Most of what we know about cluster algebras of affine type rests on the fact that every cluster algebra of affine type has an acyclic seed whose exchange matrix is an orientation of a Cartan matrix of affine type.
(Indeed, the analogous observation is true for cluster algebras of finite type.)
The associated Cartan matrix of affine type allows us to model $\g$-vectors, $\dd$-vectors, and the mutation fan by almost-positive Schur roots model, sortable elements and (doubled) Cambrian fans.
We now quote some useful results in this setting and prove some additional facts.
More details can be found in \cite{affdenom,framework,afframe,affscat}.

Let $B_0$ be an acyclic $n\times n$ exchange matrix in a cluster algebra of affine type, indexed so that $b_{ij}\ge0$ whenever $i<j$.
%Let $\tB_0$ be an extension with linearly independent columns.
Let $A_0$ be the Cartan matrix underlying $B_0$, so that $A_0$ is of affine type.
Let $\delta$ be the shortest positive imaginary root in the root system $\RS$ associated to $A$.
Let $\RSpos$ be the set of positive roots in $\RS$ and let $\Simples=\set{\alpha_1,\ldots,\alpha_n}$ be the simple roots of $\RS$.
The \newword{support} of a root is the set of simple roots appearing in an expression for the roots as a linear combination of simple roots.

The matrix $B_0$ is skew-symmetrizable, \sayN{Already used this concept earlier without defining it.}
meaning that there exist positive constants $d_1,\ldots,d_n$ such that the entries $b_{ij}$ of $B_0$ satisfy $d_i b_{ij}=-d_j b_{ji}$ for all $i,j$.
These constants also symmetrize $A_0$, meaning that the entries $a_{ij}$ of $A_0$ satisfy $d_i a_{ij}=d_j a_{ji}$ for all $i,j$.
The \newword{(skew-)symmetrizing matrix} is the diagonal matrix $D$ with diagonal entries $d_1,\ldots,d_n$.
The \newword{simple co-roots} are $\alpha_i\ck= d_i^{-1} \alpha_i$. 
Thus the matrix $D$, applied to the simple-root coordinates of a vector, gives the simple co-root coordinates of that vector.
Furthermore, the matrix $DBD^{-1}$ has entries $d_ib_{ij}d_j^{-1}=-b_{ji}$, so $DBD^{-1}=-B^T$.
Also, $(E_{\ep,k}^B)^T=F_{\ep,k}^{-B^T}=DF_{\ep,k}^BD^{-1}$.

Let $V$ be the real vector space spanned by the roots and let $V^*$ be the dual space.
The simple co-roots $\Simples\ck$ are also in $V$ (each being a positive scaling of the corresponding simple root).
Given $\beta\in V$, the notation $\beta^\perp$ indicates the set of vectors in $V^*$ that pair to $0$ with $\beta$. 
We will identify $V^*$ with the vector space $\reals^n$ that appears elsewhere in the paper by identifying the dual basis of the simple co-roots (the \newword{fundamental weights}) with the standard unit basis vectors of $\reals^n$.

The Cartan matrix $A_0$ is the matrix of a bilinear form on $V$, in the simple-roots basis on the right and the simple-co-roots basis on the left.
Each real root defines a reflection that negates the root and fixes the hyperplane that is orthogonal to the root (with orthogonality defined in terms of $K$).
Let $W$ be the Weyl group generated by these reflections (an affine Coxeter  group with simple reflections $s_1,\ldots,s_n$).
The action of $W$ on $V$ fixes~$\delta$.
Let $c$ be the Coxeter element that can be read from $B_0$ by multiplying the simple reflections with $s_i$ preceding $s_j$ whenever $b_{ij}>0$.
Because we have indexed $B_0$ so that its $ij$-entry is nonnegative whenever $i<j$, we have $c=s_1\cdots s_n$.
Define
\begin{align}
\label{rep->}
\TravProj{c}&:=\set{\alpha_1,s_1\alpha_2,\ldots,s_1\cdots s_{n-1}\alpha_n},\\
\label{rep<-}
\TravInj{c}&:=\set{\alpha_n,s_n\alpha_{n-1},\ldots,s_n\cdots s_2\alpha_1},
\end{align}%
Continuing under the assumption that $A_0$ is of affine type, the sets $\TravProj{c}$ and $\TravInj{c}$ are disjoint, and the union $\TravProj{c}\cup\TravInj{c}$ contains one representative of each of the $2n$ infinite $c$-orbits of roots.
(See \cite[Chapter~1]{Dlab-Ringel} or \cite[Theorem~1.2]{afforb}.)
The following lemma is \cite[Lemma~1.6]{Dlab-Ringel} or \cite[Lemma~4.1]{afforb}.

\begin{lemma}\label{c to pos}
Suppose $\RS$ is a root system of affine type and $\beta$ is a positive root in~$\RS$. 
Then $c\beta$ is negative if and only if $\beta\in\TravInj{c}$, in which case $-c\beta\in\TravProj{c}$.
Also $c^{-1}\beta$ is negative if and only if $\beta\in\TravProj{c}$, in which case $-c^{-1}\beta\in\TravInj{c}$.
\end{lemma}

\cref{c to pos} leads immediately to the following result.
\begin{proposition}\label{who is pos}
Suppose $\RS$ is a root system of affine type and $\beta\in\RS$ is in an infinite $c$-orbit.
Then $c^i\beta$ is a positive root for all $i\ge0$ if and only if $\beta=c^p\gamma$ for some $\gamma\in\TravProj{c}$ and $p\ge0$.
\end{proposition}

We define a bilinear form $\omega_c$ on $V$ whose matrix, in simple co-root coordinates on the left and simple root coordinates on the right is~$B$.
This bilinear form is skew-symmetric because $B$ is skew-symmetrizable and the skew-symmetrizing constants are also the scaling factors between simple roots and simple co-roots.
The following lemma is a version of \cite[Lemma~3.8]{typefree}.

\begin{lemma}\label{omega s}
$\omega_{s_1cs_1}(s_1x,s_1y)=\omega_c(x,c)=\omega_{s_ncs_n}(s_nx,s_ny)$ for any $x$ and $y$ in $V$.
\end{lemma}

Applying \cref{omega s} $n$ times, we obtain the following lemma.

\begin{lemma}\label{omega c}
$\omega_c(x,y)=\omega_c(cx,cy)$ for any $x$ and $y$ in $V$.
\end{lemma}

\begin{proposition}\label{om del}
Suppose $\RS$ is of affine type.
If $\beta=c^p\gamma$ for some $\gamma\in\TravProj{c}$ and $p\ge0$, then $\omega_c(\beta,\delta)\ge0$. 
\end{proposition}
\begin{proof}
The hypothesis is that $\beta=(s_1\cdots s_n)^ps_1\cdots s_{k-1}\alpha_k$ for some $k\in\set{1,\ldots,n}$.
We argue by induction on $np+k$.
If $p=0$ and $k=1$, then $\beta=\alpha_1$ and $\omega_c(\alpha_1,\delta)\ge0$ because every entry of $B$ in row $1$ is nonnegative with at least one positive entry and $\delta$ has all simple root coordinates positive.
Otherwise $s\beta=(s_2\cdots s_ns_1)^ps_2\cdots s_{k-1}\alpha_k$ if $k>1$ or $s\beta=(s_2\cdots s_ns_1)^{p-1}s_2\cdots s_n\alpha_1$ if $k=1$.
In either case, $s\beta$ is of the form $(scs)^{p'}\gamma'$ for $\gamma'\in\TravProj{scs}$.
By induction, $\omega_{scs}(s\beta,\delta)\ge0$, and since $s\delta=\delta$, \cref{omega s} completes the proof.
\end{proof}

Every finite $c$-orbit of roots consists entirely of positive roots or entirely of negative roots.
(See \cite[Chapter~1]{Dlab-Ringel} or \cite[Theorem~1.2(5)]{afforb}.)
%The following \lcnamecref{om del fin} is an immediate consequence of \cite[Proposition~2.16]{affdenom}, because the bilinear form $E_c$ in \cite[Proposition~2.16]{affdenom} satisfies $\omega_c(\beta,\beta')=E_c(\beta,\beta')-E_c(\beta',\beta)$.

\begin{proposition}\label{om del fin}
Suppose $\RS$ is of affine type and suppose $x\in V$.
Then $x$ is in a finite $c$-orbit if and only if $\omega_c(x,\delta)=0$.
\end{proposition}
\begin{proof}
Suppose the orbit of $x$ has $k$ elements.
The sum $x+cx+\cdots+c^{k-1}x$ of all vectors in the $c$-orbit of $x$ is fixed by the action of $c$ and thus is a scalar multiple of $\delta$, since $\delta$ spans the $1$-eigenspace of $c$.
Because $\omega_c$ is skew-symmetric, $\omega_c(x+cx+\cdots+c^{k-1}x,\delta)=0$.
Now \cref{omega c} says that $k\omega_c(x,\delta)=0$.
The subspace consisting of vectors $x$ such that $\omega_c(x,\delta)=0$ has dimension $n-1$, and the subspace consisting of vectors in finite $c$-orbits also has dimension $n-1$ (for example, it contains $n-1$ linearly independent roots).
\end{proof}

Let $\RST{c}$ be the set of roots in $\RS$ that are in finite $c$-orbits.
These form a subsystem of $\RS$, in the sense that the set of real roots in $\RST{c}$ are closed under the reflections they define, and all imaginary roots are in $\RST{c}$.
But $\RST{c}$ might not be a root system in the usual (Kac-Moody) sense.
Rather, it is the product of $1$, $2$, or $3$ affine root systems of type $\afftype{A}$, living in a vector space that may be smaller than the sum of the ranks of the irreducible factor ($0$, $1$, or $2$ dimensions smaller):
Each factor has a shortest positive imaginary root, and all of these are identified with $\delta$.
(Thus $\RST{c}$ is a Kac-Moody root system only in the case where it has one of these components.)
The set of simple roots of $\RST{c}$ is written as $\SimplesT{c}$.
The total number of roots in $\SimplesT{c}$ is $n-2$ plus the number of factors of $\RST{c}$.
If $\beta_0,\ldots,\beta_k$ are the simple roots in one of the factors of $\RST{c}$, then $\beta_0+\cdots+\beta_k=\delta$.
These simple roots can be indexed so that $c\beta_{i-1}=\beta_i$ for $i=1,\ldots,k$ and $c\beta_k=\beta_0$.

Let $\APTre{c}$ be the set of positive roots in $\RST{c}$ whose support is strictly smaller than $\Simples$.
Let $\APT{c}=\APTre{c}\cup\set\delta$.
The set $\AP{c}$ of \newword{almost positive Schur roots} is ${-\Simples\cup(\RSpos\setminus\RST{c})\cup\APT{c}}$.
The set $\APre{c}=\AP{c}\setminus\set\delta$ of real roots in $\AP{c}$ is precisely the set of denominator vectors of cluster variables, with respect to the initial exchange matrix $B_0$.
There is a notion \cite[Definition~4.3]{affdenom} of $c$-compatibility of almost positive Schur roots (similar in spirit to the compatibility of almost positive roots in~\cite{ga}) such that a set of almost positive real Schur roots is pairwise $c$-compatible if and only if the corresponding cluster variables are contained in a common cluster.
Thus almost positive real Schur roots and $c$-compatibility model the $\dd$-vector fan (the fan obtained by replacing each cluster with the nonnegative linear span of its $\dd$-vectors).
In addition to these $\dd$-vector cones, there are cones spanned by pairwise $c$-compatible sets that contain~$\delta$.
The set of all these cones is a complete fan $\Fan_c(\RS)$ in $V$ called the \newword{affine generalized associahedron fan}.

Just as in finite type \cite{ga}, the notion of $c$-compatibility is a simplification of $c$-compatibility degree, which assigns an integer to any pair of almost-positive Schur roots.
(Two roots are $c$-compatible if and only if their $c$-compatibility degree is~$0$.)
We will not need the definition of $c$-compatibility, but we give some of its crucial properties, particularly as it relates to roots in $\APT{c}$.
The following proposition is \cite[Proposition~5.6]{affdenom}.

\begin{proposition}\label{delta c compat}
A root $\alpha\in\APre{c}$ is $c$-compatible with~$\delta$ if and only if $\alpha\in\APTre{c}$.
\end{proposition}

Two roots $\alpha,\beta\in\APTre{c}$ are called \newword{nested} if $\SuppT(\alpha)\subseteq\SuppT(\beta)$ or if $\SuppT(\beta)\subseteq\SuppT(\alpha)$ or \newword{spaced} if $[\SuppT(c^{-1}\alpha)\cup\SuppT(\alpha)\cup\SuppT(c\alpha)]\cap\SuppT(\beta)=\emptyset$.
The following proposition is \cite[Proposition~5.12]{affdenom}.

\begin{proposition}\label{compatible in tubes}
Two distinct roots $\alpha$ and $\beta$ in $\APTre{c}$ are $c$-compatible if and only if they are nested or spaced.
\end{proposition}

Thus roots $\alpha,\beta\in\APTre{c}$ are $c$-compatible if they are in different factors of $\RST{c}$, while
$c$-compatibility of roots in the same factor can be understood by picturing their supports on a cycle.
(The cycle consists of the simple roots the factor with edges connecting each $\beta$ to $c\beta$.)
The roots appear as ``tubes'' in the cycle, and $c$-compatibility is the usual compatibility of tubes:
Either the tubes are nested or there is at least one vertex between them on both sides.

%There is a piecewise-linear map $\tau_c$ that induces an automorphism of $\Fan_c(\RS)$.
%The map $\tau_c$ that agrees with $c$ on all but finitely many almost positive Schur roots and in particular agrees with $c$ on all roots in $\APT{c}$.
%For every almost positive Schur root $\beta$ not in $\APT{c}$ (i.e.\ every $\beta\in{-\Simples\cup(\RSpos\setminus\RST{c})}$), $\tau_c^k\beta$ approaches the direction of $\delta$ as $k\to\infty$.
%More precisely, as $k\to\infty$, $\tau_c^k\beta$ is eventually of the form $\beta'+m\delta$, where $\beta'\in\RSfin$ continues to vary with $k$ and $m\to\infty$ as $k\to\infty$.

There is a piecewise linear homeomorphism $\nu_c$ from $V$ to $V^*$ that is linear on every cone of $\Fan_c(\RS)$ and thus defines a fan $\nu_c(\Fan_c(\RS))$ in $V^*$.
The fan $\nu_c(\Fan_c(\RS))$ is the mutation fan for $B_0^T$, which coincides in this case with the transposed cluster scattering fan of $B_0$ and the map $\nu_c$ restricts to an isomorphism from the $\dd$-vector fan of $B_0$ to the $\g$-vector fan \cite[Theorem~2.9]{affscat}.
The cones of $\Fan_c(\RS)$ spanned by $\APT{c}$ map to cones of $\nu_c(\Fan_c(\RS))$ that comprise the closure of the complement of the $\g$-vector fan, a codimension-$1$ cone that we call that \newword{imaginary wall}~$\d_\infty$.
The imaginary wall is contained in $\delta^\perp$.
The nonzero maximal cones of $\nu_c(\Fan_c(\RS))$ that are contained in $\d_\infty$ are called the \newword{imaginary cones}.
The vector $\nu_c(\delta)$ spans a ray called the \newword{imaginary ray} that is in every imaginary cone.
This is the only ray of $\nu_c(\Fan_c(\RS))$ that is not spanned by the $\g$-vector of a cluster variable.
The following \lcnamecref{nu delta} is \cite[Lemma~5.9]{affscat}, written in different notation.
(There is a bilinear form $\omega_c(\,\cdot\,,\,\cdot\,)$ on $V$ whose matrix is $B_0$ with respect to the simple coroots basis on the left and the simple root basis on the right.
The statement in \cite{affscat} is phrased in terms of $\omega_c(\,\cdot\,,\delta)$.)

\begin{lemma}\label{nu delta}  
Suppose $B_0$ is an acyclic exchange matrix of affine type.
The vector~$\nu_c(\delta)$ spanning the imaginary ray is $-\frac12B_0\delta$.
\end{lemma}

Since $\mu_{12\cdots n}(B)=B$, the map $\eta^{B_0^T}_{12\cdots n}$ is an automorphism of the fan and separately of the $\g$-vector fan.
The following propositions are part of \cite[Proposition~7.31(4)]{affscat} and \cite[Proposition~7.32]{affscat}

\begin{proposition}\label{eta on dinf}
Suppose $B_0$ is an acyclic $n\times n$ exchange matrix of affine type.
The map $\eta^{B_0^T}_{12\cdots n}$ fixes $\d_\infty$ as a set, agrees with $c$ on $\d_\infty$, and has finite order on $\d_\infty$.
\end{proposition}

\begin{proposition}\label{delta limit eta}
Suppose $x\in V^*\setminus\d_\infty$.
Then for large enough $i$, the action of $\eta_{12\cdots n}^{B^T}$ on $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i(x)$ is the same as the action of~$c$.
Also, $\br{\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i(x),\delta}>0$ for large enough $i$.
There exist integers $m$ and $I$ and a positive real number $a$ such that $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^{i+m}(x)=\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i(x)+a\nu_c(\delta)$ when $i\ge I$.
%Thus 
%\[\lim_{i\to\infty}\frac{\bigl(\eta_{12\cdots n}^{B_0^T}\bigr)^i(x)}{\bigl|\bigl(\eta_{12\cdots n}^{B_0^T}\bigr)^i(x)\bigr|}=\frac{\nu_c(\delta)}{|\nu_c(\delta)|}\]
\end{proposition}

We now prove our first preliminary result about dominance regions in the affine case.

\begin{lemma}\label{P in dinf}
Suppose $B_0$ is acyclic of affine type.
If $\lambda\in\d_\infty$, then $\P^{B_0}_\lambda\subseteq\d_\infty$.
\end{lemma}
\begin{proof}
By definition, $\P^{B_0}_\lambda\subseteq\P^{B_0}_{\lambda,\emptyset}=\set{\lambda+B_0\alpha:\alpha\ge0}$.
Write the entries of $\lambda$ as $(\lambda_1,\ldots,\lambda_n)$.
Since $B_0$ is acyclic and its $1\st$ row has nonnegative entries, in particular every point in $\P^{B_0}_\lambda$ has $1\st$ coordinate greater than or equal to $\lambda_1$.
Also, since $\delta$ has all entries positive and the first row of $B_0$ is nonzero, $B_0\delta$ has $1\st$ coordinate strictly positive.

In \cref{delta limit eta}, we can take $m$ such that $\bigl(\eta^{B_0^T}_{12\cdots n}\bigr)^m$ fixes $\d_\infty$ pointwise (replacing $m$ with the least common multiple of $m$ and the order of $c$ on $\d_\infty$).
Since $\mu_{12\cdots n}(B_0)=B_0$, \cref{shift}.\ref{shift all} says that $\bigl(\eta^{B_0^T}_{12\cdots n}\bigr)^m(\P_\lambda^{B_0})=P_\lambda^{B_0}$.

Now suppose $\P^{B_0}_\lambda$ contains a point $x\not\in\d_\infty$.
Applying \cref{delta limit eta}, we find a sequence of points in $\P^{B_0}_\lambda$ that approach the direction of $\nu_c(\delta)$ with the component in the direction of $\nu_c(\delta)$ increasing without bound.
By \cref{nu delta}, and the fact that $B_0\delta$ has $1\st$ coordinate strictly positive, we conclude that $\P^{B_0}_\lambda$ contains points with $1\st$ coordinate strictly \emph{negative}.
This contradiction shows that $\P^{B_0}_\lambda\subseteq\d_\infty$.
\end{proof}

We conclude this section with a brief discussion of $c$-sortable elements.  
Recalling that $c=s_1\cdots s_n$, write $c^\infty$ for the infinite word $s_1\cdots s_n|s_1\cdots s_n|s_1\cdots s_n|\cdots$.
(The symbols ``$|$'' are not considered as letters of the word, but are \newword{dividers} placed in the word for convenience.)
Every element $w$ of the group $W$ can be written (in many ways) as a subword of $c^\infty$.
Each subword is specified by a finite sequence of positions in $c^\infty$.
Out of all those subwords, the \newword{$c$-sorting word} for $w$ is the subword with the \emph{lexicographically leftmost} sequence of positions.
The $c$-sorting word for $w$ is also specified by a finite sequence of nonempty subsets of $\set{s_1,\ldots,s_n}$, namely the set of letters of the $c$-sorting word that are before the first divider, the set of letters between the first and second dividers, between the second and third dividers, etc.
The element $w$ is \newword{$c$-sortable} if this sequence of subsets is weakly decreasing.
(In other words, working from left to right, once a letter $s_i$ of $c^\infty$ is skipped in the $c$-sorting word, it never appears afterwards.)

The following lemma is immediate from the definition.
\begin{lemma}\label{any prefix}
If $a_1\ldots a_m$ is the $c$-sorting word for a $c$-sortable element, then for any $k\in\set{0,1,\ldots,m}$, the prefix $a_1\cdots a_k$ is the $c$-sorting word for a $c$-sortable element.
\end{lemma}


The $c$-sortable elements and their $c$-sorting words contain a lot of combinatorial data about the mutation fan.
%Each $c$-sortable element determines an $n$-dimensional cone $\Cone_c(v)$ in $V^*$ whose inward-facing normals are certain roots or co-roots in $\RS\subset V$ determined by the $c$-sorting word as follows.
Given a $c$-sortable element $v$ and an index $i\in\set{1,\ldots,n}$, let $a_1\cdots a_k$ be the prefix of the $c$-sorting word for $v$ consisting of all the letters before the first place where $s_i$ is skipped in the $c$-sorting word.
Define $C_c^i(v)$ to be the root $a_1\cdots a_k\alpha_i$ and define $C_c^i(v)\ck$ to be the corresponding co-root $a_1\cdots a_k\alpha\ck_i$.
The skip of $s_i$ in the $c$-sorting word is \newword{unforced} if $a_1\cdots a_ks_i$ is a reduced word.
Otherwise, it is \newword{forced}.
The root $C_c^i(v)$ is positive if and only if the skip is unforced.

For each $c$-sortable element $v$, define a cone  
\[\Cone_c(v)=\bigcap_{\beta \in C_c(v)}\set{x\in V^*:\br{x,\beta} \geq 0}.\]
%(Since each $C_c^i(v)\ck$ is a scaling of $C_c^i(v)$, we could equally well write $\bigcap_{\beta\ck\in C_c(v)\ck}\set{x\in V^*:\br{x,\beta\ck} \geq 0}$).
In general acyclic (but not necessarily affine) type, the union of these cones $\Cone_c(v)$ and their faces, over all $c$-sortable elements $v$, is a fan called the \newword{$c$-Cambrian fan}, which is a subfan of the $\g$-vector fan in $V^*$.
In particular, each $c$-sortable element $v$ specifies a unique seed.
The $C$-vectors of this seed are encoded in the $c$-sorting word for~$v$.
Specifically, the $C$-vector in position $i$ is the integer vector given by the simple-root coordinates of $C_c^i(v)$.
(See \cite[Therem~1.1]{framework} and \cite[Theorem~5.12]{framework}.)
The simple co-root coordinates of $C_c^i(v)\ck$ give the $C$-vector of the seed specified by~$v$ in the cluster algebra with initial exchange matrix $-B^T$.
Thus by \cite[Theorem~1.1]{framework}, the exchange matrix at the seed associated to $v$ has $ij$-entry $\omega_c(C_c^i(v)\ck,C_c^j(v))$.

When $B$ is of finite type (equivalently, when $W$ is finite), the $c$-Cambrian fan coincides with the $\g$-vector fan, but otherwise (when $W$ is infinite), the $c$-Cambrian fan is not complete and is a proper subfan of the $\g$-vector fan.
When $B$ is of affine type (equivalently, when $W$ is of affine type), the $\g$-vector fan is the \newword{doubled $c$-Cambrian fan}: the union of the $c$-Cambrian fan and the image of the $c^{-1}$-Cambrian fan under the antipodal fan. % (more briefly, the Cambrian fan and the \newword{opposite Cambrian fan}).

\begin{proposition}\label{tack on c}
Suppose $W$ is of affine type and suppose $v$ is a $c$-sortable element whose $c$-sorting word starts with $s_1\cdots s_n$.
Then $cv$ is $c$-sortable and its $c$-sorting word is $s_1\cdots s_n$ followed by the $c$-sorting word for~$v$.
\end{proposition}
\begin{proof}
Let $a_1\cdots a_m$ be the $c$-sorting word for $v$.
We first show that $s_na_1\cdots a_m$ is reduced.
The amounts to showing that $\alpha_n\not\in\set{a_1\cdots a_{k-1}\alpha(a_k):k=1,\ldots,m}$.
Suppose to the contrary that $\alpha_n=a_1\cdots a_{k-1}\alpha(a_k)$ for some~$k$, where $\alpha(a_k)$ is the simple root associated to the simple reflection~$a_k$.
Since $\set{a_1\cdots a_{k-1}\alpha(a_k):k=1,\ldots,n}$ equals $\TravProj{c}$ and $\alpha_n\in \TravInj{c}$, we must have $k>n$.
But then \cref{c to pos} says that $a_{n+1}\cdots a_{k-1}\alpha(a_k)$ is negative, contradicting the fact that $a_{n+1}\cdots a_m$ is reduced.

We conclude that $s_na_1\cdots a_m$ is reduced.
Therefore, we see that $s_na_1\cdots a_m$ is the $(s_ncs_n)$-sorting word for $s_nv$ and that $s_nv$ is $(s_ncs_n)$-sortable.
Applying that fact $n$ times implies the proposition.
\end{proof}

\begin{proposition}\label{any B is sort}
Suppose $B_0$ is an acyclic $n\times n$ exchange matrix of affine type and suppose $B$ is mutation equivalent to $B_0$.
Then there exists a $c$-sortable element whose associated seed has exchange matrix $B$.
\end{proposition}
\begin{proof}
Since $B$ is mutation equivalent to $B_0$, taking $B_0$ to be the exchange matrix at $t_0$, there exists a seed $t$ with $B$ as exchange matrix.
Taking $x$ in the interior of $\Cone^{B_0;t_0}_t$, \cref{delta limit eta} says that $\br{\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i(x),\delta}>0$  for large enough $i$.
That is, the automorphism $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i$ of $\nu_c(\Fan_c(\RS))$ maps $\Cone^{B_0;t_0}_t$ to $\Cone^{B_0;t_0}_{t'}$, where~$t'$ is a new seed associated with a $c$-sortable element.
The map $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i$ acts as an initial seed mutation and thus $t'$ has the same exchange matrix $B$ as~$t$.
\end{proof}

\subsection{General seeds}
We now prove some facts about exchange matrices of affine type, without the requirement of acyclicity.
Let $B$ be an exchange matrix of affine type.
Then $B$ is mutation equivalent to some acyclic exchange matrix $B_0$ of affine type.
Mutation maps $\eta_\kk^{B^T}$ are homeomorphisms and isomorphisms of mutation fans and also constitute initial seed mutations of $\g$-vectors.
Thus the following structure carries over from the acyclic seed:
There is a unique \newword{imaginary ray}, the unique ray of the mutation fan that is not spanned by the $\g$-vector of a cluster variable.
An \newword{imaginary cone} of $\F_{B^T}$ is a cone containing the imaginary ray.
The maximal imaginary cones have codimension~$1$.
The mutation fan $\F_{B^T}$ consists of the $\g$-vector fan of $B$ and the imaginary cones.
Write $\d^B_\infty$ for the union of the imaginary cones.
This is a finite union of cones of codimension $1$, and we will show that it is contained in a hyperplane.
Thus we will again call $\d^B_\infty$ the \newword{imaginary wall}.
In this section, we make some general statements about the imaginary ray and the imaginary wall.

First, $\d^B_\infty$ is the image of $\d_\infty$ under the fan isomorphism $\eta_\kk^{B^T}$.
Thus, the following lemma is immediate from \cref{shift,P in dinf}.

\begin{lemma}\label{P in dBinf}
Suppose $B$ is of affine type.
If $\lambda\in\d^B_\infty$, then $\P^B_\lambda\subseteq\d^B_\infty$.
\end{lemma}

Let $B_0$ be an acyclic exchange matrix that is mutation-equivalent to $B$ and consider the cluster pattern with $B_0$ the seed at $t_0$.
We will construct a seed $t$ with exchange matrix $B$ and a green sequence $\kk=k_m\cdots k_1$ for $B_0$ such that $\mu_\kk(B_0)=B$ and then establish some additional properties of $\kk$.

\cref{any B is sort} says there exists a $c$-sortable element $v$ whose associated seed has exchange matrix $B$.
Let $\kk=k_m\cdots k_1$ be the \emph{reverse} of the sequence of indices in the $c$-sorting word for $v$.
Then $\kk$ describes mutation from the initial seed $t_0$ to the seed $t$ associated to~$v$.
Sorting words have the property that each prefix of the $c$-sorting word for a $c$-sortable element is itself the $c$-sorting word of a $c$-sortable element.
Thus each $t_i$ is associated to a $c$-sortable element.
Each $t_{i+1}$ is obtained from $t_i$ by mutating in position $k_{i+1}$, the last letter of the $c$-sorting word associated to $t_{i+1}$.
Since the $c$-sorting word associated to $t_{i+1}$ is reduced, the skip of $s_{k_{i+1}}$ is unforced, and thus the $k_{i+1}$-entry of the $C$-vector at $t_i$ is \emph{positive}.
In other words,~$\kk$ is a green sequence for $B_0$.

\sayN{I feel like we ought to be able to say something simpler, because $\delta^B$ should depend only on $B$, not on a particular seed associated to $B$.  Is it true that $\delta^B=\bigl(G_t^{-B_0^T;t_0}\bigr)^T\delta$ for any $t$ with $B_t=B$?  Even if we have to go through all this to get the properties we want, that would be a much nicer definition of $\delta^B$!}
Write $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
We will show that we can choose $v$ so that $\bigl(G_{t_j}^{-B_0^T;t_0}\bigr)^T\delta$ has nonnegative entries for all $j=0,\ldots,m$, where $\delta$ is the imaginary root associated to~$B_0$.
To do this, we apply \cref{tack on c} to replace $v$ by the $c$-sortable element~$c^iv$ for large enough $i$.  
Since $\mu_{1\cdots n}(B_0)=B_0$, $c^iv$ also has exchange matrix $B$ for~$i\ge0$.
The corresponding sequence is $\kk(n\cdots1)^i$.
By \cite[Proposition~4.6]{afframe}, there are only finitely many $c$-sortable elements $v$ such that the nonnegative span of $G_{t_j}^{-B_0^T;t_0}$ intersects the halfspace $\set{x\in V^*:\br{x,\delta<0}}$. 
Thus we can choose $i$ large enough so that $\bigl(G_{t_j}^{-B_0^T;t_0}\bigr)^T\delta$ has nonnegative entries for all $j\ge in$.
But for $0\le j\le in$, it is apparent from the characterization of $C$-vectors in terms of skips that the columns of $C_{t_j}^{B_0;t_0}$ are the inward-facing normals of the cone obtained by applying $v$ to the dominant chamber.
Now \cite[Theorem~1.2]{NZ} says that $\bigl(C_t^{B_0;t_0}\bigr)^{-1}=\bigl(G_t^{-B_0^T;t_0}\bigr)^T$, so this cone is spanned by the columns of $G_{t_j}^{-B_0^T;t_0}$.
Since the cone is contained in the Tits cone, the columns of $G_{t_j}^{-B_0^T;t_0}$ pair strictly positively with $\delta$.

For this seed $t$ and green sequence $\kk$, define $\delta^B$ to be $\bigl(G_t^{-B_0^T;t_0}\bigr)^T\delta$.  \sayN{I feel like we ought to be able to say something simpler, because $\delta^B$ should depend only on $B$, not on a particular seed associated to $B$.  Is it true that $\delta^B=\bigl(G_t^{-B_0^T;t_0}\bigr)^T\delta$ for any $t$ with $B_t=B$?  Even if we have to go through all this to get the properties we want, that would be a much nicer definition of $\delta^B$.}
The following proposition in particular implies that $\delta^B$ is well defined, in the sense that it depends only on $B$ and not on the specific choices we made.

\begin{proposition}\label{delta is the man}
Suppose $B$ is an exchange matrix of affine type.
\begin{enumerate}[\quad\bf1.]
\item \label{im ray pos}
$\delta^B$ has nonnegative entries.
\item \label{im ray}
$-\frac12B\delta^B$ is the shortest integer vector in the imaginary ray of $\F_{B^T}$.
\item \label{im hyp}
All imaginary cones in $\F_{B^T}$ are contained in $(\delta^B)^\perp$.
\end{enumerate}
\end{proposition}

\begin{proof}%[Proof of \cref{delta is the man}]
Assertion~\ref{im ray pos} is true because we chose $t$ to make it true.

Let $a_1\cdots a_m$ be the $c$-sorting word for $v$.
\cref{any prefix} says that for any $\ell\in{0,1,\ldots,m}$, the prefix $a_1\cdots a_\ell$ is the $c$-sorting word for another $c$-sortable element $v_\ell$.
The corresponding postfix of $\kk$ and the corresponding seed $t_\ell$ have the same properties we used to define $\delta^B$, so taking $B_\ell$ to be the exchange matrix at $t_\ell$, we can use $t_\ell$ to define $\delta^{B_\ell}$.
Thus we can argue by induction on $m$ for the remaining assertions.

Assertion~\ref{im ray} is true by \cref{nu delta} when $m=0$ (so that $t=t_0$ and $B=B_0$) because then $\delta^B=\delta$.
By induction, $-B_{m-1}\delta^{B_{m-1}}$ is the shortest integer vector in the imaginary ray of $\F_{B_{m-1}^T}$.
The mutation map $\eta_{k_m}^{B_{m-1}^T}$ takes the shortest integer vector in the imaginary ray of $\F_{B_{m-1}^T}$ to the shortest integer vector in the imaginary ray of $\F_{B^T}$. 
Thus we check that $\eta_{k_m}^{B_{m-1}^T}(-B_{m-1}\delta^{B_{m-1}})=-B\delta^{B}$.

We first appeal to \cite[Proposition~1.3]{NZ} to say that 
\[\delta^B=\bigl(G_t^{-B_0^T;t_0}\bigr)^T\delta=\bigl(G_{t_{m-1}}^{-B_0^T;t_0}E_{+,k_m}^{B_{m-1}^T}\bigr)^T\delta=F_{-,k_m}^{B_{m-1}}\bigl(G_{t_{m-1}}^{-B_0^T;t_0}\bigr)^T\delta,\]
because $\kk$ is a green sequence, so that column $k_m$ of $C_{t_{m-1}}^{-B_0^T;t_0}$ is positive.
Now \cref{EBF trick} says that $B=E_{-,k_m}^{B_{m-1}}B_{m-1}F_{-,k_m}^{B_{m-1}}$.
Thus since $F_{-,k_m}^{B_{m-1}}$ is its own inverse, 
\[-B\delta^B=-E_{-,k_m}^{B_{m-1}}B_{m-1}\bigl(G_{t_{m-1}}^{-B_0^T;t_0}\bigr)^T\delta=E_{-,k_m}^{B_{m-1}}(-B_{m-1}\delta^{B_{m-1}}).\]
Showing that $\eta_{k_m}^{B_{m-1}^T}(-B_{m-1}\delta^{B_{m-1}})=E_{-,k_m}^{B_{m-1}}(-B_{m-1}\delta^{B_{m-1}})$ amounts to showing that the $k_m$-entry of $-B_{m-1}\delta^{B_{m-1}}$ is nonpositive.

We use \cref{BGCB} to rewrite $-B_{m-1}\delta^{B_{m-1}}=-B_{m-1}\bigl(G_{t_{m-1}}^{-B_0^T;t_0}\bigr)^T\delta$ as $\bigl(C_{t_{m-1}}^{-B_0^T;t_0}\bigr)^T(-B_0\delta)$.
Thus we must show that $\omega_c(C^{k_m}_c(v_{m-1})\ck,\delta)\ge0$. 
Since $C^{k_m}_c(v_{m-1})\ck$ is a positive scalar multiple of $C^{k_m}_c(v_{m-1})$, we need to check that $\omega_c(C^{k_m}_c(v_{m-1}),\delta)\ge0$. 

In the definition of $\delta^B$, we replaced a $c$-sortable element $v$ by the $c$-sortable element~$c^iv$ for large enough $i$.
We can increase $i$ arbitrarily without disturbing the properties we have established for $\kk$.
In particular, $c^jC^{k_m}_c(v_{m-1})=C^{k_m}_c(c^jv_{m-1})$ is a positive root for any $j\ge0$.
Thus \cref{who is pos} says that either $C^{k_m}_c(v_{m-1})$ is $c^p\gamma$ for some $\gamma\in\TravProj{c}$ and $p\ge0$ or $C^{k_m}_c(v_{m-1})$ is in a finite $c$-orbit.
Thus \cref{om del} or \cref{om del fin} says that $\omega_c(C^{k_m}_c(v_{m-1}),\delta)\ge0$. 
We have proved Assertion~\ref{im ray}.

Assertion~\ref{im hyp} is also true when $m=0$.
If $m>0$, then by induction, all imaginary cones in $\F_{B_{m-1}^T}$ are contained in $(\delta^{B_{m-1}})^\perp$.
The imaginary cones in $\F_{B^T}$ are the images, under $\eta^{B^T_{m-1}}_{k_m}$, of the imaginary cones in $\F_{B_{m-1}^T}$.
Every imaginary cone in $\F_{B_{m-1}^T}$ contains the imaginary ray, spanned by $-B_{m-1}\delta^B_{m-1}$.
We have previously shown that the $k_m$ entry of $-B_{m-1}\delta^B_{m-1}$ is nonpositive.
%More specifically, we showed that it is strictly negative unless $C^{k_m}_c(v_{m-1})$ is in a finite $c$-orbit.
Let $\lambda$ be any vector in an imaginary cone of $\F_{B_{m-1}^T}$.

If the $k_m$ entry of $-B_{m-1}\delta^B_{m-1}$ is strictly negative, then the $k_m$ entry of $\lambda$ is nonpositive, and thus $\eta^{B^T_{m-1}}_{k_m}$ acts on $\lambda$ by the same linear map as is acts on $-B_{m-1}\delta^B_{m-1}$, given, in the basis of fundamental weights, by the matrix $E_{-,k_m}^{B_{m-1}}$.
The map taking $\delta^{B_{m-1}}$ to $\delta^B$ is given, in the basis of simple roots, by the matrix $F_{-,k_m}^{B_{m-1}}$.
Recall that the fundamental weights are dual to the simple \emph{co-roots}.
Recall also that the matrix $D$, applied to the simple-root coordinates of a vector, gives the simple co-root coordinates of that vector.
We compute
\begin{align*}
\br{\eta^{B^T_{m-1}}_{k_m}(\lambda),\delta^B}
&=\br{E_{-,k_m}^{B_{m-1}}\lambda,F_{-,k_m}^{B_{m-1}}\delta^{B_{m-1}}}\\
&=(E_{-,k_m}^{B_{m-1}}\lambda)^TDF_{-,k_m}^{B_{m-1}}\delta^{B_{m-1}}\\
&=\lambda^T(E_{-,k_m}^{B_{m-1}})^TDF_{-,k_m}^{B_{m-1}}\delta^{B_{m-1}}\\
&=\lambda^TDF_{-,k_m}^{B_{m-1}}F_{-,k_m}^{B_{m-1}}\delta^{B_{m-1}}=\lambda^TD\delta^{B_{m-1}}=\br{\lambda,\delta^{B_{m-1}}}=0.
\end{align*}

If instead the $k_m$ entry of $-B_{m-1}\delta^B_{m-1}$ is zero then either linear map associated to $\eta^{B^T_{m-1}}_{k_m}$ takes $-B_{m-1}\delta^B_{m-1}$ to $-B\delta^B$, namely $E_{\ep,k_m}^{B_{m-1}}$ for $\ep\in\set{+,-}$.
Taking $\ep$ to be the sign of the $k_m$ entry of $\lambda$, we compute as above with the sign $\ep$ replacing the sign~$-$, to obtain $\br{\eta^{B^T_{m-1}}_{k_m}(\lambda),\delta^B}=0$.
\end{proof}


%\begin{proposition}\label{dBinf conv}
%If $B$ is of affine type, then $\d^B_\infty$ is a convex cone.
%\end{proposition}
%\begin{proof}
%CAN WE DO THIS?  Not sure.
%
%DO WE NEED TO DO THIS?  Not sure.
%
%But it probably will be useful somewhere.
%
%If we don't do it, update the prose above.
%\end{proof}4

\cref{delta is the man}.\ref{im hyp} establishes that $\d^B_\infty$ is contained in a hyperplane.
\cref{delta is the man} also allows us to prove one containment in \cref{affine main}.

\begin{proposition}\label{affine main partial}
Suppose $B$ is an exchange matrix of affine type.
If $\lambda$ is contained in the imaginary wall~$\d^B_\infty$, then the dominance region $\P^B_\lambda$ \emph{contains} the line segment parallel to the imaginary ray, with one endpoint at $\lambda$ and the other endpoint on the relative boundary of $\d^B_\infty$.
\end{proposition}
\begin{proof}
Suppose $\kk$ is a sequence of indices in $\set{1,\ldots,n}$ and let $B'=\mu_\kk(B)$.
Then the mutation map $\eta^{B^T}_\kk$ takes the imaginary ray in~$\F_{B^T}$ to the imaginary ray in~$\F_{(B')^T}$ and takes the imaginary cone of $\F_{B^T}$ containing $\lambda$ to an imaginary cone in~$\F_{(B')^T}$.
Since $\delta^{B'}$ has nonnegative entries, the ray $\set{\eta^{B^T}_\kk(\lambda)+aB'\delta^{B'}:a\ge0}$ is contained in $\set{\eta^{B^T}_\kk(\lambda)+B'\alpha:\alpha\ge0}$.
Since $(\eta_\kk^{B^T})^{-1}$ is linear on each imaginary cone and takes the direction $-B'\delta^{B'}$ of the imaginary ray in $\F_{(B')^T}$ to the direction $-B\delta^B$ of the imaginary ray in $\F_{B^T}$, we see that 
\begin{multline*}
\P^B_{\lambda,\kk}=(\eta_\kk^{B^T})^{-1}\set{\eta^{B^T}_\kk(\lambda)+B'\alpha:\alpha\ge0}\\
\supseteq(\eta_\kk^{B^T})^{-1}\set{\eta^{B^T}_\kk(\lambda)+aB'\delta^{B'}:a\ge0}\cap\d^B_\infty\\
=\set{\lambda+aB\delta^B:a\ge0}\cap\d^B_\infty. 
\end{multline*}
This is true for all $\kk$, so $\P^B_\lambda\supseteq\set{\lambda+aB\delta^B:a\ge0}\cap\d^B_\infty$, as desired.
\end{proof}






%\subsection{Neighboring exchange matrices}
\subsection{Neighboring seeds}
A seed \newword{neighboring the imaginary wall} (usually shortened to a \newword{neighboring seed}) is a seed that has $n-2$ of its $\g$-vectors contained in the imaginary wall.
The designation of a seed as neighboring is independent of the choice of initial seed.
In this section, we characterize the exchange matrices of neighboring seeds, taking advantage of facts that can be obtained in the almost positive Schur roots model.
We will call these exchange matrices \newword{neighboring exchange matrices}.
Since the $\g$-vector fan is the subfan of $\F_{B^T}$ consisting of all full-dimensional cones and their faces, we can describe neighboring exchange matrices as follows:
An exchange matrix of affine type is neighboring if and only if the imaginary cone in $\F_{B^T}$ shares $n-2$ rays with the imaginary wall in $\F_{B^T}$.

\begin{lemma}\label{neigh neg and T}
The property of an exchange matrix being of affine type and neighboring is preserved by transpose and preserved by negating the matrix.
\end{lemma}
\begin{proof}
The property of an exchange matrix being of affine type is preserved by transpose because the property of a Cartan matrix being of affine type is preserved by transpose.
The property of an exchange matrix being of affine type is preserved by negating the matrix because negating does not change the underlying Cartan matrix.

We have already seen that $-B^T$ is a rescaling of $B$, so $-B$ is a rescaling of $B^T$.
Thus \cite[Proposition~7.8(3)]{universal} says that $\F_{-B}$ and $\F_{B^T}$ are related by a linear map.
By \cite[Proposition~7.1]{universal}, $\F_B$ and $\F_{-B}$ are related by the antipodal map and $\F_{-B^T}$ and $\F_{B^T}$ are related by the antipodal map.
We see that negating and transposing preserves the property of an affine exchange matrix being neighboring.
\end{proof}

We will call column $i$ of an exchange matrix $B$ a \newword{quasi-leaf} if column $i$ has at most two nonzero entries, and, if column $i$ has nonzero entries in distinct indices $j$ and $k$, then the restriction of $B$ to rows and columns $i$, $j$, and $k$ is $\pm\begin{bsmallmatrix*}[r]0&1&-1\\-1&0&1\\1&-1&0\end{bsmallmatrix*}$.
It will also be convenient, in describing block decompositions of matrices, to allow \newword{empty matrices}, meaning matrices that have $0$ columns and/or $0$ rows while possibly formally having a nonzero number of rows or columns.
Thus, for example, a ``$0\times 2$ matrix''.)

\begin{theorem}\label{neigh B}
Suppose $B$ is an exchange matrix of affine type.
Then the following conditions are equivalent.
\begin{enumerate}[\quad\rm(i)]
\item \label{neigh}
$B$ is a neighboring exchange matrix.
\item \label{aff 2}
There exist indices $i$ and $j$ such that $\begin{bsmallmatrix}0&b_{ij}\\b_{ji}&0\end{bsmallmatrix}$ is of affine type.
\item \label{neigh detailed}
Up to relabeling, $B$ is
    $
      \begin{bsmallmatrix}
        B_{11} 	& 0 		& 0 		& B_{14} \\ 
        0 		& B_{22} 	& 0 		& B_{24} \\
        0 		& 0 		& B_{33} 	& B_{34} \\
        B_{41} 	& B_{42} 	& B_{43} 	& B_{44}
      \end{bsmallmatrix},
    $ 
where $B_{44}$ is a rank-$2$ exchange matrix of affine type and the following conditions hold for ${\ell\in\set{1,2,3}}$:
\begin{itemize}
\item
$B_{\ell\ell}$ is either a $0\times0$ block or an exchange matrix of finite type A; % (indexed with $B_{11}$, $B_{22}$, and $B_{33}$ in order of increasing size),
\item
If $B_{\ell\ell}$ is nonempty, then its last column is a quasi-leaf;
\item
If $B_{\ell4}$ is nonempty, then it is nonzero only in the last row;
\item
If $B_{4\ell}$ is nonempty, then it is nonzero only in the last column; and
\item
If $B_{\ell4}$ is nonempty (equivalently if $B_{4\ell}$ is nonempty), then the nonzero rows and columns in $\begin{bsmallmatrix} 0 & B_{\ell4} \\ B_{4\ell} & B_{44} \end{bsmallmatrix}$ form one of the matrices in \cref{submat tab}.
\end{itemize}
The labeling can be taken so that the blocks $B_{11}$, $B_{22}$, and $B_{33}$ are in order of increasing size. 
A submatrix in \cref{submat tab} of type $A_{4}^{(2)}$, $G_{2}^{(1)}$, or $D_{4}^{(3)}$ can only appear when $B_{11}$ and $B_{22}$ are empty.
\end{enumerate}
\end{theorem}
	\begin{table}
	\caption{Possible submatrices}
	\label{submat tab}	
	\begin{center}
	\begin{tabular}{|cc|cc|}
	Type & matrix & Type & matrix \\
	\hline & & & \\[-1ex]
	$A_{2}^{(1)}$ & $\left[\begin{array}{rrr}
	0 & 1 & -1 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
	\end{array}\right]$ & & \\[4ex]
	$C_{2}^{(1)}$ & $\left[\begin{array}{rrr}
	0 & 2 & -2 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
	\end{array}\right]$ &
	$D_{3}^{(2)}$ & $\left[\begin{array}{rrr}
	0 & 1 & -1 \\
	-2 & 0 & 2 \\
	2 & -2 & 0
	\end{array}\right]$ \\[4ex]
	$G_2^{(1)}$ & $\left[\begin{array}{rrr}
	0 & 3 & -3 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
	\end{array}\right]$ &
	$D_4^{(3)}$ & $\left[\begin{array}{rrr}
	0 & 1 & -1 \\
	-3 & 0 & 2 \\
	3 & -2 & 0
	\end{array}\right]$ \\[4ex]
	$A_{4}^{(2)}$ & $\left[\begin{array}{rrr}
	0 & 1 & -2 \\
	-2 & 0 & 4 \\
	1 & -1 & 0
	\end{array}\right]$ &
	$A_{4}^{(2)}$ & $\left[\begin{array}{rrr}
	0 & 2 & -1 \\
	-1 & 0 & 1 \\
	2 & -4 & 0
	\end{array}\right]$ 
	\end{tabular}
	\end{center}	
	\end{table}

%We now prepare to prove \cref{neigh B} as a series of propositions.  
Fix an acyclic $n\times n$ exchange matrix $B_0$ of affine type that is mutation equivalent to $B$ and let $c$ be the corresponding Coxeter element.
It is apparent that condition~\eqref{neigh detailed} implies condition~\eqref{aff 2} in \cref{neigh B}, and the following proposition proves that condition~\eqref{aff 2} implies condition~\eqref{neigh}.
\begin{proposition}\label{aff 2 block}
If $B$ is of affine type and there are indices $i$ and $j$ such that $\begin{bsmallmatrix}0&b_{ij}\\b_{ji}&0\end{bsmallmatrix}$ is of affine type, then $B$ is a neighboring exchange matrix.
\end{proposition}

\begin{proof}
We use \cite[Corollary~4.18]{afframe}, which applies to the acyclic affine exchange matrix $B_0$.
The notation $\DF_c$ in \cite{afframe} refers to the doubled Cambrian fan, which coincides with the $\g$-vector fan \cite[Corollary~1.3]{afframe}.
\cite[Corollary~4.18]{afframe} says, among other things, that if an $(n-2)$-dimensional cone $F$ of the $\g$-vector fan of $B_0$ is contained in infinitely many maximal cones of the $\g$-vector fan, then $F$ is in the boundary of the support of the $\g$-vector fan.
Thus $F$ is also in the imaginary wall $\d$, and since $F$ is a cone in the $\g$-vector fan, it is in the \emph{boundary} of~$\d$.

Now, suppose some $\begin{bsmallmatrix}0&b_{ij}\\b_{ji}&0\end{bsmallmatrix}$ is of affine type (and in particular of infinite type).
Then, in the $\g$-vector fan of $B$, there are infinitely many maximal cones of the $\g$-vector fan that contain the cone $F$ spanned by the $\g$-vectors $\set{\e_k:k\not\in\set{i,j}}$.
There is a mutation map $\eta$ that is an isomorphism from the $\g$-vector fan for $B$ to the $\g$-vector fan for $B_0$.
The image of the positive cone under $\eta$ is a maximal cone in the $\g$-vector fan for $B_0$ whose associated seed $t$ has exchange matrix $B$.
The image of $F$ under $\eta$ is an $(n-2)$-dimensional cone in the $\g$-vector fan for $B_0$ that is contained in infinitely many maximal cones of the $\g$-vector fan.
Thus the image of $F$ is in the boundary of the imaginary wall.
Since the image of the positive cone contains the image of $F$, we conclude that $t$ is a neighboring seed, so that $B$ is a neighboring exchange matrix.
\end{proof}

It remains to show that condition~\eqref{neigh} implies condition~\eqref{neigh detailed}.
Assume that $B$ is neighboring.
Then there is a seed $t$ with exchange matrix $B$ such that $n-2$ of the $\g$-vectors at $t$ are in the boundary of $\d$.
We use \cref{any B is sort} to choose a seed $t$ that corresponds to a $c$-sortable element~$v$.
(There is a subtle point that we might worry about here:
Conceivably, there exists $t$ with $\g$-vectors as described, but the seed $t'$ produced by \cref{any B is sort} does not have $\g$-vectors as described.
However, in the proof of  \cref{any B is sort}, $t'$ is obtained from $t$ by applying automorphisms of the mutation fan, and therefore preserves the property that $\Cone_t^{B_0;t_0}$ has $(n-2)$-dimensional intersection with the imaginary wall.

We reindex the rows and columns of $B_0$ so that $\set{\xi_i:i\in[1,n-2]}$ are the $\g$-vectors in the boundary of $\d$ and $\set{\xi_{n-1},\xi_n}$ are the $\g$-vectors not contained in~$\d$.
Thus the vectors $\set{\xi_i:i\in[1,n-2]}\cup\set{-B_0\delta}$ span an imaginary cone in $\F_{B_0^T}$.

Consider the set of full-dimensional cones of $\F_{B_0^T}$ that have rays spanned by the vectors $\xi_1,\ldots,\xi_{n-2]}$. 
This set is infinite.
(Perhaps the fastest way to see that:  If the set were finite, the union of these full-dimensional cones would intersect the relative interior of an imaginary cone, contradicting the fact that $\F_{-B_0^T}$ is a fan.)
Thus the entries of $B$ satisfy $b_{(n-1)n}b_{n(n-1)}\le-4$.
But also, $B$ is mutation finite, and therefore $b_{(n-1)n}b_{n(n-1)}\ge-4$.
We see that $B_44$ is a rank-$2$ exchange matrix of affine type.
If necessary, we reindex (swapping positions $n-1$ and $n$) so that $b_{(n-1)n}>0$.

There are vectors $\gamma_1,\ldots,\gamma_n\in\APre{c}$ such that $\nu_c(\gamma_i)=\xi_i$ for $i=1,\ldots,n$.
The vectors $\gamma_1,\ldots,\gamma_{n-2}$ are in $\APTre{c}$.
These roots are pairwise $c$-compatible and $\set{\gamma_1,\ldots,\gamma_{n-2}}\cup\set{\delta}$ is an imaginary $c$-cluster.
In particular,  $\gamma_1,\ldots,\gamma_{n-2}$ are all in $\RST{c}$.
Choose integers $0\leq p_1\leq p_2 \leq p_3$ such that the nonzero values $p_\ell$ record the components of $\RST{c}$, with a component of rank $p_\ell+1$ for each nonzero $p_\ell$.
As a consequence of \cref{compatible in tubes}, we see that $\set{\gamma_1,\ldots,\gamma_{n-2}}$ consists of $p_\ell$ roots from the $\ell\th$ component for each $\ell=1,2,3$.
We further reindex the rows and columns of $B$ so that each subset $\{\gamma_1,\dots,\gamma_{p_1}\}$, $\{\gamma_{p_1+1},\dots,\gamma_{p_1+p_2}\}$, and $\{\gamma_{p_1+p_2+1},\dots,\gamma_{p_1+p_2+p_3}\}$ consists of roots from the same component of $\RST{c}$.
Furthermore, if $p_1>0$, then there is a unique root in $\{\gamma_1,\dots,\gamma_{p_1}\}$ that is a sum of $p_1$ elements of $\SimplesT{c}$, and we give this the index $p_1$.
Similarly, if $p_2>0$, we make $p_1+p_2$ the index of the unique root in $\{\gamma_{p_1+1},\dots,\gamma_{p_1+p_2}\}$ that is a sum of $p_2$ elements of $\SimplesT{c}$.
In any case, we make $p_1+p_2+p_3=n-2$ the index of the root in $\{\gamma_{p_1+p_2+1},\dots,\gamma_{p_1+p_2+p_3}\}$ that is a sum of $p_3$ elements of $\SimplesT{c}$.
After this reindexing, we consider the decomposition of $B$ into blocks $B_{ij}$ given by the composition $(p_1+p_2+p_3+2)$ of $n$.

We define some terminology.
A \newword{special index} is the index of the last column/row of $B$ (if $B_{\ell\ell}$ is nonempty) for $\ell=1,2,3$.
There are $1$, $2$, or $3$ special indices, namely whichever of $p_1$, $p_1+p_2$ and $p_1+p_2+p_3=n-2$ are nonzero.
The two indices of $B_{44}$ are the \newword{affine indices} and call $B_{44}$ the \newword{affine submatrix} of~$B$.  \sayS{I think that affine indices is a misleading terminology: it can be confusing
with the affine index of the root system.}
%The \newword{tube components} of $\set{1,\ldots,n}$ are whichever of the sets $\set{1,\ldots,p_1}$, $\set{p_1+1,\ldots,p_1+p_2}$, and $\set{p_1+p_2+1,\ldots,n-2}$ are nonempty.

We next prove, for $\ell=1,2,3$, that if $p_\ell>0$, then $B_{\ell\ell}$ is an exchange matrix of finite type $A_{p_\ell}$ and the special index in $B_{\ell\ell}$ is a quasi-leaf.
We do that by comparing the combinatorics of exchange relations described by $B_{\ell\ell}$ with the combinatorics of triangulated polygons.

Write $\beta_0,\beta_1,\ldots,\beta_{p_\ell}$ for the simple roots of the component of $\RST{c}$ associated to $B_{\ell\ell}$, numbered so that $\beta_0$ is the unique one of these $\beta$ that does not occur in the support of $\gamma_i$ for any index $i$ of $B_{\ell\ell}$. 
Then every root $\gamma_i$ is $\beta_{q_ir_i}=\beta_{q_i}+\beta_{q_i+1}+\cdots+\beta_{r_i}$ for some $1\le q_i\le r_i\le p_\ell$.
For the special index $i$, $\gamma_i$ is $\beta_{1\ell}$.
%\cref{compatible in tubes} says that roots
We can represent each $\gamma_i$ pictorially as in \cref{tube fig 1}, by circling the nodes $q_i$ through $r_i$ on a cycle with nodes labeled $0,1,\ldots,p_\ell$.

We write $x_{qr}$ for the principal-coefficients cluster variable with denominator vector $\beta_{qr}$.
The denominator vector of $x_{qr}$ is $\beta_{qr}$, so the $\g$-vector of $x_{qr}$ is $\nu_c(\beta_{qr})$.
The map $\nu_c$ is linear on the subspace spanned by roots in finite orbits.
\cref{compatible in tubes} says that two of these cluster variables can be in the same cluster if and only if the corresonding roots are nested or spaced.
There is a similar characterization of which pairs of these cluster variables are exchangeable.
Since none of these roots have $\beta_0$ in their support, \cite[Theorem~7.2]{affdenom} says that the cluster variables associated to non-special indices in $B_{\ell\ell}$ are exchangeable if and only if the corresponding roots are adjacent or have overlapping support but are not nested.

Thus two exchangeable cluster variables $x_{qr}$ and $x_{q'r'}$ have, without loss of generality, $q<q'<r$, $q<r<r'$, and $q'\le r+1$.
In light of \cref{exch ind}, the only cluster variables that can occur in the exchange relation for $x_{qr}$ and $x_{q'r'}$ are the cluster variables that are in \emph{every} set $\Gamma$ of $n-1$ cluster variables such that $\Gamma\cup\set{x_{qr}}$ is a cluster and $\Gamma\cup\set{x_{q'r'}}$ is a cluster.
These are the cluster variables $x_{qr'}$, $x_{q'r}$, $x_{q(q'-2)}$, and $x_{(r+2)r'}$, except that the last three only exist if, respectively, $q'\le r$, $q\le q'-2$, or $r'\ge r+2$.

In the proof of \cref{exch ind}, we saw that the exchange relation for $x_{qr}$ and $x_{q'r'}$ has a term with no coefficient variables whose $\g$-vector is the sum of the $\g$-vectors of $x_{qr}$ and $x_{q'r'}$.
Since $\nu_c$ is linear on these cluster variables, that monomial must be $x_{qr'}x_{q'r}$.
The other term in the exchange relation is therefore $y^\phi x_{q(q'-2)}^ax_{(r+2)r'}^b$, where $y^\phi$ is the monomial in the coefficient variables with exponent vector $\phi$ (and this vector has nonnegative entries).
The $\g$-vector of $y^\phi$ is $B_0\phi$.
Since the $\g$-vectors of all cluster variables in the exchange relation are in $\delta^\perp$ and since the exchange relation is homogeneous in the $\g$-vector grading, we see that $B_0\phi$ is also in $\delta^\perp$.
In other words, $\omega_c(\delta,\phi)=0$, so \cref{om del fin} says that $\phi$ is in a finite $c$-orbit.

Write $E_c$ for the $n\times n$ matrix whose $ij$-entry is~$(B_0)_{ij}$ if $i>j$, is $1$ if $i=j$, or is $0$ if $i<j$.
Similarly, write $E_{c^{-1}}$ for the $n\times n$ matrix with $ij$-entry $0$ if $i>j$, is $1$ if $i=j$, and is $-b_{ij}$ if $i<j$.
Then $B=E_c-E_{c^{-1}}$.
The piecewise linear map $\nu_c$, when applied to vectors with nonnegative entries, is defined by the matrix $E_c$ (taking a column vector of simple-root coordinates to a column vector of fundamental-weight coordinates).
Furthermore, a result of Howlett \cite[Theorem~2.1]{Howlett} (see also \cite[Theorem~2.6]{affdenom}) says that the matrix for $c$, as it acts on the basis of simple roots, is~$-E_{c^{-1}}^{-1}E_c$.
We use these facts about $E_c$ to rewrite the $\g$-vector of $y^\phi$ as 
\[B_0\phi=(E_c-E_{c^{-1}})\phi=(E_c-E_cc^{-1})\phi=\nu_c(1+c^{-1})\phi.\]
Thus because the exchange relation is homogeneous, the vector $(1+c^{-1})\phi$ equals $\beta_{qr}+\beta_{q'r'}-a\beta_{q(q'-2)}-b\beta_{(r+2)r'}$.
We conclude that $a=b=1$ and that $\phi=\beta_{q'(r+1)}$.

We have written the exchange relation between for $x_{qr}$ and $x_{q'r'}$ as 
\[x_{qr}x_{q'r'}=x_{qr'}x_{q'r}+y^{\beta_{q'(r+1)}}x_{q(q'-2)}x_{(r+2)r'}.\]
Up to a global change of sign, these exchange relations determine the entries $b_{ij}$ of $B_{\ell\ell}$ for non-special~$j$.
Furthermore, appealing to \cref{neigh neg and T}, we know that $B^T$ is also neighboring.
In particular, the argument to this point implies that the entries of $B_{\ell\ell}^T$ in non-special columns are $0$, $1$, or $-1$.
We see in particular that $B_{\ell\ell}$ has only entries $0$, $1$, or $-1$ and thus is skew-symmetric. %, so we have also determined the entries $b_{ij}$ of $B_{\ell\ell}$ for special $j$.

%we can write down the entries $b_{ij}$ of $B_{\ell\ell}$ for non-special~$j$:
%For $\gamma_j=\beta_{q_jr_j}$, among roots $\gamma_k\neq\gamma_j$ with $q_k\le q_j\le r_j\le r_k$, there is a unique $k$ that minimizes $r_k-q_k$, and for this $k$, either $q_k=q_j\le r_j< r_k$ or $q_k<q_j\le r_j=r_k$.
% ...
%there is a unique $m_j\in[q_j,r_j]$ such that every $\gamma_k$ with $\SuppT(\gamma_k)\subseteq\SuppT(\gamma_j)$ has $\beta_{m_j}\not\in\SuppT(\gamma_k)$.
%There is also a unique largest $m'_j$ such that there exists $\gamma_k=\beta_{(r_j+2)m'_j}$ (allowing $m'_j=r_j+1$ so that no such $\gamma_k$ exists).
%Then the entry $b_{ij}$ is $1$ if $\gamma_i=\beta_{q_j(m_j-1)}$ or $\gamma_i=\beta_{(r_j+2)m'_j}$, and $b_{ij}$ is $-1$ if $\gamma_i=\beta_{q_jm'_j}$ or $\gamma_i=\beta{(m_j+1)r_j}$, and otherwise $b_{ij}=0$.
%Now, appealing to \cref{neigh neg and T}, we know that $B^T$ is also neighboring.
%In particular, the argument to this point implies that the entries of $B_{\ell\ell}^T$ in non-special columns are $0$, $1$, or $-1$.
%We see in particular that $B_{\ell\ell}$ has only entries $0$, $1$, or $-1$ and thus is skew-symmetric.
%If $j$ is special, then $q_j=1$ and $r_j=p_\ell$.
%Defining $m_j$ as before (and ignoring $m'_j$), we see that $b_{ij}$ is $1$ if $\gamma_i=\beta_{q_j(m_j-1)}$, and $b_{ij}$ is $-1$ if $\gamma_i=\beta{(m_j+1)r_j}$, and otherwise $b_{ij}=0$.
%In particular, we have shown that the special index $j$ is a quasi-leaf.
%
%
%BLACK=NEGATIVE if I take the LEFT RED!

On the other hand, consider a $(p_\ell+3)$-gon with vertices labeled $0,1,\ldots,p_\ell+2$ and as initial triangulation $T_0$, take all diagonals with an endpoint at $p_\ell+2$.
The roots $\gamma_i$ specify a triangulation $T$ that doesn't contain any of these initial arcs.
For $i$ not special, $\gamma_i=\beta_{q_i,r_i}$ corresponds to the diagonal from $q_i-1$ to $r_i+1$, i.e. the diagonal that crosses the initial diagonals with endpoints $q_i,q_i+1,\ldots,r_i$ and no other initial diagonals.  
The diagonal associated to $\gamma_i$ is contained in a quadrilateral in~$T$.
If the other diagonal is specified by $\gamma'_i$, then up to swapping primed and unprimed indices, the exchange relation is exactly what was given above.
We conclude that, up to a global sign, the entries of  $B_{\ell\ell}$ in non-special columns agree with the entries of the signed adjacency matrix of $T$.
Since both matrices are skew-symmetric, they are therefore equal.
We have shown that $B_{\ell\ell}$ is of finite type~A.

%\begin{lemma}\label{C good}
%Given $i\in\set{1,\ldots,n-2}$, the root $C^i_c(v)$ is in a finite $c$-orbit and is in the same component of $\RST{c}$ as $\gamma_i$.
%\end{lemma}
%\begin{proof}
%We prove the equivalent statement about co-roots $C^i_c(v)\ck$.
%(The two statements are equivalent because $C^i_c(v)\ck$ is a positive scalar multiple of $C^i_c(v)$.)
%The $\g$-vectors $\xi_1,\ldots,\xi_n$ at the seed $t$ are the basis of $V^*$ that is dual to $C^1_c(v)\ck,\ldots C^n_c(v)\ck$.
%(Recall that the basis of fundamental weights in $V^*$ is dual to the basis of simple co-roots in $V$.)
%\cref{eta on dinf} implies that $\x_1,\ldots,\x_{n-2}$ are in finite $c$-orbits.
%
%
%
%
%
%\cref{tack on c} and the definition of $C^i_c(v)\ck$ let us apply positive powers of $c$ to $C^i_c(v)\ck$ by replacing $v$ with $c^kv$ for $k>0$.
%
%Since the $\g$-vectors 
%
%FINISH CHECKING that it's in a finite $c$-orbit!
%
%
%Having proved that $C^i_c(v)\ck$ is in a finite $c$-orbit (and thus $C^i_c(v)\ck$ is in a finite $c$-orbit), we now check that it is in the same component of $\RST{c}$ as $\gamma_i$.
%Real roots in different components are orthogonal in the sense of the bilinear form $K$ (determined by the Cartan matrix $A_0$).
%
%
%
%
%\end{proof}
%
%
%We now prove, for $\ell=1,2,3$, that if $p_\ell>0$, then $B_{\ell}$ is skew-symmetric.
%First, \cref{C good} implies that all indices $i$ of $B_{\ell\ell}$ have $C^i_c(v)$ in the same component of $\RST{c}$.
%Since each component of $\RST{c}$ is of affine type A, in particular all of the roots $C^i_c(v)$ for indices $i$ of $B_{\ell\ell}$ are the same length, so the associated co-roots $C^i_c(v)\ck$ are obtained from $C^i_c(v)$ by the same scale factor.
%The entries of $B_{\ell\ell}$ are $\omega_c(C_c^i(v)\ck,C_c^j(v))$, and skew-symmetry follows.
%
%We next prove, for $\ell=1,2,3$, that if $p_\ell>0$, then $B_{\ell\ell}$ is an exchange matrix of type $A_{p_\ell}$.
%We do that by comparing the combinatorics of exchange relations described by $B_{\ell\ell}$ with the combinatorics of triangulated polygons.

Since the exchange relations to exchange out the cluster variable with non-special indices in $B_{\ell\ell}$ only involve cluster variables associated to indices in $B_{\ell\ell}$, we see that $b_{ij}=0$ whenever one of $i$ or $j$ is a non-special index in $B_{\ell\ell}$ and the other is not an index in $B_{\ell\ell}$.
In other words, the blocks $B_{k\ell}$ with $k\neq\ell\in[1,2,3]$ can only have a non-zero entry in the bottom right corner (indexed by the two special indices).
Similarly blocks $B_{\ell4}$ for $\ell\in[1,2,3]$ can only be non-zero in the last row and blocks $B_{4\ell}$ for $\ell\in\set{1,2,3}$ can only be non-zero in the last column.

We show that if $B_{\ell4}$ is nonempty, then it is nonzero, and equivalently $B_{4\ell}$ is nonzero.
Let $k$ be the special index associated to $B_{\ell\ell}$.
Continuing notation from above, $\SuppT(\gamma_k)$ is $\set{\beta_1,\ldots,\beta_{p_\ell}}$, so \cite[Theorem~7.2]{affdenom} says that the cluster variable $x_{1p_\ell}$ for $\xi_k$ is not exchangeable with any other cluster variable whose $\g$-vector is in the imaginary wall $\d^B_\infty$.
(In the language of that theorem, if $\gamma_k$ is $c$-exchangeable with any other root in $\APTre{c}$, then that root has $\beta_0$ is its support, and therefore is not $c$-real-exchangeable with $\gamma_k$.)
Therefore the exchange relation for $x_{1p_\ell}$ exchanges it with a cluster variable whose $\g$-vector is not in $\d^B_\infty$.  
In particular, the right side of that exchange relation can't involve only cluster variables in indexed by non-affine indices, and therefore $B_{\ell4}$ has at least one nonzero entry in column $k$.

We now show that submatrix with rows and columns indexed by $k,n-1,n$ is mutation-finite.
To make the notation more compact, we use $p$ to stand for $n-1$.
Since $B$ is of affine type, it is mutation finite.
Therefore \cref{mut fin sub} says that the submatrix with rows and columns indexed by $k,p,n$ is mutation-finite.
That submatrix has a pair of opposite off-diagonal entries whose product is $-4$, so its underlying Cartan matrix is not of finite or affine type.
(There are no $3\times3$ Cartan matrices of finite or affine type with opposite off-diagonal entries whose product is $4$.)
\cref{acyc mut fin} thus says that the submatrix is not acyclic.
In other words, $b_{kp}>0$ and $b_{kn}<0$, and therefore $b_{pk}<0$ and $b_{nk}>0$.
By \cref{mut fin 2x2}, each of these entries has absolute value $1$, $2$, $3$, or~$4$.

We deal with the case where the affine submatrix has entries $\pm2$.
Below, we display some mutations of the submatrix.
We will derive some inequalities from the fact that each of the mutations must also be non-acyclic and have opposite off-diagonal entries with products at least $-4$.
\begin{multline*}
\begin{bsmallmatrix*}
	0 & b_{kp} & b_{kn} \\
	b_{pk} & 0 & 2 \\
	b_{nk} & -2 & 0
\end{bsmallmatrix*}
\overset{p}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -b_{kp} & 2b_{kp}+b_{kn} \\
	-b_{pk} & 0 & -2 \\
	2b_{pk}+b_{nk}& 2 & 0
\end{bsmallmatrix*}
\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & 3b_{kp}+2b_{kn} & -2b_{kp}-b_{kn} \\
	3b_{pk}+2b_{nk} & 0 & 2 \\
	-2b_{pk}-b_{nk} & -2 & 0
\end{bsmallmatrix*}\\
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -3b_{kp}-2b_{kn} & 2b_{kp}+b_{kn} \\
	-3b_{pk}-2b_{nk} & 0 & 2+(2b_{kp}+b_{kn})(3b_{pk}+2b_{nk}) \\
	2b_{pk}+b_{nk} & -2-(2b_{pk}+b_{nk})(3b_{kp}+2b_{kn})& 0
\end{bsmallmatrix*}
\end{multline*}
\begin{multline*}
\begin{bsmallmatrix*}
	0 & b_{kp} & b_{kn} \\
	b_{pk} & 0 & 2 \\
	b_{nk} & -2 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & b_{kp}+2b_{kn} & -b_{kn} \\
	b_{pk}+2b_{nk} & 0 & -2 \\
	-b_{nk} & 2 & 0
\end{bsmallmatrix*}\overset{p}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -b_{kp}-2b_{kn} & 2b_{kp}+3b_{kn} \\
	-b_{pk}-2b_{nk} & 0 & 2 \\
	2b_{pk}+3b_{nk} & -2 & 0
\end{bsmallmatrix*}
\end{multline*}
The fact that each of these matrices are non-acyclic determines the sign of each entry.
Two of these inequalities are shown here, with an inequality that follows:
\[
3b_{kp}+2b_{kn}>0,\quad 2b_{kp}+3b_{kn}<0\quad \implies\quad b_{kn}>-\frac32b_{kp}>\frac94b_{kn}\\
\]
This rules out all pairs $(b_{kp},b_{kn})$ except $(1,-1)$, $(2,-2)$, $(3,-3)$, $(3,-4)$, and $(4,-4)$.
If $b_{kn}=-4$, then $b_{nk}=1$, and also $b_{kp}\ge3$ so also $b_{pk}=-1$.
Skew-symmetrizability of $B$ implies that $\frac{b_{pk}}{b_{kp}}=\frac{b_{nk}}{b_{kn}}$, which rules out the possibility that $(b_{kp},b_{kn})=(3,-4)$.
Similar considerations starting with the inequalities $3b_{pk}+2b_{nk}<0$ and $2b_{pk}+3b_{nk}>0$ rule out all pairs $(b_{pk},b_{nk})$ except $(-1,1)$, $(-2,2)$, $(-3,3)$, and $(-4,4)$.

We see that, in every case, $b_{kp}=-b_{kn}>0$ and $b_{pk}=-b_{nk}<0$.
To verify that all possibilities are listed in \cref{submat tab}, it remains to rule out the cases where $(b_{kp},b_{kn},b_{pk},b_{nk})$ are $(1,-1,-4,4)$, $(2,-2,-2,2)$, or $(4,-4,-1,1)$.
None of these three cases are possible because, as explained in \cref{growth sec}, these submatrices would  have exponential growth and thus contradict the fact that $B$ has linear growth.

Next, we deal with the case where the affine submatrix has entries $-1$ and $4$.
\begin{multline*}
\begin{bsmallmatrix*}
	0 & b_{kp} & b_{kn} \\
	b_{pk} & 0 & 4	 \\
	b_{nk} & -1 & 0
\end{bsmallmatrix*}
\overset{p}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -b_{kp} & 4b_{kp}+b_{kn} \\
	-b_{pk} & 0 & -4 \\
	b_{pk}+b_{nk}& 1 & 0
\end{bsmallmatrix*}
\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & 3b_{kp}+b_{kn}& -4b_{kp}-b_{kn} \\
	3b_{pk}+4b_{nk} & 0 & 4 \\
	-b_{pk}-b_{nk}& -1 & 0
\end{bsmallmatrix*}\\
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -3b_{kp}-b_{kn}& 4b_{kp}+b_{kn} \\
	-3b_{pk}-4b_{nk} & 0 & 4+(3b_{pk}+4b_{nk})(4b_{kp}+b_{kn}) \\
	b_{pk}+b_{nk}& -1-(b_{pk}+b_{nk})(3b_{kp}+b_{kn}) & 0
\end{bsmallmatrix*}
\end{multline*}
\begin{multline*}
\begin{bsmallmatrix*}
	0 & b_{kp} & b_{kn} \\
	b_{pk} & 0 & 4	 \\
	b_{nk} & -1 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & b_{kp}+b_{kn} & -b_{kn} \\
	b_{pk}+4b_{nk} & 0 & -4	 \\
	-b_{nk} & 1 & 0
\end{bsmallmatrix*}\overset{p}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -b_{kp}-b_{kn} & 4b_{kp}+3b_{kn} \\
	-b_{pk}-4b_{nk} & 0 & 4	 \\
	b_{pk}+3b_{nk} & -1 & 0
\end{bsmallmatrix*}
\end{multline*}
These matrices determine inequalities, including
\[
3b_{kp}+b_{kn}>0,\quad 4b_{kp}+3b_{kn}<0\quad \implies\quad b_{kn}>-\frac32b_{kp}>\frac94b_{kn},\\
\]
which rules out all pairs $(b_{kp},b_{kn})$ except $(1,-2)$, $(2,-3)$, and $(2,-4)$.
If $b_{kn}=-4$, then $b_{nk}=1$, and also $b_{kp}\ge3$ so also $b_{pk}=-1$.
In this case, skew-symmetrizability of $B$ says that $\frac{b_{pk}}{b_{kp}}=4\frac{b_{nk}}{b_{kn}}$, which rules out the possibility that $(b_{kp},b_{kn})=(2,-3)$.
Similar considerations starting with $b_{pk}+3b_{nk}>0$ and $3b_{pk}+4b_{nk}<0$ rule out all pairs $(b_{pk},b_{nk})$ except $(-2,1)$ and $(-4,2)$.
The remaining possibilities for $(b_{kp},b_{kn},b_{pk},b_{nk})$ are $(1,-2,-2,1)$, $(1,-2,-4,2)$, and $(2,-4,-2,1)$, but we rule out the second and third possibilities because they would imply exponential growth.

The last case, where the affine submatrix has entries $-4$ and $1$, is related to the previous case by passing to the negative transpose, so the same calculations lead to the desired result.

If the submatrix is of type $A_{4}^{(2)}$ in \cref{submat tab}, then the underlying Cartan matrix defines a root system with 3 distinct root lengths, and thus $\RS$ is of type $A_{2l}^{(2)}$ in the notation of \cite[Chapter~4]{Kac}.
In this case, the root system can be rescaled to be of type $C^{(1)}$, and therefore $\RST{c}$ has only one component (see \cite[Table 1]{affdenom}).
If the submatrix is of type $G_{2}^{(1)}$ or $D_{4}^{(3)}$, then there are pairs of roots whose lengths are related by a factor of $\sqrt(3)$, and thus $\RS$ is of type $G_{2}^{(1)}$ or $D_{4}^{(3)}$, so again $\RST{c}$ has only one component (and indeed the submatrix is all of $B$).
In these cases, we are done.

In the remaining cases, the situation is as follows.
We have showed that for every special index $k$, the submatrix determined by indices $k,(n-1),n$ is one of the top three matrices on \cref{submat tab}.
It remains only to show that the matrices $B_{12}$, $B_{13}$, $B_{23}$, $B_{21}$, $B_{31}$, and $B_{32}$, are zero.
We have already established that every entry is zero except possibly the entry indexed by the two special indices, so it remains to show that, for any two distinct special indices $j$ and $k$, the entry $b_{jk}$ is zero.

Suppose distinct special indices $j$ and $k$ have $b_{jk}\neq0$.
We may as well choose $j$ and $k$ such that $b_{jk}>0$.
We consider the $4\times4$ submatrix of $B$ with rows and columns indexed by $j,k,p,n$.
In all the remaining cases, the affine submatrix is $\begin{bsmallmatrix*}[r]0&2\\-2&0\end{bsmallmatrix*}$.
Thus $b_{jp}=-b_{jn}>0$, $b_{kp}=-b_{kn}>0$, $b_{pj}=-b_{nj}<0$, and $b_{pk}=-b_{nk}<0$.
%WON'T BE NECESSARY:
%Also, skew-symmetrizability of $B$ implies that $b_{kj}=-\frac{b_{nj}b_{kn}}{b_{jn}b_{nk}}b_{jk}<0$.
We compute the mutation in direction $j$.
\begin{align*}
\begin{bsmallmatrix*}
0&b_{jk}&-b_{jn}&b_{jn}\\
b_{kj}&0&-b_{kn}&b_{kn}\\
-b_{nj}&-b_{nk}&0&2\\
b_{nj}&b_{nk}&-2&0
\end{bsmallmatrix*}
&\overset{j}{\longrightarrow}
\begin{bsmallmatrix*}
0&-b_{jk}&b_{jn}&-b_{jn}\\
-b_{kj}&0&-b_{kn}&b_{kn}-b_{kj}b_{jn}\\
b_{nj}&-b_{nk}&0&2+b_{jn}b_{nj}\\
-b_{nj}&b_{nk}+b_{nj}b_{jk}&-2-b_{jn}b_{nj}&0
\end{bsmallmatrix*}\\
\end{align*}
Since $b_{jk}$, $b_{nj}$, and $b_{nk}$ are all strictly positive $b_{nk}+b_{nj}b_{jk}\ge2$.
Similarly, $b_{kn}$, $b_{kj}$, and $b_{jn}$ are strictly negative, so $b_{kn}-b_{kj}b_{jn}\le-2$.
Both of these inequalities must be equality, and we see that $b_{jk}=b_{nj}=b_{nk}=1$ and $b_{kn}=b_{kj}=b_{jn}=-1$, and we have determined all entries of the $4\times 4$ submatrix.
One easily verifies that the submatrix has the property that every mutation agrees with some permutation of the row/column indices.  
Therefore, we see that this submatrix is not on the list exchange matrices of sub-exponential growth in \cite[Theorem~1.1]{FeShThTu12}.
Therefore $B$ has exponential growth, and by this contradiction, we conclude that $b_{jk}=0$.

We have proved \cref{neigh B}.
We conclude this section with some useful facts that can be proved using the \lcnamecref{neigh B}.

\begin{lemma}\label{affine mut}
Suppose $B$ is a neighboring exchange matrix and let $\kk$ be a $2$-element sequence consisting of the two affine indices. 
Then $\mu_\kk(B)=B$.
\end{lemma}
\begin{proof}
\cref{neigh B} implies that most entries of $B$ are fixed under mutation in an affine position.
The only entries that might not be fixed are entries $b_{ij}$ where one of $i$ or $j$ is affine and the other is affine or special and entries $B_{ii}$ where $i$ is special.
Thus we only need to check the submatrices shown in \cref{submat tab}.
In each case, we see that mutation in one of the affine entries has the effect of negating the submatrix.
An additional mutation in the other affine index still fixes all entries of $B$ except those described above and again negates the submatrix, so that $\mu_\kk(B)=B$.  \sayN{Maybe we'll leave out these computations, which are really easy?}
\begin{align*}
&\begin{bsmallmatrix*}[r]
	0 & 1 & -1 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & -1 & 1 \\
	1 & 0 & -2 \\
	-1 & 2 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & 1 & -1 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
\end{bsmallmatrix*}\\
&\begin{bsmallmatrix*}[r]
	0 & 2 & -2 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & -2 & 2 \\
	1 & 0 & -2 \\
	-1 & 2 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & 2 & -2 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
\end{bsmallmatrix*}\\
&\begin{bsmallmatrix*}[r]
	0 & 1 & -1 \\
	-2 & 0 & 2 \\
	2 & -2 & 0
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & -1 & 1 \\
	2 & 0 & -2 \\
	-2 & 2 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & 1 & -1 \\
	-2 & 0 & 2 \\
	2 & -2 & 0
\end{bsmallmatrix*}\\
&\begin{bsmallmatrix*}[r]
	0 & 3 & -3 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & -3 & 3 \\
	1 & 0 & -2 \\
	-1 & 2 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & 3 & -3 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
\end{bsmallmatrix*}\\
&\begin{bsmallmatrix*}[r]
	0 & 1 & -1 \\
	-3 & 0 & 2 \\
	3 & -2 & 0
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & -1 & 1 \\
	3 & 0 & -2 \\
	-3 & 2 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & 1 & -1 \\
	-3 & 0 & 2 \\
	3 & -2 & 0
\end{bsmallmatrix*}\\ 
&\begin{bsmallmatrix*}[r]
	0 & 1 & -2 \\
	-2 & 0 & 4 \\
	1 & -1 & 0
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & -1 & 2 \\
	2 & 0 & -4 \\
	-1 & 1 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
	0 & 1 & -2 \\
	-2 & 0 & 4 \\
	1 & -1 & 0
\end{bsmallmatrix*}\\ 
&\begin{bsmallmatrix*}[r]
	0 & 2 & -1 \\
	-1 & 0 & 1 \\
	2 & -4 & 0
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow} 
\begin{bsmallmatrix*}[r]
	0 & -2 & 1 \\
	1 & 0 & -1 \\
	-2 & 4 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow} 
\begin{bsmallmatrix*}[r]
	0 & 2 & -1 \\
	-1 & 0 & 1 \\
	2 & -4 & 0
\end{bsmallmatrix*} \qedhere
\end{align*}
\end{proof}

\begin{proposition}\label{neigh good stuff}
Suppose $B$ is a neighboring exchange matrix.
\begin{enumerate}[\quad\bf1.]
\item\label{neigh delta}
The vector $\delta^B$ is zero in all non-affine indices.
Its affine entries are
\begin{itemize}
\item
$1,1$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&2\\-2&0\end{bsmallmatrix*}$,
\item
$2,1$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&4\\-1&0\end{bsmallmatrix*}$, or
\item
$1,2$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&1\\-4&0\end{bsmallmatrix*}$.
\end{itemize}
%If the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&2\\-2&0\end{bsmallmatrix*}$, then $\delta^B$ has entries $1,1$ in the affine indices. 
%If the affine submatrix is $\begin{bsmallmatrix*}[r]0&4\\-1&0\end{bsmallmatrix*}$ or $\begin{bsmallmatrix*}[r]0&1\\-4&0\end{bsmallmatrix*}$, then $\delta^B$ has entries $2,1$ or $1,2$ respectively in the affine indices.
\item\label{neigh im ray}
The vector $-\frac12B\delta^B$ (the shortest integer vector that spans the imaginary ray) is zero in all non-affine entries.
Its affine entries are
\begin{itemize}
\item
$-1,1$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&2\\-2&0\end{bsmallmatrix*}$,
\item
$-2,1$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&4\\-1&0\end{bsmallmatrix*}$, or
\item
$-1,2$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&1\\-4&0\end{bsmallmatrix*}$.
\end{itemize}
\end{enumerate}
\end{proposition}
\begin{proof}
We state these assertions about $\delta^B$ and $-\frac12B\delta^B$ together because the proofs of the assertions are intertwined.

Considering a cluster pattern with $B$ as the initial exchange matrix, the initial $\g$-vectors for non-affine indices are contained in the imaginary wall~$\d^B_\infty$.
Thus \cref{delta is the man}.\ref{im hyp} implies that $\delta^B$ is zero in all non-affine positions.
Furthermore, \cref{delta is the man}.\ref{im ray} says that $\delta^B$ has nonnegative entries and is minimal among integer vectors obtained from it by positive integer scaling.

Before determining the entries of $\delta^B$ in the two affine positions, we determine some entries of $-\frac12B\delta^B$.
\cref{neigh B} and the fact that $\delta^B$ is zero in all non-affine positions implies that $-\frac12B\delta^B$ is zero in non-special non-affine positions.
Since $-\frac12B\delta^B$ is the shortest integer vector in the imaginary ray, \cref{affine mut} implies that it is fixed under $\eta^{B^T}_\kk$, where $\kk$ is a $2$-element sequence consisting of the two affine indices. 
We use the fact that $-\frac12B\delta^B$ is fixed under $\eta^{B^T}_\kk$ to determine its affine entries $r_{n-1}$, and $r_n$.
%We need only consider the submatrices shown in \cref{submat tab}.
%The mutations are computed below, where the entries $r_k$ (for special~$k$), $r_{n-1}$, and $r_n$ denote entries in $-\frac12B\delta^B$.
To make the notation more compact, we use $p$ to stand for $n-1$.
We use the notation 
\[m(a,b)=\begin{cases}
ab&\text{if }\sgn(a)=\sgn(b)=+\\
-ab&\text{if }\sgn(a)=\sgn(b)=-\\
0&\text{otherwise}.
\end{cases}\]
To determine the affine entries, we need only the $2\times2$ affine submatrices of $B$.
In the most common case from \cref{submat tab}, the mutations are as follows:
\[
\begin{bsmallmatrix*}
0 & 2 & r_p\\
-2 & 0 & r_n
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}
0 & -2 & -r_p\\
2 & 0 & r_n+m(r_p,-2)
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
0 & 2 & -r_p+m(r_n+m(r_p,-2),-2)\\
-2 & 0 & -r_n-m(r_p,-2)
\end{bsmallmatrix*}
\]
We see that $r_n=-r_n-m(r_p,-2)$ and $r_p=-r_p+m(r_n+m(r_p,-2),-2)$, which can be rewritten $r_p=-r_p+m(-r_n,-2)$.
If $r_p$ is positive, then $r_n$ is zero, and thus also $r_p$ is zero.
By this contradiction, we see that $r_p$ is nonpositive.
Similarly, we see that $r_n$ is nonnegative.
%If $r_n$ is negative, then $r_p$ is zero, and thus also $r_n$ is zero, and we see that $r_n$ is nonnegative.
Thus both equations say $r_n=-r_p$.

Let $d_p$ and $d_n$ be the entries of $\delta^B$ in its affine positions.
Since $\delta^B$ is zero in its non-affine positions and since $r_p$ and $r_n$ are the affine entries of $-\frac12B\delta^B$, we compute that $r_p=-d_n$ and $r_n=d_p$.
Therefore $d_p=d_n$, and since $\delta^B$ is the smallest nonzero integer vector in the ray it spans, we see that $d_p=d_n=1$, as desired.
From there, we see that the special entries of $-\frac12B\delta^B$ are also zero and that $r_n=1=-r_p$ as desired.

In the next case from \cref{submat tab}, the mutations are
\[\begin{bsmallmatrix*}
0 & 4 & r_p\\
-1 & 0 &r_n
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}
0 & -4 & -r_p\\
1 & 0 &r_n+m(r_p,-1)
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
0 & 4 &-r_p+m(r_n+m(r_p,-1),-4)\\
-1 & 0 &-r_n-m(r_p,-1)
\end{bsmallmatrix*}
\]
%We see that $r_n=-r_n-m(r_p,-1)$ and $r_p=-r_p+m(r_n+m(r_p,-1),-4)$, which can be rewritten $r_p=-r_p+m(-r_n,-4)$.
%If $r_p$ is positive, then $r_n$ is zero, and thus also $r_p$ is zero.
%By this contradiction, we see that $r_p$ is nonpositive.
%Similarly, we see that $r_n$ is nonnegative.
%If $r_n$ is negative, then $r_p$ is zero, and thus also $r_n$ is zero, and we see that $r_n$ is nonnegative.
%One equation says $2r_n=-r_p$ and the other says $2r_p=-4r_n$.
Arguing as before, we see that $r_n$ is nonnegative and $2r_n=-r_p$.
Again taking $d_p$ and $d_n$ to be the affine entries of $\delta^B$, we also find that $d_p=2r_n=-r_p=2d_n$, so that $d_p=2$, $d_n=1$, $r_p=-2$, and $r_n=1$ as desired.
Again, we easily compute that the special entries of $-\frac12B\delta^B$ are zero.
%Since $\delta^B$ is zero in its non-affine positions and since $r_p$ and $r_n$ are the affine entries of $-\frac12B\delta^B$, we compute that $r_p=-2d_n$ and $r_n=\frac12d_p$.
%Therefore $d_p=2r_n=-r_p=2d_n$,...

In the final case, the mutations are
\[\begin{bsmallmatrix*}
0 & 1 & r_p\\
-4 & 0 &r_n
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow} 
\begin{bsmallmatrix*}
0 & -1 & -r_p\\
4 & 0 &r_n+m(r_p,-4)
\end{bsmallmatrix*}\overset{n}{\longrightarrow} 
\begin{bsmallmatrix*}
0 & 1 &-r_p+m(r_n+m(r_p,-4),-1)\\
-4 & 0 &-r_n-m(r_p,-4)
\end{bsmallmatrix*}
\]
Arguing as before, we see that $r_n=-2r_p\ge0$ and $2d_p=r_n=-2r_p=d_n$, so that $d_p=1$, $d_n=2$, $r_p=-1$, and $r_n=2$ as desired, and compute that the special entries of $-\frac12B\delta^B$ are zero.
%We see that $r_n=-r_n-m(r_p,-4)$ and $r_p=-r_p+m(r_n+m(r_p,-4),-1)$, which can be rewritten $r_p=-r_p+m(-r_n,-1)$.
%If $r_p$ is positive, then $r_n$ is zero, and thus also $r_p$ is zero.
%By this contradiction, we see that $r_p$ is nonpositive.
%Similarly, we see that $r_n$ is nonnegative.
%If $r_n$ is negative, then $r_p$ is zero, and thus also $r_n$ is zero, and we see that $r_n$ is nonnegative.
%One equation says $2r_n=-4r_p$ and the other says $2r_p=-r_n$.
%Since $\delta^B$ is zero in its non-affine positions and since $r_p$ and $r_n$ are the affine entries of $-\frac12B\delta^B$, we compute that $r_p=-\frac12d_n$ and $r_n=2d_p$.
%Therefore $2d_p=r_n=-2r_p=d_n$, ...
\end{proof}

\subsection{Type-C companions of neighboring exchange matrices}
To each neighboring exchange matrix $B$, we now associate an $(n-2)\times(n-2)$ exchange matrix called the \newword{type-C companion} of $B$ and written $\Comp(B)$.
The type-C companion $\Comp(B)$ agrees with the \emph{non-affine} rows and columns of $B$, except that all entries in \emph{special columns} are multiplied by $2$.
The following \lcnamecref{Comp is C} is immediate from \cref{neigh B} and \cite[Proposition~3.3]{NakanishiStella}.

\begin{proposition}\label{Comp is C}
Given a neighboring exchange matrix $B$, its type-B companion $\Comp(B)$ has a diagonal block decomposition with $1$, $2$, or $3$ diagonal blocks, each of finite type C, and having the same sizes as the blocks $B_{11}$, $B_{22}$, and/or $B_{33}$ of~$B$.
\end{proposition}
%\begin{proof}
%This should just be Salvatore and Tomoki's characterizations of type C exchange matrices, together with the quasi-leaf part of \cref{neigh B}\eqref{neigh detailed}.
%
%\end{proof}

In the following proposition, we think of the columns of $\Comp(B)$ as vectors in $\reals^n$ by considering them to have entries $0$ in the affine indices.

\begin{proposition}\label{Comp span}
Suppose $B$ is a neighboring exchange matrix.
Then the intersection of the hyperplane $(\delta^B)^\perp$ with the nonnegative linear span of the columns of $B$ is contained in the nonnegative linear span of $\frac12B\delta^B$ and the columns of $\Comp(B)$.
\end{proposition}
\begin{proof}
By inspection of \cref{submat tab} and in light of \cref{neigh good stuff}, we observe the following facts:
First, the vector $-\frac12B\delta^B$ spanning the affine ray is a \emph{negative} linear combinatiion of the last two columns of $B$.
Second, the linear span of the last two columns of $B$ intersected with the hyperplane $(\delta^B)^\perp$ is the line spanned by $\frac12B\delta^B$.
Third, $\frac12B\delta^B$ has nonzero entries only in the affine indices, and those entries are $1,-1$ unless the affine submatrix is $\begin{bsmallmatrix*}[r]0&4\\-1&0\end{bsmallmatrix*}$ or $\begin{bsmallmatrix*}[r]0&1\\-4&0\end{bsmallmatrix*}$, in which case, those entries are $2,-1$ or $1,-2$ respectively.
Fourth, each non-affine column of $B$ is half the corresponding column of $\Comp(B)$ plus a nonnegative multiple of $-\frac12B\delta^B$. 
(This multiple is zero unless the column is a special column.)
The proposition follows.
\end{proof}

\begin{proposition}\label{nonspecial mut}
Suppose $B$ is a neighboring exchange matrix and $k$ is a non-special, non-affine index of~$B$.
Then 
\begin{itemize}
\item
$\mu_k(B)$ is neighboring,
\item
$\Comp(\mu_k(B))=\mu_k(\Comp(B))$,
\item
the mutation map $\eta^{B^T}_k$ fixes the imaginary ray pointwise, and 
\item
If $x$ is a vector whose affine entries are zero, then $\eta^{B^T}_k(x)=\eta^{\Comp(B)^T}_k(x)$.
\end{itemize}
\end{proposition}
\begin{proof}
Mutating at $k$ means replacing the root $\gamma_k$ in the real $c$-cluster $\set{\gamma_1,\ldots,\gamma_n}$.
Since $k$ is not special and not affine, $\gamma_k$ is replaced by another root in $\APTre{c}$.
Thus $\xi_k$ is replaced by another vector in $\d_\infty$.
We see that $\mu_k(B)$ is neighboring.

Furthermore, since $k$ is not special and not affine, row $k$ and column $k$ only have nonzero entries within one of the blocks $B_{\ell\ell}$ for $\ell\in\set{1,2,3}$.
Thus mutating~$B$ at~$k$ amounts to replacing $B_{\ell\ell}$ by $\mu_k(B_{\ell\ell})$.
Mutating $B_{\ell\ell}$ at $k$ commutes with applying a positive scaling to a column of $B_{\ell\ell}$ other than~$k$.
Thus ${\Comp(\mu_k(B))=\mu_k(\Comp(B))}$.

\cref{neigh good stuff}.\ref{neigh im ray} says that $\frac12B\delta^B$ is zero in all non-affine entries.
Since $k$ is a non-affine entry, we see that $\eta^{B^T}_k$ fixes $\frac12B\delta^B$ and thus fixes the imaginary ray pointwise.
Since $k$ is non-special, the non-affine entries of column $k$ of $B$ agree with column $k$ of $\Comp(B)$.
Thus if $x$ is a vector whose affine entries are zero (so that $\eta^{\Comp(B)^T}_k(x)$ makes sense) we have $\eta^{B^T}_k(x)=\eta^{\Comp(B)^T}_k(x)$.
\end{proof}

\begin{proposition}\label{special mut}
Suppose $B$ is a neighboring exchange matrix and $k$ is a special index of~$B$.
Then there exists a sequence $\kk$ of indices in $\set{k,n-1,n}$ such that 
\begin{itemize}
\item
$\mu_\kk(B)$ is neighboring,
\item
$\Comp(\mu_\kk(B))=\mu_k(\Comp(B))$,
\item
the mutation map $\eta^{B^T}_\kk$ fixes the imaginary ray pointwise, and 
\item
If $x$ is a vector whose affine entries are zero, then $\eta^{B^T}_\kk(x)=\eta^{\Comp(B)^T}_k(x)$.
\end{itemize}
\end{proposition}
\begin{proof}
The special index $k$, together with the affine indices, determines a $3\times3$ submatrix of $B$ that agrees with one of the entries in \cref{submat tab}.
For convenience in the rest of the proof, we call this $3\times3$ submatrix merely ``the submatrix'' and refer to the labels in the table as ``the type'' of the submatrix.
We argue separately for the different cases in the table, but are able to combine some cases that are related by scaling.

\medskip

\noindent
\textbf{Case 1.}
The submatrix is of type $A_2^{(1)}$.
The sequence $\kk$ is $k(n-1)knk$.
Recalling that mutations in the sequence are applied from right to left, mutation by this sequence acts on the submatrix as
%\[
%\begin{bsmallmatrix*}[r]
%0&1&-1\\
%-1&0&2\\
%1&-2&0
%\end{bsmallmatrix*}
%\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-1&\,\,\,1\\
%1&0&1\\
%-1&-1&0
%\end{bsmallmatrix*}
%\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&\,\,\,1&1\\
%-1&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}
%\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&2&-1\\
%-2&0&1\\
%1&-1&0
%\end{bsmallmatrix*}.
%\]
\begin{multline*}
\begin{bsmallmatrix*}[r]
0&1&-1\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,1\\
1&0&1\\
-1&-1&0
\end{bsmallmatrix*}
\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,-1\\
1&0&-1\\
1&1&0
\end{bsmallmatrix*}\\
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&1&\,\,\,1\\
-1&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,1\\
1&0&1\\
-1&-1&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&1&\,\,\,-1\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}.
\end{multline*}
In particular, $\mu_\kk(B)$ restricts, in its affine indices, to an affine exchange matrix of rank~$2$,
so \cref{neigh B} implies that it is neighboring.

To compute the remaining entries of $\mu_\kk(B)$, we consider each $4\times4$ submatrix whose rows are indexed by $i,k,n-1,n$ and whose columns are indexed by $j,k,n-1,n$, for arbitrary $i,j\not\in\set{k,n-1,n}$.
%To make the notation more compact, we use $p$ to stand for $n-1$.
%We use the notation 
%\[m(a,b)=\begin{cases}
%ab&\text{if }\sgn(a)=\sgn(b)=+\\
%-ab&\text{if }\sgn(a)=\sgn(b)=-\\
%0&\text{otherwise},
%\end{cases}\]
We continue the notation $m(a,b)$ from the proof of \cref{neigh good stuff} and we will use the identity $m(a,b)+m(a,-b)=ab$ for $b>0$ several times.
We will also use the facts (from \cref{neigh B}) that $b_{(n-1)j}=-b_{nj}\le0$, and $b_{i(n-1)}=-b_{in}\ge0$.
\begin{align*}
\begin{bsmallmatrix*}[r]
b_{ij}&b_{ik}&-b_{in}&b_{in}\\
b_{kj}&0&1&-1\\
-b_{nj}&-1&0&2\\
b_{nj}&1&-2&0
\end{bsmallmatrix*}
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & -b_{ik} & -b_{in}+m(b_{ik},1) & b_{in}+m(b_{ik},-1)\\
-b_{kj} & 0 & -1 & 1\\
-b_{nj}+m(b_{kj},-1) & 1 & 0 & 1\\
b_{nj}+m(b_{kj},1) & -1 & -1 & 0
\end{bsmallmatrix*}\\
&\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & b_{in}-m(b_{ik},1) & b_{ik} & -b_{in}-m(b_{ik},-1)\\
b_{nj}-m(b_{kj},-1) & 0 & -1 & -1\\
b_{kj} & 1 & 0 & -1\\
-b_{nj}-m(b_{kj},1) & 1 & 1 & 0
\end{bsmallmatrix*}\\
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & -b_{in}+m(b_{ik},1) & b_{in}+m(b_{ik},-1) & -b_{ik}\\
-b_{nj}+m(b_{kj},-1) & 0 & 1 & 1\\
b_{nj}+m(b_{kj},1)  & -1 & 0 & -1\\
-b_{kj}& -1 & 1 & 0
\end{bsmallmatrix*}\\
&\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & b_{ik} & -b_{in}-m(b_{ik},-1) & b_{in}-m(b_{ik},1)\\
b_{kj}& 0 & -1 & 1\\
-b_{nj}-m(b_{kj},1)& 1 & 0 & 1\\
b_{nj}-m(b_{kj},-1) & -1 & -1 & 0
\end{bsmallmatrix*}\\
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+2m(b_{ik},b_{kj}) & -b_{ik} & -b_{in} & b_{in}\\
-b_{kj} & 0 & 1 & -1\\
-b_{nj} & -1 & 0 & 2\\
b_{nj} & 1 & -2 & 0
\end{bsmallmatrix*}.
\end{align*}

%ASIDE:
%Trying this with fewer assumptions, specifically ditching the assumption that $b_{(n-1)j}=-b_{nj}\le0$ and using $p$ to stand for $n-1$.
%UPDATE:  It's awful, because you don't know what sign anything is...  
%ONLY CHANGED the first two matrices.
%\begin{align*}
%\begin{bsmallmatrix*}[r]
%b_{ij}&b_{ik}&b_{ip}&b_{in}\\
%b_{kj}&0&1&-1\\
%-b_{nj}&-1&0&2\\
%b_{nj}&1&-2&0
%\end{bsmallmatrix*}
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+m(b_{ik},b_{kj}) & -b_{ik} & b_{ip}+m(b_{ik},1) & b_{in}+m(b_{ik},-1)\\
%-b_{kj} & 0 & -1 & 1\\
%-b_{nj}+m(b_{kj},-1) & 1 & 0 & 1\\
%b_{nj}+m(b_{kj},1) & -1 & -1 & 0
%\end{bsmallmatrix*}\\
%&\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+m(b_{ik},b_{kj}) & b_{in}-m(b_{ik},1) & b_{ik} & -b_{in}-m(b_{ik},-1)\\
%b_{nj}-m(b_{kj},-1) & 0 & -1 & -1\\
%b_{kj} & 1 & 0 & -1\\
%-b_{nj}-m(b_{kj},1) & 1 & 1 & 0
%\end{bsmallmatrix*}\\
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+m(b_{ik},b_{kj}) & -b_{in}+m(b_{ik},1) & b_{in}+m(b_{ik},-1) & -b_{ik}\\
%-b_{nj}+m(b_{kj},-1) & 0 & 1 & 1\\
%b_{nj}+m(b_{kj},1)  & -1 & 0 & -1\\
%-b_{kj}& -1 & 1 & 0
%\end{bsmallmatrix*}\\
%&\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+m(b_{ik},b_{kj}) & b_{ik} & -b_{in}-m(b_{ik},-1) & b_{in}-m(b_{ik},1)\\
%b_{kj}& 0 & -1 & 1\\
%-b_{nj}-m(b_{kj},1)& 1 & 0 & 1\\
%b_{nj}-m(b_{kj},-1) & -1 & -1 & 0
%\end{bsmallmatrix*}\\
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+2m(b_{ik},b_{kj}) & -b_{ik} & -b_{in} & b_{in}\\
%-b_{kj} & 0 & 1 & -1\\
%-b_{nj} & -1 & 0 & 2\\
%b_{nj} & 1 & -2 & 0
%\end{bsmallmatrix*}.
%\end{align*}


%\begin{align*}
%\begin{bsmallmatrix*}[r]
%a_{ij}&x_i&0&0\\
%y_j&0&1&-1\\
%0&-1&0&2\\
%0&1&-2&0
%\end{bsmallmatrix*}
%&\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j) & -x_i & m(x_i,1) & m(x_i,-1)\\
%-y_j & 0 & -1 & 1\\
%m(y_j,-1) & 1 & 0 & 1\\
%m(y_j,1) & -1 & -1 & 0
%\end{bsmallmatrix*}\\
%&\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j) & -x_i+m(x_i,-1) & x_i & -m(x_i,-1)\\
%-y_j+m(y_j,1) & 0 & -1 & -1\\
%y_j & 1 & 0 & -1\\
%-m(y_j,1) & 1 & 1 & 0
%\end{bsmallmatrix*}\\
%&\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j) & x_i-m(x_i,-1) & m(x_i,-1) & -x_i+m(x_i,-1)-m(x_i,-1)\\
%y_j-m(y_j,1) & 0 & 1 & 1\\
%m(y_j,1) & -1 & 0 & -1\\
%-y_j+m(y_j,1)-m(y_j,1) & -1 & 1 & 0
%\end{bsmallmatrix*}\\
%&\overset{2}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j) & x_i & -m(x_i,-1) & -x_i+m(x_i,-1)\\
%y_j & 0 & -1 & 1\\
%-m(y_j,1) & 1 & 0 & 1\\
%-y_j+m(y_j,1) & -1 & -1 & 0
%\end{bsmallmatrix*}\\
%&\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+2m(x_i,y_j) & -x_i & 0 & 0\\
%-y_j & 0 & 1 & -1\\
%0 & -1 & 0 & 2\\
%0 & 1 & -2 & 0
%\end{bsmallmatrix*}\\
%\end{align*}
%where we observe in the middle mutation in direction 1 that $-x_i+m(x_i,-1)$ is always non-positive and $-y_j+m(y_j,1)$ is always non-negative.

It is much easier to find the $ij$-entry of $\mu_k(\Comp(B))$.
We compute
\[
\begin{bsmallmatrix*}
b_{ij}&2b_{ik}\\
b_{kj}&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(2b_{ik},b_{kj})&-2b_{ik}\\
-b_{kj}&0
\end{bsmallmatrix*}
\]
and we conclude that $\Comp(\mu_\kk(B))=\mu_k(\Comp(B))$.

The computations above can be reused to prove the assertions about $\eta^{B^T}_\kk$.
%also let us determine mutation maps by taking $j$ to index a new column that has been used to extend $B$ but still taking $i$ to be a non-affine index in~$\set{1,\ldots,n}$.
\cref{neigh good stuff}.\ref{neigh im ray} says that the vector $-\frac12B\delta^B$ that spans the imaginary ray is zero in non-affine entries and has affine entries $-1,1$.
This, the computations above are valid, replacing the $i\th$ column of $B$ in the calculations with $-\frac12B\delta^B$.
Specifically, we replace $b_{ij}$ and $b_{kj}$ with $0$ and replace $b_{nj}$ with $1$, and the computations show that $\eta^{B^T}_\kk$ fixes $-\frac12B\delta^B$.

Similarly, if $x$ is a vector whose affine entries are zero, it can take the place of the $j\th$ column of $B$ in the above calculations, and will satisfy the requirements $b_{(n-1)j}=-b_{nj}\le0$, and $b_{i(n-1)}=-b_{in}\ge0$ that were used in the calculation.  
Therefore, we see that $\eta^{B^T}_\kk(x)=\eta^{\Comp(B)^T}_k(x)$.
We have proved the \lcnamecref{special mut} in this case.

\noindent
\textbf{Case 2.}
The submatrix is of type $C_2^{(1)}$ or $A_4^{(2)}$.
We first consider the case of $C_2^{(1)}$.
The sequence $\kk$ is again $k(n-1)knk$, and mutation acts on the submatrix as shown here:
%Since $\mu_k(DBD^{-1})=D\mu_k(B)D^{-1}$ for any positive diagonal matrix $D$, it suffices to only consider the cases $C_2^{(1)}$, which implies both $A_4^{(2)}$ cases. % and $D_3^{(2)}$, and $G_2^{(1)}$, which implies $D_4^{(3)}$.
%(Let $B'=DBD^{-1}$, then $E'=DED^{-1}$ and $F'=DFD^{-1}$ so that $D(EBF)D^{-1}=E'B'F'$.)
\begin{multline*}
\begin{bsmallmatrix*}[r]
0&2&-2\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-2&\,\,\,2\\
1&0&0\\
-1&0&0
\end{bsmallmatrix*}
\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-2&\,\,\,-2\\
1&0&0\\
1&0&0
\end{bsmallmatrix*}\\
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&2&\,\,\,2\\
-1&0&0\\
-1&0&0
\end{bsmallmatrix*}
\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-2&\,\,\,2\\
1&0&0\\
-1&0&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&2&-2\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}.
\end{multline*}
Again, \cref{neigh B} implies that $\mu_\kk(B)$ is neighboring.

We again compute the remaining entries of $\mu_\kk(B)$ by considering each $4\times4$ submatrix with rows indexed by $i,k,n-1,n$ and columns indexed by $j,k,n-1,n$, for arbitrary $i,j\not\in\set{k,n-1,n}$.
Again, we have $b_{(n-1)j}=-b_{nj}\le0$, and $b_{i(n-1)}=-b_{in}\ge0$.
We again use the identity $m(a,b)+m(a,-b)=ab$ for $b\ge0$ and now also use the identity $m(a,b)-m(a,-b)=|a|b$ for $b\ge0$ and $m(a,pb)=pm(a,b)$ for $p\ge0$.
\begin{align*}
\begin{bsmallmatrix*}[r]
b_{ij}&b_{ik}&-b_{in}&b_{in}\\
b_{kj}&0&2&-2\\
-b_{nj}&-1&0&2\\
b_{nj}&1&-2&0
\end{bsmallmatrix*}
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & -b_{ik} & -b_{in}+m(b_{ik},2) & b_{in}+m(b_{ik},-2)\\
-b_{kj} & 0 & -2 & 2\\
-b_{nj}+m(b_{kj},-1) & 1 & 0 & 0\\
b_{nj}+m(b_{kj},1) & -1 & 0 & 0
\end{bsmallmatrix*}\\
&\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & b_{in}-|b_{ik}| & -b_{in}+m(b_{ik},2) & -b_{in}-m(b_{ik},-2)\\
2b_{nj}+|b_{kj}| & 0 & -2 & -2\\
-b_{nj}+m(b_{kj},-1) & 1 & 0 & 0\\
-b_{nj}-m(b_{kj},1) & 1 & 0 & 0
\end{bsmallmatrix*}\\
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & -b_{in}+|b_{ik}| & b_{in}+m(b_{ik},-2) & b_{in}-m(b_{ik},2)\\
-2b_{nj}+|b_{kj}| & 0 & 2 & 2\\
b_{nj}+m(b_{kj},1)  & -1 & 0 & 0\\
b_{nj}-m(b_{kj},-1)& -1 & 0 & 0
\end{bsmallmatrix*}\\
&\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & b_{ik} & -b_{in}-m(b_{ik},-2) & b_{in}-m(b_{ik},2)\\
b_{kj}& 0 & -2 & 2\\
-b_{nj}-m(b_{kj},1)& 1 & 0 & 0\\
b_{nj}-m(b_{kj},-1) & -1 & 0 & 0
\end{bsmallmatrix*}\\
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+2m(b_{ik},b_{kj}) & -b_{ik} & -b_{in} & b_{in}\\
-b_{kj} & 0 & 2 & -2\\
-b_{nj} & -1 & 0 & 2\\
b_{nj} & 1 & -2 & 0
\end{bsmallmatrix*}.
\end{align*}

%\begin{align*}
%\begin{bsmallmatrix*}[r]
%b_{ij}&b_{ik}&-b_{in}&b_{in}\\
%b_{kj}&0&2&-2\\
%-b_{nj}&-1&0&2\\
%b_{nj}&1&-2&0
%\end{bsmallmatrix*}
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j)&-x_i&m(x_i,2)&m(x_i,-2)\\
%-y_j&0&-2&2\\
%m(y_j,-1)&1&0&0\\
%m(y_j,1)&-1&0&0
%\end{bsmallmatrix*}\\
%&\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j)&-x_i+m(x_i,-2)&m(x_i,2)&-m(x_i,-2)\\
%-y_j+2m(y_j,1)&0&-2&-2\\
%m(y_j,-1)&1&0&0\\
%-m(y_j,1)&1&0&0
%\end{bsmallmatrix*}\\
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j)&x_i-m(x_i,-2)&m(x_i,-2)&-2x_i+m(x_i,-2)\\
%y_j-2m(y_j,1)&0&2&2\\
%m(y_j,1)&-1&0&0\\
%-y_j+m(y_j,1)&-1&0&0
%\end{bsmallmatrix*}\\
%&\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j)&x_i&-m(x_i,-2)&-2x_i+m(x_i,-2)\\
%y_j&0&-2&2\\
%-m(y_j,1)&1&0&0\\
%-y_j+m(y_j,1)&-1&0&0
%\end{bsmallmatrix*}\\
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+2m(x_i,y_j)&-x_i&0&0\\
%-y_j&0&2&-2\\
%0&-1&0&2\\
%0&1&-2&0
%\end{bsmallmatrix*}
%\end{align*}
%where we observe in the middle mutation in direction 1 that $-x_i+m(x_i,-2)$ is always non-positive and $-y_j+2m(y_j,1)$ is always non-negative.

The computation of the $ij$-entry of $\mu_k(\Comp(B))$ is the same as in Case~1, and we again conclude that $\Comp(\mu_\kk(B))=\mu_k(\Comp(B))$.
The assertions about $\eta^{B^T}_\kk$ are proved exactly as in Case~1.
We have proved the \lcnamecref{special mut} in the case of type $C_2^{(1)}$.

The matrices of type $A_4^{(2)}$ shown in \cref{submat tab} are obtained from the matrix of type $C_1{(1)}$ by conjugating by a diagonal matrix (with diagonal entries either $1,2,1$ or $1,1,2$).
Since $\mu_\kk(D^{-1}BD)=D^{-1}\mu_\kk(B)D$ for any positive diagonal matrix $D$ \cite[Proposition~4.5]{ca1}, we can reuse the computations above, conjugated by a diagonal matrix, to obtain the \lcnamecref{special mut} in the cases of type $A_4^{(2)}$.

\noindent
\textbf{Case 3.}
The submatrix is of type  $G_2^{(1)}$ or $D_4^{(3)}$.
In these cases, the associated root system has roots whose squared lengths are related by a factor of $3$.
The classification of affine root systems thus implies that $n=3$, so that $B$ in fact it equals one of the submatrices shown in \cref{submat tab}.
For that reason, the computation is smaller in this case, and the indices $k,(n-1),n$ are $1,2,3$.

We begin with type $G_2^{(1)}$.
%The sequence $\kk$ is $132131$, and the mutation acts on $B$ as shown here.
%\begin{multline*}
%\begin{bsmallmatrix*}[r]
%0&3&-3\\
%-1&0&2\\
%1&-2&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-3&3\\
%1&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}\\
%\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&1\\
%1&-1&0
%\end{bsmallmatrix*}
%\overset{2}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&-1\\
%1&1&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&3\\
%0&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}\\
%\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&3&-3\\
%-1&0&1\\
%1&-1&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-3&3\\
%1&0&-2\\
%-1&2&0
%\end{bsmallmatrix*}.
%\end{multline*}
%The sequence $\kk$ is $13121231$, and the mutation acts on $B$ as shown here.
%\begin{multline*}
%\begin{bsmallmatrix*}[r]
%0&3&-3\\
%-1&0&2\\
%1&-2&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-3&3\\
%1&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}
%\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&1\\
%1&-1&0
%\end{bsmallmatrix*}\\
%\overset{2}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&-1\\
%1&1&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&3\\
%0&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}
%\overset{2}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&3\\
%0&0&1\\
%-1&-1&0
%\end{bsmallmatrix*}\\
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&1\\
%1&-1&0
%\end{bsmallmatrix*}
%\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-3&3\\
%1&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&3&-3\\
%-1&0&2\\
%1&-2&0
%\end{bsmallmatrix*}.
%\end{multline*}
The sequence $\kk$ is $1313231$, and the mutation acts on $B$ as shown here.
\begin{multline*}
\begin{bsmallmatrix*}[r]
0&3&-3\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-3&3\\
1&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&-3\\
0&0&1\\
1&-1&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&3\\
0&0&1\\
-1&-1&0
\end{bsmallmatrix*}\\
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&-3\\
0&0&-1\\
1&1&0
\end{bsmallmatrix*}
\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&-3\\
0&0&1\\
1&-1&0
\end{bsmallmatrix*}
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-3&3\\
1&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&3&-3\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}.
\end{multline*}
Once again, \cref{neigh B} implies that $\mu_\kk(B)$ is neighboring.

There are no additional entries of $\mu_\kk(B)$ to compute, and $\Comp(B)$ is $[0]$, so we see that $\Comp(\mu_\kk(B))=\mu_k(\Comp(B))$.

To prove the assertions about mutation maps, we consider adding an extra column with entries $c_1,c_2,c_3$ with $c_3=-c_2\ge0$.
\begin{multline*}
\begin{bsmallmatrix*}
0&3&-3&c_1\\
-1&0&2&c_2\\
1&-2&0&-c_2
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
0&-3&3&-c_1\\
1&0&-1&c_2+m(c_1,-1)\\
-1&1&0&-c_2+m(c_1,1)
\end{bsmallmatrix*}\\
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
0&0&-3&-3c_2+|c_1|+m(c_1,1)\\
0&0&1&c_2+m(c_1,-1)\\
1&-1&0&c_2-m(c_1,1)
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
0&0&3&3c_2-|c_1|-m(c_1,1)\\
0&0&1&c_2+m(c_1,-1)\\
-1&-1&0&-2c_2+|c_1|
\end{bsmallmatrix*}\\
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
0&0&-3&-3c_2+2|c_1|-m(c_1,1)\\
0&0&-1&-c_2+m(c_1,1)\\
1&1&0&2c_2-|c_1|
\end{bsmallmatrix*}
\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}
0&0&-3&-3c_2+2|c_1|-m(c_1,1)\\
0&0&1&c_2-m(c_1,1)\\
1&-1&0&c_2+m(c_1,-1)
\end{bsmallmatrix*}\\
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
0&-3&3&c_1\\
1&0&-1&c_2-m(c_1,1)\\
-1&1&0&-c_2-m(c_1,-1)
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
0&3&-3&-c_1\\
-1&0&2&c_2\\
1&-2&0&-c_2
\end{bsmallmatrix*}.
\end{multline*}
The imaginary ray is spanned by the vector with entries $c_1=0$, $c_2=-1$, $c_3=1$, and we see that $\eta^{B^T}_\kk$ fixes that vector.
We also see that if $x$ is a vector with entries $c_1,0,0$, then $\eta_\kk^{B^T}(x)$ has entries $-c_1,0,0$.
We compute $\eta_1^{\Comp(B)^T}$ by mutating $[0\,\,c_1]\overset{1}{\longrightarrow}[0\,\,-c_1]$, so $\eta^{B^T}_\kk(x)=\eta^{\Comp(B)^T}_k(x)$ as desired.

The matrix of type $D_4^{(3)}$ in \cref{submat tab} is obtained from the matrix of type $G_2{(1)}$ by conjugating by a diagonal matrix with diagonal entries $3,1,1$.
Since $\mu_\kk(D^{-1}BD)=D^{-1}\mu_\kk(B)D$ for any positive diagonal matrix $D$ \cite[Proposition~4.5]{ca1}, we can reuse the computations above, conjugated by a diagonal matrix, to obtain the \lcnamecref{special mut} in the case of type $D_4^{(3)}$.
\end{proof}

\cref{nonspecial mut,special mut} allow us to prove the following key fact.

\begin{proposition}\label{neigh im wall}
Suppose $B$ is neighboring and is indexed as in \cref{neigh B}.
Then $\d^B_\infty$ is the half-hyperplane contained in $(\delta^B)^\perp$, containing the vector $-\frac12B\delta^B$, with relative boundary the codimension-$2$ space consisting of vectors that are zero in the affine indices.
\end{proposition}
\begin{proof}
Suppose $\kk$ is a sequence of non-affine indices.  
Let $\ll$ be the corresponding sequence where each \emph{special} index in $\kk$ is replaced with the corresponding sequence whose existence is proved in \cref{special mut}.
\cref{nonspecial mut,special mut} combine to say that 
\begin{itemize}
\item
$\Comp(\mu_\ll(B))=\mu_\kk(\Comp(B))$,
\item
the mutation map $\eta^{B^T}_\ll$ fixes the imaginary ray pointwise, and 
\item
If $x$ is a vector whose affine entries are zero, then $\eta^{B^T}_\ll(x)=\eta^{\Comp(B)^T}_\kk(x)$.
\end{itemize}
(The fact, in each of \cref{nonspecial mut,special mut}, that the mutated matrix is neighboring is crucial to concluding these facts for $\ll$.)

Since $\Comp(B)$ is of finite type, $\F_{\Comp(B)^T}$ is finite and complete.
We consider the mutation fan $\F_{\Comp(B)^T}$ as a complete fan in the subspace of $\reals^n$ consisting of vectors whose affine entries are zero. 
(We refer to this subspace as $\reals^{n-2}$.)
Every maximal cone of $\F_{\Comp(B)^T}$ is the image of the positive cone in $\reals^{n-2}$ under some mutation map $\eta^{(B')^T}_{\kk^{-1}}$, where $B'=\mu_\kk(\Comp(B))$ and $\kk$ is a sequence of non-affine indices.
The considerations of the paragraph above imply that $B'$ is the type-C companion of $\mu_\ll(B)$.

Let $U$ be any maximal cone in $\F_{\Comp(B)^T}$ and let $\kk$ be the sequence such that $U$ is the image of the positive cone in $\reals^{n-2}$ under $\eta^{(B')^T}_{\kk^{-1}}$, for $B'$ as above.
There is an imaginary cone in $\F_{\mu_ll(B)^T}$ that is the nonnegative linear span of the imaginary ray and the positive cone in $\reals^{n-2}$.
Since $\eta^{\mu_ll(B)^T}_{\ell^{-1}}$ is an isomorphism from $\F_{\mu_ll(B)^T}$ to $\F_{B^T}$, the results above imply that the positive span of $U$ and the imaginary ray is an imaginary cone in $\F_{B^T}$.
We see that the imaginary cones in $\F_{B^T}$ fill the half-hyperplane described in the \lcnamecref{neigh im wall}.
Since the imaginary ray is in the relative interior of that half-hyperplane, there are no other imaginary cones.
\end{proof}

We now prove the main theorem.

\begin{proof}[Proof of \cref{affine main}]
\cref{affine main partial} says that $\P_\lambda^B$ contains the line segment described in the \lcnamecref{affine main}.
We need to show the opposite containment.

Given $B$ of affine type, let $\lambda$ be in the imaginary wall $\d_\infty^B$.
Let $\kk$ be a sequence of indices in $\set{1,\ldots,n}$ and write $B'$ for $\mu_\kk(B)$ and $\lambda'$ for $\eta_\kk^{B^T}(\lambda)$.
The mutation map $\eta_\kk^{B^T}$ acting on $\F_{B^T}$ takes the imaginary ray of $\F_{B^T}$ to the imaginary ray of $\F_{\mu_\kk(B)^T}$, and more specifically takes the vector $-\frac12B\delta^B$ to the vector $-\frac12\mu_\kk(B)\delta^{\mu_\kk(B)}$.
Furthermore, $\eta_\kk^{B^T}$ is linear on each imaginary cone of $\F_{B^T}$, and thus takes the line segment that \cref{affine main} claims is equal to $\P_\lambda^B$ to the analogous line segment that is claimed to equal $\P_{\lambda'}^{B'}$.
Now \cref{shift} implies that it is enough to prove the \lcnamecref{affine main} for any one $B$ in the exchange pattern.
We choose $B$ to be a neighboring exchange matrix.

Now suppose $\kk$ is a sequence of \emph{non-affine} indices and, as before, let $\ll$ be the corresponding sequence where each \emph{special} index in $\kk$ is replaced with the corresponding sequence whose existence is proved in \cref{special mut}.

By \cref{neigh im wall}, we can write $\lambda$ as $\lambda_0+\lambda_\infty$, where $\lambda_0$ is zero in the affine indices and $\lambda_\infty$ is a nonnegative scaling of $-\frac12B\delta^B$.
\cref{nonspecial mut,special mut} imply that $\Comp(B')=\mu_\kk(\Comp(B))$ and $\eta_\ll^{B^T}(\lambda)=\eta_\kk^{\Comp(B)^T}(\lambda_0)+\lambda_\infty$.
\cref{Comp span} implies that 
\begin{multline*}
\set{B'\alpha:\alpha\in\reals^n,\alpha\ge0}\cap(\delta^{B'})^\perp\\
\subseteq\set{\Comp(B')\alpha+B'\delta^{B'}a:\alpha\in\reals^{n-2},\alpha\ge0,a\ge0}.
\end{multline*}
We see that 
\begin{multline*}
\set{\eta_\ll^{B^T}(\lambda)+B'\alpha:\alpha\in\reals^n,\alpha\ge0}\cap\d^{B'}_\infty\\
\begin{aligned}
&\subseteq\set{\eta_\ll^{B^T}(\lambda)+\Comp(B')\alpha+B'\delta^{B'}p:\alpha\in\reals^{n-2},\alpha\ge0,p\ge0}\cap\d^{B'}_\infty\\
&=\set{\eta_\kk^{\Comp(B)}(\lambda_0)+\Comp(B')\alpha:\alpha\ge0}+\set{q\lambda_\infty:0\le q\le1}.
\end{aligned}
\end{multline*}
Now applying $(\eta_\ll^{B^T})^{-1}$ to both sides of the containment, we see that 
\[\P^B_{\lambda,\ell}\subseteq\P^{\Comp(B)}_{\lambda_0,\kk}+\set{q\lambda_\infty:0\le q\le1}.\]
\cref{finite P point} says that $\P^{\Comp(B)}_{\lambda_0}$ (the intersection of the $\P^{\Comp(B)}_{\lambda_0,\kk}$ over all sequences $\kk$) equals $\set{\lambda_0}$.
\cref{P in dBinf} says that $\P^B_\lambda\subseteq\d^B_\infty$.
Thus $\P^B_{\lambda}$ (the intersection of the $\P^B_{\lambda,\ll}$ over all sequences $\ll$) is contained in $\P^{\Comp(B)}_{\lambda_0,\kk}+\set{q\lambda_\infty:0\le q\le1}$, which equals $\set{\lambda-q\lambda_\infty:0\le q\le1}$.
\end{proof}





\subsection{Extended exchange matrices of affine type}

Let $B$ be an exchange matrix of affine type and let $\tB$ be an extension of $B$ with linearly independent columns.


\sayN{We are going to use \cref{contains proj} to turn \cref{P in dinf} into a statement about $\P^\tB_\lambda$, but we don't yet have a definition of $\d_\infty$ in the extended-exchange matrix sense.}


imaginary ray 
imaginary wall $\d^\tB$





% bibliography
\bibliographystyle{plain}
\bibliography{bibliography}
\vspace{-0.175 em}


\end{document}

