%!TEX TS-program =  pdflatex 

%%!TEX TS-program =  arara 
%% arara: pdflatex
%% arara: bibtex

%Changes since the initial arXiv submission are marked:
%SinceArXiv:  

\documentclass{amsart}
\usepackage{array}
\newcolumntype{P}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{graphicx,verbatim, amsmath, amssymb, amsthm, amsfonts, epsfig, amsxtra,ifthen,mathtools,epstopdf,caption,enumerate,hhline,bbm,capt-of,longtable}	
\usepackage[bookmarks=true, bookmarksopen=false,%
    colorlinks=true,%
    linkcolor=darkblue,%
    citecolor=darkblue,%
    filecolor=darkblue,%
    menucolor=darkblue,%
%    linktoc=page,%
    linktoc=all,%
    urlcolor=darkblue
]{hyperref}
\usepackage[usenames]{xcolor}
\definecolor{darkblue}{cmyk}{1,0.3,0,0.1}  %blue
\usepackage[capitalize]{cleveref}
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widebar}{0}{mathx}{"73}
\epstopdfsetup{suffix=}
\DeclareGraphicsExtensions{.ps}
\DeclareGraphicsRule{.ps}{pdf}{.pdf}{`ps2pdf -dEPSCrop -dNOSAFER #1 \noexpand\OutputFile}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{thm}[proposition]{Theorem}
\newtheorem{conj}[proposition]{Conjecture}
\newtheorem{phen}{Phenomenon}

\renewcommand\thephen{\Roman{phen}}

\theoremstyle{definition}
\newtheorem{example}[proposition]{Example}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{question}[proposition]{Question}

\theoremstyle{remark}
\newtheorem{remark}[proposition]{Remark}
\newtheorem{problem}[proposition]{Problem}

\numberwithin{equation}{section}


% This is for setting off words we define in a separate typeface.
\newcommand{\newword}[1]{\textbf{\emph{#1}}}

\newcommand{\integers}{\mathbb Z}
\newcommand{\rationals}{\mathbb Q}
\newcommand{\naturals}{\mathbb N}
\newcommand{\reals}{\mathbb R}

\newcommand{\edge}{\,\,\rule[2.7pt]{20pt}{0.5pt}\,\,}

\newcommand{\ep}{\varepsilon}
\newcommand{\thet}{\vartheta}
\newcommand{\col}{\operatorname{col}}
\newcommand{\cut}{\operatorname{cut}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\op}{\operatorname{op}}
\newcommand{\JIrr}{\operatorname{JIrr}}
\newcommand{\ji}{\operatorname{ji}}
\newcommand{\Sh}{\operatorname{Sh}}
\newcommand{\Wall}{\operatorname{Wall}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\cw}{\operatorname{cw}}
\newcommand{\ccw}{\operatorname{ccw}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\vsgn}{\mathbf{sgn}}
\newcommand{\Seed}{\operatorname{Seed}}
%\newcommand{\Sh}{{\mathcal Sh}}
\newcommand{\nnspan}{{\!\!\tiny\begin{array}{r}\mathbf{span}\\\ge0\end{array}\hspace*{-0.57em}}}
%\newcommand{\nnspan}{{\!\!\tiny\begin{array}{r}\mathbf{nonneg}\\\mathbf{span}\end{array}\hspace*{-0.57em}}}
%\newcommand{\nnspan}{{\!\!\tiny\begin{array}{r}\mathbf{pos}\\\mathbf{span}\end{array}\hspace*{-0.57em}}}
%\newcommand{\nnspan}{\underset{\text{span}}{\overset{\text{pos}}{}}}
%\newcommand{\nnspan}{\mathbf{span_+}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Int}{\operatorname{Int}}
\newcommand{\Fix}{\operatorname{Fix}}
\newcommand{\Stab}{\operatorname{Stab}}
\newcommand{\geom}{{\operatorname{geom}}}
\newcommand{\mon}{{\operatorname{mon}}}
\newcommand{\Ray}{{\operatorname{Ray}}}
\newcommand{\Ram}{{\operatorname{Ram}}}
\newcommand{\uf}{{\operatorname{uf}}}
\newcommand{\fr}{{\operatorname{fr}}}
\newcommand{\Geom}{{\operatorname{\textbf{Geom}}}}
\newcommand{\gFan}{\g\!\operatorname{Fan}}
\newcommand{\Cg}{\mbox{{\rm Cg}}}
\newcommand{\Con}{\mbox{{\rm Con}}}
\newcommand{\Irr}{\mbox{{\rm Irr}}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\fs}{\mathrm{fs}}
\newcommand{\ufs}{\mathrm{ufs}}
\newcommand{\covers}{{\,\,\,\cdot\!\!\!\! >\,\,}}
\newcommand{\covered}{{\,\,<\!\!\!\!\cdot\,\,\,}}
\newcommand{\set}[1]{{\lbrace #1 \rbrace}}
\newcommand{\sett}[1]{{\bigl\lbrace #1 \bigr\rbrace}}
\newcommand{\settt}[1]{{\Bigl\lbrace #1 \Bigr\rbrace}}
\newcommand{\setttt}[1]{{\biggl\lbrace #1 \biggr\rbrace}}
\newcommand{\settttt}[1]{{\Biggl\lbrace #1 \Biggr\rbrace}}
\newcommand{\pidown}{\pi_\downarrow}
\newcommand{\piup}{\pi^\uparrow}
\newcommand{\br}[1]{{\langle #1 \rangle}}
\newcommand{\brr}[1]{{\bigl\langle #1 \bigr\rangle}}
\newcommand{\brrr}[1]{{\Bigl\langle #1 \Bigr\rangle}}
\newcommand{\brrrr}[1]{{\biggl\langle #1 \biggr\rangle}}
\newcommand{\brrrrr}[1]{{\Biggl\langle #1 \Biggr\rangle}}
\newcommand{\A}{{\mathcal A}}
\newcommand{\I}{{\mathcal I}}
\newcommand{\GG}{{\mathbf G}}
\newcommand{\CC}{{\mathbf C}}
\newcommand{\EL}{{\mathcal L}}
\newcommand{\F}{{\mathcal F}}
\newcommand{\D}{{\mathfrak D}}
\newcommand{\N}{{\mathcal N}}
\newcommand{\p}{{\mathfrak p}}
\newcommand{\X}{{\mathcal X}}
\newcommand{\W}{{\mathcal W}}
\newcommand{\join}{\vee}
\newcommand{\meet}{\wedge}
\renewcommand{\Join}{\bigvee}
\newcommand{\Meet}{\bigwedge}
\newcommand{\bigmeet}{\Meet}
\newcommand{\bigjoin}{\Join}
\newcommand{\leftq}[2]{\!\!\phantom{.}^{#1} {#2}}
\newcommand{\closeleftq}[2]{\!\!\phantom{.}^{#1}\! {#2}}
\newcommand{\Pge}{{\Phi_{\ge -1}}}
%\newcommand{\ck}{^\vee}
%\newcommand{\ck}{^{\scalebox{0.5}[0.5]{$\vee$}}}
\newcommand{\ck}{\spcheck}
\newcommand{\letw}{\le_{\mathrm{tw}}}
\newcommand{\Alg}{\mathrm{Alg}}
\newcommand{\toname}[1]{\overset{#1}{\longrightarrow}}
\newcommand{\dashname}[1]{\overset{#1}{\mbox{---\!---}}}
\newcommand{\st}{^\mathrm{st}}
\renewcommand{\th}{^\text{th}}
\newcommand{\nd}{^\text{nd}}
\newcommand{\rd}{^\text{rd}}
\newcommand{\0}{{\mathbf{0}}}
\newcommand{\Vol}{\mathrm{Vol}}
\newcommand{\lleq}{\le\!\!\!\le}
\newcommand{\notlleq}{\le\!\!\!\!\not\,\le}
\newcommand{\ggeq}{\ge\!\!\!\ge}
\newcommand{\Cone}{\mathrm{Cone}}
\newcommand{\Comp}{\mathrm{Comp}_C}
\newcommand{\CompPlus}{\overline{\mathrm{Comp}}_C}
\newcommand{\Star}{\mathrm{Star}}
\newcommand{\Lin}{\mathrm{Lin}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Proj}{\mathrm{Proj}}
\newcommand{\relint}{\mathrm{relint}}
\newcommand{\Clust}{\mathrm{Clust}}
\newcommand{\into}{\hookrightarrow}
\newcommand{\equivalent}{\Longleftrightarrow}
\newcommand{\onto}{\twoheadrightarrow}
\newcommand{\isomorph}{\cong}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Asym}{A_{\mathrm{sym}}}
\newcommand{\Cox}{\mathrm{Cox}}
\newcommand{\Des}{\mathrm{Des}}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\inv}{inv}
\newcommand{\odd}{\mathrm{odd}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\newcommand{\m}{\mathbf{m}}
\renewcommand{\c}{\mathbf{c}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\k}{\mathbbm{k}}
\newcommand{\kk}{\mathbf{k}}
\renewcommand{\ll}{{\boldsymbol\ell}}
\newcommand{\ks}{\mathbf{k}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\renewcommand{\t}{\mathbf{t}}
\renewcommand{\v}{\mathbf{v}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\tB}{{\tilde{B}}}
\newcommand{\tM}{{\widetilde{M}}}
\newcommand{\tN}{{\tilde{N}}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\ZP}{\mathbb{ZP}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\BB}{\mathbf{B}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\U}{\mathcal{U}}
\renewcommand{\O}{\mathcal{O}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Rel}{\operatorname{Rel}}
\newcommand{\Trop}{\operatorname{Trop}}
\newcommand{\pr}{{\operatorname{pr}}}
\newcommand{\bB}{\widebar{B}}
\renewcommand{\S}{\mathbf{S}}
\newcommand{\Clear}{\operatorname{Clear}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\Scat}{\operatorname{Scat}}
\newcommand{\Fan}{\operatorname{Fan}}
\newcommand{\ScatFan}{\operatorname{ScatFan}}
\newcommand{\ClusFan}{\operatorname{ClusFan}}
\newcommand{\CambScat}{\operatorname{CambScat}}
\newcommand{\ChamberFan}{\operatorname{ChamberFan}}
\newcommand{\Nar}{\operatorname{Nar}}
\newcommand{\can}{\operatorname{can}}
\renewcommand{\mid}{\operatorname{mid}}
\newcommand{\re}{\mathrm{re}}
\newcommand{\im}{\mathrm{im}}
%\newcommand{\init}{\mathrm{in}}
\renewcommand{\d}{{\mathfrak d}}
\newcommand{\bfd}{{\mathbf d}}
%\newcommand{\f}{{\mathfrak f}}
\newcommand{\seg}[1]{\overline{#1}}
\newcommand{\hy}{\hat{y}}
\newcommand{\ab}{\uparrow}
\newcommand{\bel}{\downarrow}

\newcommand{\complexes}{\mathbb{C}}
\newcommand{\Up}{\Upsilon}
\newcommand{\tUp}{\widetilde\Upsilon}
\newcommand{\cm}[3]{(#1\Vert#2)_{#3}}
\newcommand{\Cm}[3]{\bigl(#1\big\Vert#2\bigr)_{#3}}
\newcommand{\CM}[3]{\Bigl(#1\Big\Vert#2\Bigr)_{#3}}
\newcommand{\cmrarrow}{{\small{\rightarrow}}}
\newcommand{\cmlarrow}{{\small{\leftarrow}}}
\newcommand{\cmcircarrow}{{\small{\circlearrowright}}}
\newcommand{\cmr}[3]{(#1\cmrarrow#2)_{#3}}
\newcommand{\Cmr}[3]{\bigl(#1\cmrarrow#2\bigr)_{#3}}
\newcommand{\CMr}[3]{\Bigl(#1\cmrarrow#2\Bigr)_{#3}}
\newcommand{\cml}[3]{(#1\cmlarrow#2)_{#3}}
\newcommand{\Cml}[3]{\bigl(#1\cmlarrow#2\bigr)_{#3}}
\newcommand{\CMl}[3]{\Bigl(#1\cmltarrow#2\Bigr)_{#3}}
\newcommand{\cmcirc}[3]{(#1\,\cmcircarrow\,#2)_{#3}}
\newcommand{\Cmcirc}[3]{\bigl(#1\,\cmcircarrow\,#2\bigr)_{#3}}
\newcommand{\CMcirc}[3]{\Bigl(#1\,\cmcircarrow\,#2\Bigr)_{#3}}
\newcommand{\intnum}[2]{(#1\,|\,#2)}
\newcommand{\Phire}{\Phi^{\operatorname{re}}}
\newcommand{\dist}{\operatorname{dist}}
\renewcommand{\c}{{\mathbf c}}
\newcommand{\dd}{{\mathbf d}}
\newcommand{\aff}{\mathrm{aff}}
\newcommand{\fin}{\mathrm{fin}}
\renewcommand{\th}{^\text{th}}
%\newcommand{\laff}{<}
%\newcommand{\gaff}{>}
\newcommand{\laff}{\triangleleft}
\newcommand{\gaff}{\triangleright}
\newcommand{\DF}{{\mathcal {DF}}}
\newcommand{\DCScat}{{\operatorname{DCScat}}}
\newcommand{\adj}[2]{\operatorname{adj}_{#1}(#2)}
\newcommand{\Lower}{\operatorname{Lower}}
\newcommand{\Upper}{\operatorname{Upper}}


% Notation mess
% the space of eigenvectors of c
\newcommand{\eigenspace}[1]{U^{#1}}
% the full root system
\newcommand{\RSChar}{\Phi}
\newcommand{\RS}{\RSChar}
\newcommand{\RSre}{\RS^\re}
\newcommand{\RSpos}{\RS^+}
\newcommand{\RSneg}{\RS^-}
\newcommand{\RSfin}{\RS_\fin}
\newcommand{\RSfinpos}{\RSfin^+}
\newcommand{\RSfinneg}{\RSfin^-}
% simples in \RS
\newcommand{\SimplesChar}{\Pi}
\newcommand{\Simples}{\SimplesChar}
\newcommand{\simple}{\alpha}
% the root subsystem in \eigenspace (T is for "tubes")
\newcommand{\RSTChar}{\Upsilon}
\newcommand{\RST}[1]{\RSTChar^{#1}}
\newcommand{\RSTfin}[1]{\RST{#1}_\fin}
% simples in \RST
\newcommand{\SimplesTChar}{\Xi}
\newcommand{\SimplesT}[1]{\SimplesTChar^{#1}}
\newcommand{\simpleT}{\beta}
% Supports
\newcommand{\Supp}{\operatorname{Supp}_\SimplesChar}
\newcommand{\SuppT}{\operatorname{Supp}_\SimplesTChar}
% traversals for \tau-orbits
\newcommand{\TravInfChar}{\Psi}
\newcommand{\TravInf}[1]{\TravInfChar^{#1}}
\newcommand{\proj}{\to}
\newcommand{\TravProj}[1]{\overrightarrow{\TravInfChar}^{#1}}
\newcommand{\inj}{\leftarrow}
\newcommand{\TravInj}[1]{\overleftarrow{\TravInfChar}^{#1}}
%\newcommand{\proj}{\medvertdot}
%\newcommand{\TravProj}[1]{\TravInfChar^{#1}_\proj}
%\newcommand{\inj}{\dotmedvert}
%\newcommand{\TravInj}[1]{\TravInfChar^{#1}_\inj}
\newcommand{\TravRegChar}{\Omega}
\newcommand{\TravReg}[1]{\TravRegChar^{#1}}
% Schur roots
\newcommand{\AP}[1]{\RS_{#1}}
\newcommand{\APre}[1]{\AP{#1}^\re}
%\newcommand{\APT}[1]{\RST{#1}_{#1}}           % THIS NEEDS A BETTER NOTATION possibly \Lambda_c
%\newcommand{\APTre}[1]{\RST{#1;\re}_{#1}}     % THIS NEEDS A BETTER NOTATION possibly \Lambda_c^\re
\newcommand{\APTChar}{\Lambda}
\newcommand{\APT}[1]{\APTChar_{#1}}      
\newcommand{\APTre}[1]{\APT{#1}^\re}     


\newcommand{\fakesubsec}[1]{\medskip\noindent\textbf{#1.}}  %unnumbered

%\allowdisplaybreaks

%  Uncomment the following to remove all figures (useful for checking how many pages are taken up by figures)
%\usepackage{comment}
%\excludecomment{figure}
%\let\endfigure\relax



\newcommand{\afftype}[1]{{\widetilde{\raisebox{0pt}[6pt][0pt]{#1}}}}




% Commands for marginal notes below
\usepackage[draft]{say}
\newcommand{\saySS}[1]{\say[S]{#1}}
\newcommand{\sayS}[1]{\say[S]{#1}}
\newcommand{\sayN}[1]{\say[N]{#1}}
\newcommand{\sayD}[1]{\say[D]{#1}}
\newcommand{\sayDR}[1]{\say[D]{#1}}
\newcommand{\margin}[1]{\say[N]{#1}}
% control the width of your comments
\addtolength{\marginparwidth}{3mm}

%  If you want to switch which margin you're using, do the command  \switchmargin before your marginal comment.
% But it won't let you switch which margin you use in the middle of a paragraph of the main text.
% Also, you can only switch if there is room on the other margin.  (I.e. if you switch too often, things may overlap
\makeatletter
\newcommand{\switchmargin}{
\if@reversemargin
\normalmarginpar
\else
\reversemarginpar
\fi
}
\makeatother

%\newcommand{\response}[2]{{\color{red}#1:--}#2{\color{red}--:#1}}
\newcommand{\response}[2]{ {\color{red}#1:}~#2}
\newcommand{\rn}[1]{\response{NR}{#1}}
\newcommand{\rs}[1]{\response{SS}{#1}}

\newcommand{\ok}[1]{ {\color{blue}#1: OK }}
\newcommand{\okn}{\ok{NR}}
\newcommand{\oks}{\ok{SS}}

% A quick way to get rid of the red text.
%\renewcommand{\textcolor}[2]{}


\author{Nathan Reading}
\author{Dylan Rupel}
\author{Salvatore Stella}
\title{Dominance Regions for Affine Cluster Algebras}
\address[N. Reading]{Department of Mathematics, North Carolina State University, Raleigh, NC, USA}
\address[D. Rupel]{NEED THIS}
\address[S. Stella]{NEED THIS}
%\keywords{}
\thanks{Nathan Reading was partially supported by the Simons Foundation under award number 581608 and by the National Science Foundation under award number DMS-2054489.
Dylan Rupel was partially supported by ????.  
Salvatore Stella was partially supported by ????.  }
%\received{}
%\revised{}
%\accepted{}

\allowdisplaybreaks


\begin{document}

\begin{abstract}
NEED THIS
\end{abstract}

\maketitle

\vspace{-8pt}

\setcounter{tocdepth}{2}
\tableofcontents


\section{Introduction}

...

The first part of this paper contains a short proof of the first key fact about dominance regions due to Fan Qin:
Under certain hypotheses, if $\lambda$ is the $\g$-vector of a cluster monomial with respect to an initial extended exchange matrix $\tB$, then the dominance region $\P^\tB_\lambda$ is $\set\lambda$.
(Qin's result is an immediate consequence of combining parts (1) and (3) of \cite[Theorem~1.2.1]{FanQin}.)
Our proof takes stronger hypotheses than Qin's, \sayN{UPDATE:  Some versions of our proof take *weaker* hypotheses, specifically in finite type, we don't need $B$ to be full rank to get the coefficient-free result!}
but is sufficient for the purposes of the paper, uses only the basic discrete geometry of matrix mutation, and is short and self-contained.
The first part of the paper also introduces much of the background that will be used later.

\section{Dominance regions}
In this section, we define dominance regions \sayN{except maybe that will be in the intro?} and prove some foundational facts about them.
We assume the basic background on cluster algebras, following the approach of~\cite{ca4}.
Also, throughout the section, we use results from~\cite{NZ}.


\subsection{Definitions and background}\label{def sec}
We begin with a skew-symmetrizable exchange matrix~$B=[b_{ij}]$ and write $d_1,\ldots,d_n$ for the skew-symmetrizing constants of $B$ (so that $d_i b_{ij}=-d_j b_{ji}$ for all $i,j$).
The diagonal matrix $\Sigma$ with diagonal entries $d_1,\ldots,d_n$ has $\Sigma B\Sigma^{-1}=-B^T$.

Given a sequence $\kk=k_m\cdots k_1$ of indices in $\set{1,\ldots,n}$, we read the sequence from right to left for the purposes of matrix mutation.
That is, $\mu_\kk(B)$ means $\mu_{k_m}(\mu_{k_{m-1}}(\cdots(\mu_{k_1}(B))\cdots))$.
We write $\kk^{-1}$ for $k_1\cdots k_m$, the reverse of $\kk$.
Throughout, we will use without comment the fact that matrix mutation commutes with the maps $B\mapsto-B$ and $B\mapsto B^T$.

Given an exchange matrix $B$, the \newword{mutation map} $\eta^B_\kk:\reals^n\to\reals^n$ takes the input vector in $\reals^n$, places it as an additional row below $B$, mutates the resulting matrix according to the sequence $\kk$, and outputs the bottom row of the mutated matrix.
In this paper, it is convenient to think of vectors in $\reals^n$ as column vectors, and also, the mutation maps we need use transposes $B^T$ of exchange matrices.
Thus we write maps $\eta_\kk^{B^T}$.
This map takes a vector, places it as an additional \emph{column} to the right of $B$ (not $B^T$), does mutations according to $\kk$, and reads the rightmost column of the mutated matrix.

Given a vector $\lambda\in\reals^n$, define $\P^B_{\lambda,\kk}=\bigl(\eta_{\kk}^{B^T}\bigr)^{-1}\sett{\eta_\kk^{B^T}(\lambda)+B_t\alpha:\alpha\in\reals^n,\alpha\ge0}$, where the symbol $\ge$ denotes componentwise comparison.
(Throughout the paper, we will define sets indexed by vectors $\alpha\in\reals^n$ with $\alpha\ge0$, or sometimes $\alpha\in\reals^m$ with $\alpha\ge0$.
When we can do so without confusion, we will omit the explicit statement that $\alpha\in\reals^n$ or $\alpha\in\reals^m$.)
Define the \newword{dominance region} of $\lambda$ with respect to $B$ to be $\P^B_\lambda=\bigcap_\kk\P^B_{\lambda,\kk}$, where the intersection is over all sequences~$\kk$.
\begin{lemma}\label{shift}
If $\lambda'=\eta^{B^T}_\kk(\lambda)$ and $B'=\mu_\kk(B)$, then 
\begin{enumerate}[\quad\bf1.]
\item \label{shift all}
$\eta^{B^T}_\kk\!\!(\P^B_\lambda)=\P^{B'}_{\lambda'}$.
\item \label{shift one}
$\eta^{B^T}_\kk\!\!(\P^B_{\lambda,\ll})=\P^{B'}_{\lambda',\ll\kk^{-1}}$ for any $\ll$.
\end{enumerate}
\end{lemma}
\begin{proof}
For any $\ll$,
\begin{align*}
\eta^{B^T}_\kk\!\!(\P^B_{\lambda,\ll})
&=\eta^{B^T}_\kk\!\!\left(\bigl(\eta_{\ll}^{B^T}\bigr)^{-1}\settt{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\right)\\
%&=\left(\eta^{B^T}_\ll\!\!\left(\eta_{\kk}^{B^T}\right)^{-1}\right)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\\
&=\left(\eta^{B^T}_\ll\!\!\eta_{\kk^{-1}}^{\mu_\kk(B)^T}\right)^{-1}\settt{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\\
%&=\left(\eta^{\mu_\kk(B)^T}_{\ll\kk^{-1}}\right)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\\
&=\left(\eta^{\mu_\kk(B)^T}_{\ll\kk^{-1}}\right)^{-1}\settt{\eta^{\mu_\kk(B)^T}_{\ll\kk^{-1}}\left(\eta^{B^T}_\kk(\lambda)\right)+B_t\alpha:\alpha\ge0}\\
&=\P^{B'}_{\lambda',\ll\kk^{-1}}.
\end{align*}
Thus $\eta^{B^T}_\kk\!\!(\P^B_\lambda)=\bigcap_\ll\P^{B'}_{\lambda',\ll\kk^{-1}}=\P^{B'}_{\lambda'}$.
\end{proof}


For seeds $t_0$ and $t$ and an exchange matrix $B$, let $C_t^{B;t_0}$ be the matrix whose columns are the $C$-vectors at $t$ relative to the initial seed $t_0$ with exchange matrix~$B$.
Each column of $C_t^{B;t_0}$ is nonzero and all of its nonzero entries have the same sign.
(This is ``sign-coherence of $C$-vectors'', which was implicitly conjectured in \cite{FZ07} and proved as \cite[Corollary~5.5]{GHKK18}.)
Thus we will refer to the \newword{sign} of a column of $C_t^{B;t_0}$.
For $\kk=k_m\cdots k_1$, define seeds $t_1,\ldots,t_m$ by $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m$.
The sequence $\kk$ is a \newword{green sequence} for an exchange matrix $B$ if column $k_\ell$ of $C_{t_{\ell-1}}^{B;t_0}$ is \emph{positive} for all $\ell$ with $1\le\ell<m$.
A \newword{maximal green sequence} for $B$ is a green sequence that cannot be extended.
That is, a green sequence $\kk$ is a maximal green sequence if every column of $C_{t_m}^{B;t_0}$ is \emph{negative}.
We will call $\kk$ a \newword{red sequence} for~$B$ if it is a green sequence for $-B$.
A \newword{maximal red sequence} is a red sequence that cannot be extended.
(A red sequence relates to antiprincipal coefficients: 
If we were to define a version of $C$-vectors recursively starting with the negative of the identity matrix, the requirement for a red sequence would be that the $k_\ell$ column is negative at every step.)

Let $G_t^{B;t_0}$ be the matrix whose columns are the $\g$-vectors at $t$ relative to the initial seed $t_0$ with exchange matrix~$B$.
Let $\Cone^{B;t_0}_t$ be the nonnegative linear span of the columns of $G_t^{B;t_0}$.
For each $k\in\set{1,\ldots,n}$, the entries in the $k\th$ row of $G_t^{B;t_0}$ are not all zero and the nonzero entries have the same sign.
(This is ``sign-coherence of $\g$-vectors'', conjectured as \cite[Conjecture~6.13]{FZ07} and proved as \cite[Theorem 5.11]{GHKK18}.)
Thus all vectors in $\Cone^{B;t_0}_t$ all have weakly the same sign in the $k\th$ position.
The inverse of $G_t^{B;t_0}$ is $\bigl(C_t^{-B^T;t_0}\bigr)^T$.
(This is \cite[Theorem~1.2]{NZ12} or \cite[Theorem~1.1]{RS16} and \cite[Theorem~3.30]{RS16}.)
Thus $\Cone^{B;t_0}_t=\sett{x\in\reals^n:x^TC_t^{-B^T;t_0}\ge0}$, where $0$ is a row vector and ``$\ge$'' means componentwise comparison. 

Given $\kk$ with $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m$, let $B_i$ be the exchange matrix at~$t_i$, so that in particular, $B_0=B$.
The map $\eta_{\kk}^{B^T}$ is ${\eta_{k_m}^{B_{m-1}^T}\circ\cdots\circ\eta_{k_2}^{B_1^T}\circ\eta_{k_1}^{B_0^T}}$.
The definition of each $\eta_{k_i}^{B_{i-1}^T}$ has two cases, separated by the hyperplane $x_{k_i}=0$.
Two vectors are in the same \newword{domain of definition} of $\eta_\kk^{B^T}$ if, at every step, the same case applies for the two vectors.
(Both cases apply on the hyperplane, so domains of definition are closed.)
In particular, $\eta_\kk^{B^T}$ is linear in each of its domains of definition, but the domains of linearity of $\eta_\kk^{B^T}$ can be larger than its domains of definition.

There is a fan $\F_{B^T}$ called the \newword{mutation fan} for $B^T$ \cite[Definition~5.12]{universal}.
We will not need the details of the definition, but roughly, the cones of $\F_{B^T}$ are the intersections of domains of definition of all mutation maps $\eta_\kk^{B^T}$, as $\kk$ varies.
Thus for each $\kk$, each cone of $\F_{B^T}$ is contained in a domain of definition of $\eta_\kk^{B^T}$, and the mutation map $\eta_\kk^{B^T}$ is linear on every cone of $\F_{B^T}$ \cite[Proposition~5.3]{universal}.
Every cone $\Cone^{B;t_0}_t$ is a maximal cone in the mutation fan $\F_{B^T}$ \cite[Proposition~8.13]{universal}.
Thus in particular, the mutation map $\eta_\kk^{B^T}$ is linear on every cone $\Cone^{B;t_0}_t$.
Furthermore, $\Cone_t^{B_m;t_m}=\eta_\kk^{B^T}\bigl(\Cone_t^{B;t_0}\bigr)$ for every seed~$t$.
(This amounts to the initial seed mutation formula for $\g$-vectors, conjectured as \cite[Conjecture~7.12]{FZ07} and shown in \cite[Proposition~4.2(v)]{NZ12} to follow from sign-coherence of $C$-vectors.
The restatement in terms of mutation maps is \cite[Conjecture~8.11]{universal}.)
A similar fact applies to the entire mutation fan:
For any sequence $\kk$, the map $\eta_\kk^{B^T}$ induces an isomorphism of fans \cite[Proposition~7.3]{universal} from $\F_{B^T}$ to $\F_{\mu_\kk(B)^T}$.

\begin{remark}\label{conditional}
As written, \cite[Proposition~8.13]{universal} is conditional on the conjectured sign-coherence of $C$-vectors, which is now a theorem \cite[Corollary~5.5]{GHKK18}.
\end{remark}

We will need to relate the cones $\Cone^{B;t_0}_t$ and $\Cone^{-B^T;t_0}_t$.
It is immediate from \cite[Proposition~7.5]{universal} that $-B^T$ is a \newword{rescaling} of $B$, meaning that there is a diagonal matrix $\Sigma$ with positive entries on the diagonal such that $-B^T=\Sigma^{-1}B\Sigma$.
Therefore, \cite[Proposition~8.20]{universal} says that the $i\th$ column of $G_t^{-B^T;t_0}$ is a positive scalar multiple of the $i\th$ column of $\Sigma G_t^{B;t_0}$.
(In the statement of \cite[Proposition~8.20]{universal}, $\Sigma$ is multiplied on the right, because in~\cite{universal}, the $\g$-vectors are row vectors rather than column vectors.)
Thus we have the following fact.
\begin{lemma}\label{B or -BT}
For any $k$, the $k\th$ entries of vectors in $\Cone^{B;t_0}_t$ have the same sign as the $k\th$ entries of vectors in $\Cone^{-B^T;t_0}_t$.
\end{lemma}
%Therefore, \cite[Proposition~8.20]{universal} implies the following lemma.
%(In the statement of \cite[Proposition~8.20]{universal}, $\Sigma$ is multiplied on the right, because there $\g$-vectors are row vectors rather than column vectors.)
%\begin{lemma}\label{SigmaG}
%For $i=1,\ldots,n$, the $i\th$ column of $G_t^{-B^T;t_0}$ is a positive scalar multiple of the $i\th$ column of $\Sigma G_t^{B;t_0}$.
%\end{lemma}
%The following lemma is an immediate consequence.
%\begin{lemma}\label{B or -BT}
%The $k\th$ entries of vectors in $\Cone^{B;t_0}_t$ have the same sign as the $k\th$ entries of vectors in $\Cone^{-B^T;t_0}_t$.
%\end{lemma}

For $k\in\set{1,\ldots,n}$, let $J_k$ be the $n\times n$ matrix that agrees with the identity matrix except that $J_k$ has $-1$ in position $kk$.
For an $n\times n$ matrix $M$ and $k\in\set{1,\ldots,n}$, let $M^{\bullet k}$ be the matrix that agrees with $M$ in column $k$ and has zeros everywhere outside of column $k$.
Let $M^{k\bullet}$ be the matrix that agrees with $M$ in row $k$ and has zeros everywhere outside of row $k$.
Given a real number $a$, let $[a]_+$ denote $\max(a,0)$.
Given a matrix $M=[m_{ij}]$, define $[M]_+$ to be the matrix whose $ij$-entry is $[m_{ij}]_+$.
Given an exchange matrix~$B$, an index $k\in\set{1,\ldots,n}$ and a sign $\ep\in\set{\pm1}$, define
\begin{align*}
E_{\ep,k}^B&=J_k+[\ep B]_+^{\bullet k}\\
F_{\ep,k}^B&=J_k+[-\ep B]_+^{k\bullet}.
\end{align*}
Each matrix $E_{\ep,k}^B$ is its own inverse, and each $F_{\ep,k}^B$ is its own inverse.
We note that $(E_{\ep,k}^B)^T=J_k+[\ep B^T]_+^{k\bullet}=F_{\ep,k}^{-B^T}=\Sigma F_{\ep,k}^B\Sigma^{-1}$, where~$\Sigma$ is the diagonal matrix of skew-symmetrizing constants of~$B$.
The following is essentially a result of \cite{NZ12}, although it is not stated there in this form.  \margin{Do I have this attribution right?}
\begin{lemma}\label{EBF trick}
For $k\in\set{1,\ldots,n}$ and either choice of $\ep\in\set{\pm1}$, the mutation of $B$ at $k$ is $\mu_k(B)=E_{\ep,k}^BBF_{\ep,k}^B$.
\end{lemma}
\begin{proof}
We expand the product $(J_k+[\ep B]_+^{\bullet k})B(J_k+[-\ep B]_+^{k\bullet})$ to four terms.
The term $[\ep B]_+^{\bullet k}B[-\ep B]_+^{k\bullet}$ is zero because $B$ is skew-symmetric.
The term $[\ep B]_+^{\bullet k}BJ_k$ is $[\ep B]_+^{\bullet k}B^{k\bullet}J_k$, which equals $[\ep B]_+^{\bullet k}B^{k\bullet}$ because $b_{kk}=0$.
Similarly, the term $J_kB[-\ep B]_+^{k\bullet}$ equals $B^{\bullet k}[-\ep B]_+^{k\bullet}$.
%\begin{align*}
%(J_k+[\ep B]_+^{\bullet k})B(J_k+[-\ep B]_+^{k\bullet})
%&=J_kBJ_k+J_kB[-\ep B]_+^{k\bullet}+[\ep B]_+^{\bullet k}BJ_k+[\ep B]_+^{\bullet k}B[-\ep B]_+^{k\bullet}.
%\end{align*}
Thus the $ij$-entry of $E_{\ep,k}^BBF_{\ep,k}^B$ is 
\[
\left\{\begin{aligned}
-b_{ij}&\quad\text{if }k\in\set{i,j}\\
b_{ij}&\quad\text{otherwise}
\end{aligned}\right\}
+
\left\{\begin{aligned}
|b_{ik}|b_{kj}&\quad\text{if }\sgn b_{ik}=\ep\\
0&\quad\text{otherwise}
\end{aligned}\right\}
+
\left\{\begin{aligned}
b_{ik}|b_{kj}|&\quad\text{if }\sgn b_{kj}=-\ep\\
0&\quad\text{otherwise}
\end{aligned}\right\}.
\]
This coincides with the $ij$-entry of $\mu_k(B)$.
\end{proof}

Given a matrix $M$, write $M_{\col(j)}$ for the $j\th$ column of $M$.
For matrices $M$ and~$N$ such that $MN$ is defined, we observe that $(MN)_{\col j}=M(N)_{\col j}$.
\begin{lemma}\label{columns lem}
Suppose $B=[b_{ij}]$ is an exchange matrix, let $k\in\set{1,\ldots,n}$, and choose a sign $\ep\in\set{\pm1}$.
\begin{enumerate}[\quad\bf1.]
\item \label{col j}
$(E_{\ep,k}^BB)_{\col j}=J_k(B)_{\col j}+b_{kj}([\ep B]_+)_{\col k}$.
\item \label{col k}
$(E_{\ep,k}^BB)_{\col k}=(E_{-\ep,k}^BB)_{\col k}=B_{\col k}$.
\item \label{cols k}
$(E_{-\ep,k}^BB)_{\col j}=(E_{\ep,k}^BB)_{\col j}-\ep b_{kj}B_{\col k}$.
\end{enumerate}
\end{lemma}
\begin{proof}
The first two assertions hold because $(MN)_{\col j}=M(N)_{\col j}$ and because $b_{kk}=0$.
By the first assertion, $(E_{-\ep,k}^BB)_{\col j}=(E_{\ep,k}^BB)_{\col j}-b_{kj}([\ep B]_+-[-\ep B]_+)_{\col k}$.  
The third assertion follows.
\end{proof}

%We will also need the following simple fact about nonnegative linear spans.
Let~$\nnspan(S)$ denote the nonnegative linear span of a set $S$ of vectors.
For ${k\in\set{1,\ldots,n}}$ and $\ep\in\set{\pm1}$, let $S_{k,\ep}$ be the set of vectors in $S$ whose $k\th$ entry has sign strictly agreeing with $\ep$.

\begin{lemma}\label{ps lemma}
Suppose $\lambda$ is a vector in $\reals^n$ whose $k\th$ entry $\lambda_k$ has $\ep\lambda_k\le0$.
Then %any vector in $\set{\lambda+\nnspan(S)}\cap\set{x\in\reals^n:\sgn x_k=\ep}$ can be written as a vector in $\set{\lambda+\nnspan(S)}\cap\set{x\in\reals^n:x_k=0}$ plus a vector in $nnspan(S_{k,\ep})$.
\begin{multline*}
\sett{\lambda+\nnspan(S)}\cap\set{x\in\reals^n:\ep x_k\ge0}\\
=\bigl(\sett{\lambda+\nnspan(S)}\cap\set{x\in\reals^n:x_k=0}\bigr)+\nnspan(S_{k,\ep}).
\end{multline*}
\end{lemma}
\begin{proof}
The set on the right side is certainly contained in the set on the left side.
If~$x$ is an element of the left side, then $x$ is $\lambda$ plus a nonzero element $y$ of $\nnspan(S_{k,\ep})$ plus an element $z$ of $\nnspan(S\setminus S_{k,\ep})$.
Since $\ep x_k\ge0$ and $\ep\lambda_k\le0$, there exists~$t$ with ${0\le t\le1}$ such that $\lambda+ty+z$ has $k\th$ entry $0$.
Thus ${x=(\lambda+ty+z)+(1-t)y}$ is an element of the right side.
\end{proof}

\subsection{Linearizing the dominance region}\label{lin sec}
The difficult thing about computing the regions $\P^B_{\lambda,\kk}$ whose intersection is the dominance region $\P^B_\lambda$ is applying the piecewise linear map $\bigl(\eta^{B^T}_\kk\bigr)^{-1}$, because the number of its domains of definition may grow without bound as the length of $\kk$ increases.
In this section, we describe the cones (at $\lambda$) that coincide with $\P^B_{\lambda,\kk}$ inside a domain of definition of $\bigl(\eta^{B^T}_\kk\bigr)^{-1}$ containing~$\lambda$ and show, in some cases, that these cones contain $\P^B_{\lambda,\kk}$.
We work here with square exchange matrices.
In \cref{ext sec}, we extend the result to tall extended exchange matrices.
%As an application, we prove in \cref{ext sec} that (when $B$ is replaced by an extended exchange matrix with linearly independent columns) that the dominance region of a point $\lambda$ in the $\g$-vector fan... hypotheses... never mind

Let $B_0$ be an exchange matrix.
For a sequence $\kk=k_m\cdots k_1$ of indices, define seeds $t_1,\ldots,t_m=t$ by $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
We will prove the following \lcnamecref{P in B0C}.
%
%\noindent
%START ALTERNATIVE WORDING
%
%There may be more than one sequence connecting $t_0$ to $t$.
%
%\noindent
%NOT TRUE:\\
%The mutation map $\eta_\kk^{B_0^T}$ depends only on $t$, not on the choice of $\kk$.
%
%\noindent
%HOWEVER, we can rescue the following (by showing that different $\kk$s with the same $t$ are related by a global permutation of rows/coumns):\\
%AFTER MORE THOUGHT:  I think that's probably true, but there are some issues.
%It would come down to showing that there is only one automorphism of the mutation fan that fixes the positive cone pointwise, and that's probably true, but would take some thought, and it's not really necessary for what we're trying to accomplish this spring.
%(And actually, it's not true as I just stated it... Think about Markov.  
%But for piecewise linear automorphisms of the mutation fan with the pieces separated by codim-1 cones that contain codim-1 faces of the fan, it's probable true.
%That's the kind of complication we would need to deal with.
%
%Thus we define $\P^{B_0}_{\lambda,t}$ to be $\P^{B_0}_{\lambda,\kk}$ for any sequence $\kk=k_m\cdots k_1$ with $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
%Our first main result is about $\P^{B_0}_{\lambda,t}$ in the case where $\lambda$ is in $\Cone^{B_0;t_0}_t$.
%
%Our first main result is about $\P^{B_0}_{\lambda,\kk}$ in the case where $\lambda$ is in $\Cone^{B_0;t_0}_t$ for some sequence $\kk$.
%
%
%\begin{theorem}\label{P in B0C}
%Fix an exchange pattern with $B_0$ at $t_0$.
%For some vertex $t$, suppose there exists a red sequence for $B_t$ that ends at $t_0$.
%Then for $\lambda\in\Cone^{B_0;t_0}_t$,
%\[\P^{B_0}_{\lambda,t}\subseteq\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}.\]
%\end{theorem}
%
%\noindent
%END ALTERNATIVE WORDING
%

\begin{theorem}\label{P in B0C}
Suppose $\kk=k_m\cdots k_1$ and $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
If $\kk^{-1}=k_1\cdots k_m$ is a red sequence for $B_t$, then for any~$\lambda$ in the domain of definition of $\eta_\kk^{B_0^T}$ that contains $\Cone^{B_0;t_0}_t$,
\[\P^{B_0}_{\lambda,\kk}\subseteq\set{\lambda+G_t^{B_0;t_0}B_t\alpha:\alpha\in\reals^n,\alpha\ge0}=\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}.\]
\end{theorem}

Since $\bigl(\eta_{\kk}^{B_0^T}\bigr)^{-1}=\eta_{\kk^{-1}}^{B_t^T}$, we have $\P^{B_0}_{\lambda,\kk}=\eta_{\kk^{-1}}^{B_t^T}\sett{\eta_\kk^{B_0^T}(\lambda)+B_t\alpha:\alpha\ge0}$.
Let $D$ be the domain of definition of $\eta_{\kk}^{B_0^T}$  that contains $\Cone^{B_0;t_0}_t$.
Then $\eta_{\kk^{-1}}^{B_t^T}$ is linear on $\eta_{\kk}^{B_0^T}(D)$.
Let~$\L_{\kk^{-1}}^{B_t^T}$ be the linear map that agrees with $\eta_{\kk^{-1}}^{B_t^T}$ on~$\eta_{\kk}^{B_0^T}(D)$.

\begin{proposition}\label{L mat}
The matrix for $\L_{\kk^{-1}}^{B_t^T}$, acting on column vectors, is $G_t^{B_0;t_0}$.
\end{proposition}
\begin{proof}
By \cite[Proposition~8.13]{universal}, $\Cone^{B_0;t_0}_t=\eta_{\kk^{-1}}^{B_t^T}\left(\left(\reals_{\ge0}\right)^n\right)$, and therefore also ${\eta_\kk^{B_0^T}\bigl(\Cone^{B_0;t_0}_t\bigr)=\left(\reals_{\ge0}\right)^n}$.
The proof of \cite[Proposition~8.13]{universal} shows not only an equality of cones, but also that $\eta_{\kk^{-1}}^{B_t^T}$ takes the extreme ray of $\left(\reals_{\ge0}\right)^n$ spanned by $e_i$ to the extreme ray of $\Cone^{B_0;t_0}_t$ spanned by the $i\th$ $\g$-vector at $t$ relative to $B_0;t_0$, where the total order on these $\g$-vectors at $t$ is obtained from the order $e_1,\ldots,e_n$ on $\g$-vectors at $t_0$ by the sequence $\kk$ of mutations.
\end{proof}

The following is a result of \cite{NZ12}.
It follows from the proof of \cite[Proposition~1.3]{NZ12}, or from \cite[(6.14)]{FZ07}, as explained in \cite[Remark~2.1]{NZ12}.

\begin{proposition}\label{GBBC}
$G_t^{B_0;t_0}B_t=B_0C_t^{B_0;t_0}$.
\end{proposition}

We can rewrite \cref{GBBC} as follows.

\begin{proposition}\label{BGCB}
$B_t\bigl(G_t^{-B_0^T;t_0}\bigr)^T=\bigl(C_t^{-B_0^T;t_0}\bigr)^TB_0$.
\end{proposition}
\begin{proof}
We use \cite[Theorem~1.2]{NZ} to rewrite $G_t^{B_0;t_0}$ as $\bigl(C_t^{-B_0^T;t_0}\bigr)^{-T}$ and $C_t^{B_0;t_0}$ as $\bigl(G_t^{-B_0^T;t_0}\bigr)^{-T}$, so \cref{GBBC} says that $\bigl(C_t^{-B_0^T;t_0}\bigr)^{-T}B_t=B_0\bigl(G_t^{-B_0^T;t_0}\bigr)^{-T}$.
\end{proof}

Since $G_t^{B_0;t_0}$ is the matrix for~$\L_{\kk^{-1}}^{B_t^T}$ and since $\L_{\kk^{-1}}^{B_t^T}\eta_\kk^{B_0^T}(\lambda)=\lambda$, the following result is immediate from \cref{GBBC}.

\begin{proposition}\label{B0C}
\begin{align*}
\L_{\kk^{-1}}^{B_t^T}\sett{\eta_\kk^{B_0^T}(\lambda)+B_t\alpha:\alpha\in\reals^n,\alpha\ge0}
&=\sett{\lambda+G_t^{B_0;t_0}B_t\alpha:\alpha\in\reals^n,\alpha\ge0}\\
&=\sett{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}.
\end{align*}
\end{proposition}

In light of \cref{B0C}, the conclusion of \cref{P in B0C} is equivalent to
\[\P^{B_0}_{\lambda,\kk}\subseteq\L_{\kk^{-1}}^{B_t^T}\sett{\eta_\kk^{B_0^T}(\lambda)+B_t\alpha:\alpha\ge0}.\]


\begin{proof}[Proof of \cref{P in B0C}]
We will prove that $P^{B_0}_{\lambda,\kk}\subseteq\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}$ by induction on $m$ (the length of $\kk$).
The base case, where $\kk=\emptyset$, is true because $C_{t_0}^{B_0;t_0}$ is the identity matrix and $\P_{\lambda,\emptyset}=\set{\lambda+B_0\alpha:\alpha\ge0}$.
 
\cite[Proposition~1.4]{NZ12} says that $C_t^{B_0;t_0}=F^{B_1}_{\ep,k_1}C_t^{B_1;t_1}$, where $\ep$ is the sign of the $k_1$-column of $C_{t_1}^{-B_t;t}$.  
(The hypothesis that $\kk^{-1}$ is a red sequence for $B_t$ determines $\ep$, but we leave $\ep$ unspecified for now in order to highlight later where this hypothesis is relevant.)
By \cref{EBF trick} and because $E^{B_1}_{\ep,k_1}$ and $F^{B_1}_{\ep,k_1}$ are their own inverses,
\begin{equation}\label{ind B0C}\begin{aligned}
\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}
&=\set{\lambda+B_0F^{B_1}_{\ep,k_1}C_t^{B_1;t_1}\alpha:\alpha\ge0}\\
&=\set{\lambda+E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}\\
&=E^{B_1}_{\ep,k_1}\set{E^{B_1}_{\ep,k_1}\lambda+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.
\end{aligned}\end{equation}

The map $\eta_{\kk}^{B_0^T}$ is linear on $\Cone_t^{B_0;t_0}$.  
This map is $\eta_{\kk}^{B_0^T}={\eta_{k_m}^{B_{m-1}^T}\circ\cdots\circ\eta_{k_2}^{B_1^T}\circ\eta_{k_1}^{B_0^T}}$.
The map $\eta_{k_1}^{B_0^T}$ restricts to a linear map from $\Cone_t^{B_0;t_0}$ to $\Cone_t^{B_1;t_1}$.
The inverse of $\eta_{k_1}^{B_0^T}$ is $\eta_{k_1}^{B_1^T}$.
We claim that $E^{B_1}_{\ep,k_1}$ is the matrix for the linear map on column vectors that agrees with $\eta_{k_1}^{B_1^T}$ on $\Cone_t^{B_1;t_1}$.
Since $E^{B_1}_{\ep,k_1}$ is its own inverse, the claim is equivalent to saying that implies that $E^{B_1}_{\ep,k_1}$ is the linear map that agrees with $\eta_{k_1}^{B_0^T}$ on $\Cone_t^{B_0;t_0}$.

By \cite[(1.13)]{NZ12}, $\ep$ is the sign of the $k_1$-column of $\bigl(G_t^{-B_1^T;t_1}\bigr)^T$.
That is, $\ep$ is the sign of the $k_1$-row of $G_t^{-B_1^T;t_1}$, or in other words, the sign of the $k_1$-entry of vectors in $\Cone_t^{-B^T_1;t_1}$.
By \cref{B or -BT}, $\ep$ is the sign of the $k_1$-entry of vectors in $\Cone_t^{B_1;t_1}$, which is the sign that determines how $\eta_{k_1}^{B_1^T}$ acts on $\Cone_t^{B_1;t_1}$.
Now, one easily checks that the action of $\eta_{k_1}^{B_1^T}$ on vectors whose $k_1$-entry has sign $\ep$ is precisely the action of $E^{B_1}_{\ep,k_1}$.
%It negates the $k_1$ entry.
%The $i\th$ ($i\neq k$) entry gets sent to itself plus the $k_1$ entry times $(B_1)_{ik}$ times $\ep$, if $(B_1)_{ik}$ also has sign $\ep$.

Let $\lambda'=\eta_{k_1}^{B_0^T}(\lambda)$, so that $\lambda'$ is in the same domain of definition of $\eta_{k_m\cdots k_2}^{B_1^T}$ as $\Cone_t^{B_1;t_1}$ and so that $\lambda'=E^{B_1}_{\ep,k_1}\lambda$.
By induction on $m$, 
\[\eta_{k_2\cdots k_m}^{B_t^T}\set{\eta_{k_m\cdots k_2}^{B_1^T}(\lambda')+B_t\alpha:\alpha\ge0}\subseteq\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.\]
Applying the homeomorphism $\eta_{k_1}^{B_1^T}$ to both sides, we obtain
\[\eta_{\kk^{-1}}^{B_t^T}\set{\eta_\kk^{B_0^T}(\lambda')+B_t\alpha:\alpha\ge0}\subseteq\eta_{k_1}^{B_1^T}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.\]
In light of \eqref{ind B0C}, we can complete the proof by showing that
\[\eta_{k_1}^{B_1^T}\sett{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}\subseteq E^{B_1}_{\ep,k_1}\sett{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.\]

We have seen that $E^{B_1}_{\ep,k_1}$ is the linear map that agrees with $\eta_{k_1}^{B_1^T}$ on the set $\set{x\in\reals^n:\sgn x_{k_1}=\ep}$.
We can similarly check that $E^{B_1}_{-\ep,k_1}$ is the linear map that agrees with $\eta_{k_1}^{B_1^T}$ on $\set{x\in\reals^n:\sgn x_{k_1}=-\ep}$.
Thus $\eta_{k_1}^{B_1^T}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}$ is
\[(U\cap\set{x\in\reals^n:\sgn x_{k_1}=-\ep})\cup(V\cap\set{x\in\reals^n:\sgn x_{k_1}=\ep}),\]
where, writing $\nnspan$ for the nonnegative linear span of a set of vectors,
{\small
\begin{align*}
U&=E^{B_1}_{\ep,k_1}\sett{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}=E^{B_1}_{\ep,k_1}\lambda'+\nnspan\settt{\bigl(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\bigr)_{\col i}}_{i=1}^n\\
V&=E^{B_1}_{-\ep,k_1}\sett{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}=E^{B_1}_{-\ep,k_1}\lambda'+\nnspan\settt{\bigl(E^{B_1}_{-\ep,k_1}B_1C_t^{B_1;t_1}\bigr)_{\col i}}_{i=1}^n.
\end{align*}
}

We need to show that $V\cap\set{x\in\reals^n:\sgn x_{k_1}=\ep}\subseteq U$.
Since $\eta_{k_1}^{B_1^T}$ is a homeomorphism that fixes $\set{x\in\reals^n:x_{k_1}=0}$ pointwise, $U\cap\set{x\in\reals^n:x_{k_1}=0}$ equals $V\cap\set{x\in\reals^n:x_{k_1}=0}$.
By \cref{ps lemma}, any vector in $V\cap\set{x\in\reals^n:\sgn x_{k_1}=\ep}$ equals a vector in $V\cap\set{x\in\reals^n:x_{k_1}=0}$ plus a positive combination of vectors $\bigl(E^{B_1}_{-\ep,k_1}B_1C_t^{B_1;t_1}\bigr)_{\col i}$ whose $k_1$-entry has sign $\ep$.
%Write $v_i$ for the vector $\bigl(E^{B_1}_{-\ep,k_1}B_1C_t^{B_1;t_1}\bigr)_{\col i}$.
Therefore, it suffices to show that every vector $\bigl(E^{B_1}_{-\ep,k_1}B_1C_t^{B_1;t_1}\bigr)_{\col i}$ whose $k_1$-entry has sign~$\ep$ is contained in $\nnspan\sett{\bigl(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\bigr)_{\col i}}_{i=1}^n$.

As a temporary shorthand, write $b_{ij}$ for the entries of $B_1$ and write $k$ for $k_1$.
Suppose $v_i=\bigl(E^{B_1}_{-\ep,k}B_1C_t^{B_1;t_1}\bigr)_{\col i}$ for some~$i$ and suppose the $k$-entry of $v_i$ has sign $\ep$.
Write $M$ for $E^{B_1}_{-\ep,k}B_1$ and write $N$ for $E^{B_1}_{\ep,k}B_1$.
\cref{columns lem}.\ref{col j} implies that $M_{kj}=-b_{kj}$ for all~$j$.
\cref{columns lem}.\ref{cols k} implies that if $\ep M_{kj}\ge0$, then $M_{\col j}=N_{\col j}+|b_{kj}|N_{\col k}$.
Similarly, if $\ep M_{kj}\le0$, then $M_{\col j}=N_{\col j}-|b_{kj}|N_{\col k}$.

Now $v_i=E^{B_1}_{-\ep,k}B_1\bigl(C_t^{B_1;t_1}\bigr)_{\col i}$, and $\bigl(C_t^{B_1;t_1}\bigr)_{\col i}$ has a sign $\delta\in\set{\pm1}$, meaning that it is not zero and all of its nonzero entries have sign $\delta$.
(This is ``sign-coherence of $C$-vectors''.  
See Remark~\ref{conditional}.)
Thus there are nonnegative numbers $\gamma_j$ such that $v_i=\delta\sum_{j=1}^n\gamma_jM_{\col j}$.
Write $\set{1,\ldots,n}=S\cup T$ with $S\cup T=\emptyset$ such that $\ep M_{kj}\ge0$ for all $j\in S$ and $\ep M_{kj}\le0$ for all $j\in T$.
Then
\begin{align*}
v_i
&=\delta\sum_{j\in S}\gamma_jM_{\col j}+\delta\sum_{j\in T}\gamma_jM_{\col j}\\
&=\delta\sum_{j\in S}\gamma_j(N_{\col j}+|b_{kj}|N_{\col k})+\delta\sum_{j\in T}\gamma_j(N_{\col j}-|b_{kj}|N_{\col k})\\
&=\delta\sum_{j=1}^n\gamma_jN_{\col j}-\delta\sum_{j=1}^n\ep\gamma_jb_{kj}N_{\col k}\\
&=N\bigl(C_t^{B_1;t_1}\bigr)_{\col i}+\delta\sum_{j=1}^n\ep\gamma_jM_{kj}N_{\col k}\\
&=N\bigl(C_t^{B_1;t_1}\bigr)_{\col i}+\sigma N_{\col k},
\end{align*}
where $\sigma=\ep\delta\sum_{j=1}^n\gamma_jM_{kj}$ is a positive scalar, because $\delta\sum_{j=1}^n\gamma_jM_{kj}$ is the $k$-entry of $v_i$, which has sign $\ep$.

As noted above, $\ep$ is the sign of the $k_1$-entry of vectors in $\Cone_t^{-B_1^T;t_1}$.
Since $\Cone^{-B_1^T;t_1}_t=\set{x\in\reals^n:x^TC_t^{B_1;t_1}\ge0}$, the rows of $\bigl(C_t^{B_1;t_1}\bigr)^{-1}$ span the extreme rays of $\Cone_t^{-B_1^T;t_1}$.
In particular $\bigl(C_t^{B_1;t_1}\bigr)^{-1}(\ep e_k)$ has nonnegative entries.
Thus $C_t^{B_1;t_1}\bigl(C_t^{B_1;t_1}\bigr)^{-1}(\ep e_k)=\ep e_k$ is a nonnegative linear combination of columns of~$C_t^{B_1;t_1}$.

Now, the hypothesis that $\kk^{-1}$ is a red sequence for $B_t$, or equivalently a green sequence for $-B_t$, says that $\ep=+1$, so that $e_k$ is a nonnegative linear combination of columns of~$C_t^{B_1;t_1}$.
Thus $N_{\col k}=Ne_k$ is a nonnegative linear combination of columns of~$NC_t^{B_1;t_1}$.
We have shown that $v_i=N\bigl(C_t^{B_1;t_1}\bigr)_{\col i}+\sigma N_{\col k}$ is a nonnegative linear combination of columns of~$NC_t^{B_1;t_1}$.
In other words, $v_i$ is in $\nnspan\sett{\bigl(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\bigr)_{\col i}}_{i=1}^n$, as desired.
\end{proof}

\subsection{Dominance regions for extended exchange matrices}\label{ext sec}
We follow \cite{FZ07} in considering $m\times n$ extended exchange matrices $\tB$ that are ``tall'', in the sense that $m\ge n$.
We will also consider $m\times m$ matrices related to $\tB$:
Writing $\tB$ in block form $\begin{bsmallmatrix}B\\R\end{bsmallmatrix}$, let $\BB$ be the matrix with block form $\begin{bsmallmatrix}B&-R^T\\R&0\end{bsmallmatrix}$.
Most importantly, $\BB$ is skew-symmetrizable and agrees with $\tB$ in columns $1$ to $n$.
Throughout, if we have defined an extended exchange matrix $\tB$, without comment we will take $B$ to be the underlying exchange matrix and $\BB$ to be the associated $m\times m$ matrix.

The matrix $\BB$ defines mutation maps $\eta_\kk^{\BB^T}$ that act on $\reals^m$ rather than $\reals^n$, but without exception we will only consider mutations in positions $1,\ldots,n$.
Also, given $\BB$, a sequence $\kk=k_m\cdots k_1$ of indices in $\set{1,\ldots,n}$, and seeds $t_1,\ldots,t_m$ by $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$, there are associated matrices of $\g$-vectors and $C$-vectors, which we write as $\GG_t^{\BB;t_0}$ and $\CC_t^{\BB;t_0}$.
Since $\kk$ only contains indices in $\set{1,\ldots,n}$, these matrices have block forms
\[
\GG_t^{\BB;t_0}=\begin{bsmallmatrix}G_t^{B;t_0}&0\\H_t^{\tB;t_0}&I_{m-n}\end{bsmallmatrix}
\quad\text{ and }\quad
\CC_t^{\BB;t_0}=\begin{bsmallmatrix}C_t^{B;t_0}&D_t^{\tB;t_0}\\0&I_{m-n}\end{bsmallmatrix},
\]
where $H_t^{\tB;t_0}$ is an $(m-n)\times n$ matrix, $D_t^{\tB;t_0}$ is an $n\times(m-n)$ matrix, and $I_{m-n}$ is the identity matrix.

\begin{proposition}\label{GBBC ext}
$\GG_t^{\BB_0;t_0}\tB_t=\tB_0C_t^{B;t_0}$.
\end{proposition}
\begin{proof}
\cref{GBBC} says that $\GG_t^{\BB_0;t_0}\BB_t=\BB_0\CC_t^{\BB_0;t_0}$.
Restricted to the first $n$ columns, this says $\GG_t^{\BB_0;t_0}\tB_t=\BB_0\begin{bsmallmatrix}C_t^{B;t_0}\\0\end{bsmallmatrix}$.
\end{proof}

\begin{proposition}\label{BGCB ext}
$\tB_t\bigl(G_t^{-B_0^T;t_0}\bigr)^T=\bigl(\CC_t^{-\BB_0^T;t_0}\bigr)^T\tB_0$.
\end{proposition}
\begin{proof}
\cref{BGCB} says that $\BB_t\bigl(\GG_t^{-\BB_0^T;t_0}\bigr)^T=\bigl(\CC_t^{-\BB_0^T;t_0}\bigr)^T\BB_0$.
Restricted to the first $n$ columns, this says $\BB_t\bigl(\begin{bsmallmatrix}G_t^{-B^T;t_0}&\,0\end{bsmallmatrix}\bigr)^T=\bigl(\CC_t^{-\BB_0^T;t_0}\bigr)^T\tB_0$.
\end{proof}

Given a vector $\lambda\in\reals^m$, define $\P^\tB_{\lambda,\kk}=\bigl(\eta_{\kk}^{\BB^T}\bigr)^{-1}\sett{\eta_\kk^{\BB^T}(\lambda)+\tB_t\alpha:\alpha\in\reals^n,\alpha\ge0}$.
Define the \newword{dominance region} $\P^\tB_\lambda$ of $\lambda$ with respect to $\tB$ to be the intersection $\bigcap_\kk\P^\tB_{\lambda,\kk}$ over all sequences~$\kk$ of indices in $\set{1,\ldots,n}$.

We relate the dominance region $\P_\lambda^\tB$ with respect to the extended exchange matrix $\tB$ to the dominance region $\P_\lambda^B$ with respect to the (non-extended) exchange matrix $B$.
Let $\Proj_n$ be the projection from $\reals^m$ to $\reals^n$ that ignores the last $m-n$ coordinates.

\begin{proposition}\label{contains proj}
Let $\tB$ be an extended exchange matrix with underlying exchange matrix $B$.
If $\lambda\in\reals^m$ and $\lambda'=\Proj_n\lambda\in\reals^n$, then $\Proj_n(\P_\lambda^\tB)\subseteq\P_{\lambda'}^B$.
\end{proposition}
\begin{proof}
It is apparent that $\Proj_n\circ\eta_\kk^{\BB^T}=\eta_\kk^{B^T}\circ\Proj_n$ as maps on $\reals^n$, for any sequence $\kk$ of indices in $\set{1,\ldots,n}$.
Since $\bigl(\eta_\kk^{\BB^T}\bigr)^{-1}=\eta_{\kk^{-1}}^{\mu_\kk(\BB)^T}$, the analogous fact is true for $\bigl(\eta_\kk^{\BB^T}\bigr)^{-1}$.
Also, $\Proj_n\set{\tB_t\alpha:\alpha\in\reals^n,\alpha\ge0}=\set{B_t\alpha:\alpha\in\reals^n,\alpha\ge0}$.
The \lcnamecref{contains proj} follows.
\end{proof}

Since $\kk$ consists only of indices in $\set{1,\ldots,n}$, the domains of definition of $\eta_\kk^{\BB^T}$ are determined by the domains of definition of $\eta_\kk^{B^T}$.
Specifically, each domain of definition of $\eta_\kk^{\BB^T}$ is $\Proj_n^{-1}D$ for some domain $D$ of definition of $\eta_\kk^{B^T}$.
Accordingly, we define $\Cone_t^{\tB;t_0}$ to be %the nonnegative span of the columns of $\GG_t^{\BB;t_0}$ and the columns of $\begin{bsmallmatrix}0\\-I_{m-n}\end{bsmallmatrix}$.
$\Proj_n^{-1}\Cone_t^{B;t_0}$.
Since $\Cone_t^{B_m;t_m}=\eta_\kk^{B^T}\bigl(\Cone_t^{B;t_0}\bigr)$ for every seed~$t$, also $\Cone_t^{\tB_m;t_m}=\eta_\kk^{\BB^T}\bigl(\Cone_t^{\tB;t_0}\bigr)$ for every seed~$t$.

Similarly, we define the mutation fan for $\tB^T$ to be the set $\F_{\tB^T}$ of cones $\Proj_n^{-1}C$ such that $C$ is a cone in the mutation fan $\F_{B^T}$.
Since $\Proj_n\circ\eta_\kk^{\BB^T}=\eta_\kk^{B^T}\circ\Proj_n$ for sequences $\kk$ of indices in $\set{1,\ldots,n}$, the map $\eta_\kk^{\BB^T}$ is linear on every cone of $\F_{\tB^T}$ and acts as an isomorphism of fans from $\F_{\tB^T}$ to $\F_{\mu_\kk(\tB)^T}$.

To understand dominance regions $\P^\tB_\lambda$, it is enough to consider the case where $\lambda$ has nonzero entries only in positions $1,\ldots,n$.
Other dominance regions are obtained by translation, as explained in the following lemma.
The lemma is immediate, because the domains of definition of a mutation map $\eta_\kk^{\BB^T}$ depend only on the first $n$ coordinates and each $\eta_\kk^{\BB^T}$ is linear on each domain of definition.
\begin{lemma}\label{after all coefficients are just coefficients}
If $\lambda$ and $\lambda'$ are vectors in $\reals^m$ that agree in the first $n$ coordinates, then $\P^\tB_{\lambda'}=\P^\tB_\lambda-\lambda+\lambda'$.
\end{lemma}

\cref{shift} immediately implies the following lemma.
\begin{lemma}\label{shift extended}
If $\lambda'=\eta^{\BB^T}_\kk$ and $\tB'=\mu_\kk(\tB)$, then 
\begin{enumerate}[\quad\bf1.]
\item \label{shift extended all}
$\eta^{\BB^T}_\kk\!\!(\P^\tB_\lambda)=\P^{\tB'}_{\lambda'}$.
\item \label{shift extended one}
$\eta^{\BB^T}_\kk\!\!(\P^\tB_{\lambda,\ll})=\P^{\tB'}_{\lambda',\ll\kk^{-1}}$ for any $\ll$.
\end{enumerate}
\end{lemma}

We will prove the following extension of \cref{P in B0C}.

\begin{theorem}\label{P in B0C extended}
Suppose $\kk=k_m\cdots k_1$ is a sequence of indices in $\set{1,\ldots, n}$ and $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
If $\kk^{-1}=k_1\cdots k_m$ is a red sequence for $B_t$, then for any~$\lambda$ in the domain of definition of $\eta_\kk^{\BB_0^T}$ that contains $\Cone^{\tB_0;t_0}_t$,
\[\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda+\GG_t^{\BB_0;t_0}\tB_t\alpha:\alpha\in\reals^n,\alpha\ge0}=\set{\lambda+\tB_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}.\]
\end{theorem}
\begin{proof}
First, we notice that $\kk^{-1}=k_1\cdots k_m$ is a red sequence for $\BB_t$, or in other words, $\kk$ is a green sequence for $-\BB_t$.
Indeed, since $\CC_{t_{\ell-1}}^{-\BB;t_0}=\begin{bsmallmatrix}C_{t_{\ell-1}}^{-B;t_0}&*\\0&I_{m-n}\end{bsmallmatrix}$, the sign of column $k_\ell$ of $\CC_{t_{\ell-1}}^{-\BB;t_0}$ equals the sign of column $k_\ell$ of $C_{t_{\ell-1}}^{-B;t_0}$ whenever $1\le\ell<m$.
Thus \cref{P in B0C} says that
\[\P^{\BB_0}_{\lambda,\kk}\subseteq\set{\lambda+\GG_t^{\BB_0;t_0}\BB_t\alpha:\alpha\in\reals^m,\alpha\ge0}=\set{\lambda+\BB_0\CC_t^{\BB_0;t_0}\alpha:\alpha\in\reals^m,\alpha\ge0}.\]
The assertion of \cref{P in B0C extended} is that the same holds even when, in each term, the conditions $\alpha\in\reals^m,\alpha\ge0$ are strengthened by requiring that $\alpha$ is zero in coordinates $n+1,\ldots,m$.

Thus we run through the proof of \cref{P in B0C} with $\BB$ replacing $B$ and $m$ replacing $n$ throughout and these additional conditions on $\alpha$ in all relevant expressions.
There is no effect on the argument until the point of showing that $V\cap\set{x\in\reals^m:\sgn x_{k_1}=\ep}\subseteq U$.
Here, we need to show that every vector $v_i=\bigl(E^{\BB_1}_{-\ep,k_1}\BB_1\CC_t^{\BB_1;t_1}\bigr)_{\col i}$ with $i\in\set{1,\ldots,n}$ whose $k_1$-entry has sign~$\ep$ is contained in $\nnspan\sett{\bigl(E^{\BB_1}_{\ep,k_1}\BB_1\CC_t^{\BB_1;t_1}\bigr)_{\col i}}_{i=1}^n$.
We argue as in the proof of \cref{P in B0C} that $v_i=N\bigl(\CC_t^{\BB_1;t_1}\bigr)_{\col i}+\sigma N_{\col k}$ and that $\ep e_k$ is a nonnegative linear combination of columns of~$\CC_t^{\BB_1;t_1}$.
Since $\CC_t^{\BB;t_0}=\begin{bsmallmatrix}C_t^{B;t_0}&*\\0&I_{m-n}\end{bsmallmatrix}$, we conclude that $\ep e_k$ is a nonnegative linear combination of columns $1$ through $n$ of~$\CC_t^{\BB_1;t_1}$.
Thus $v_i$ is a nonnegative linear combination of columns $1$ through $n$ of~$N\CC_t^{\BB_1;t_1}$ as desired.
\end{proof}

\subsection{Sufficient conditions for the dominance region to be a point}\label{point sec}
We now use the tools developed so far to give sufficient conditions for the dominance region to be a point.
We use the conditions to show that the dominance region is always a point in finite type (except for exchange matrices with too many zero columns).
Later, we prove similar results for affine type.

Call an extended exchange matrix $\tB$ \newword{salient} if the nonnegative linear span of the columns of $\tB$ contains no line.
(This term is a reference to convex geometry, where a cone is called salient if and only if there is no nonzero vector $x$ such that $x$ and $-x$ are contained in the cone.)
The hypothesis that $\tB$ is salient is strictly weaker than requiring that the columns of $\tB$ are linearly independent.
One can show that $\tB$ is salient by exhibiting a vector $x\in\reals^m$ whose dot product is strictly positive with every nonzero column of $\tB$.
For us, the crucial fact \sayN{the ``salient point''?}
about a salient extended exchange matrix $\tB$ is that the intersection of the nonnegative linear span of the columns of $\tB$ with the nonpositive linear span of the columns of $\tB$ is $\set{0}$.

\begin{theorem}\label{P point}  
Suppose $\tB_0$ is a salient extended exchange matrix.
Suppose $t$ is a seed in the exchange graph for $\tB_0;t_0$ and take $\lambda\in\Cone^{\tB_0;t_0}_t$.
If there exists a maximal red sequence for $B_t$, then $\P^{\tB_0}_\lambda=\set{\lambda}$.
\end{theorem}

\begin{proof}%[Proof of \cref{P point}]
Let $t'$ be the seed at the end of the maximal red sequence for $B_t$.
There exists $\ll=\ell_q\ell_{q-1}\cdots\ell_1$ with $t_0=t'_0\overset{\ell_1}{\edge}t'_1\overset{\ell_2}{\edge}\,\cdots\,\overset{\ell_q}{\edge}t'_q=t'$.
Let $\lambda'=\eta^{\BB_0^T}_\ll\!(\lambda)$.
\cref{shift extended} says $\eta^{\BB_0^T}_\ll\!(\P^{\tB_0}_\lambda)=\P^{\tB_{t'}}_{\lambda'}$.
Thus it is enough to prove that $\P^{\tB_{t'}}_{\lambda'}=\set{\lambda'}$.
Since $\eta_\ll^{\BB_0^T}\bigl(\Cone_t^{\tB_0;t_0}\bigr)=\Cone_t^{\tB_{t'};t'}$, we have reduced the proof to the case where there is a maximal red sequence for $B_t$ starting from $t$ and ending at $t_0$.

Working in that reduction, let $\kk=k_m\cdots k_1$ be the reverse of the maximal red sequence and define seeds $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
Then \cref{P in B0C extended} says that $\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda+\tB_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}$.

Since $\kk^{-1}$ is a maximal red sequence for $B_t$, or in other words a maximal green sequence for $-B_t$, every column of $C_{t_0}^{-B_t;t}$ has negative sign, so $\Cone_{t_0}^{B_t^T;t}=\set{x\in\reals^n:x^TC_{t_0}^{-B_t;t}\ge0}$ consists of vectors with nonpositive entries.
Since $\left(\reals_{\le0}\right)^n$ is a cone in the mutation fan $\F_{-B_t}$ (for example, combining \mbox{\cite[Proposition~7.1]{universal}}, \mbox{\cite[Proposition~8.9]{universal}}, and sign-coherence of $C$-vectors) and also $\Cone_{t_0}^{B_t^T;t}$ is a cone in $\F_{-B_t}$, we see that $\Cone_{t_0}^{B_t^T;t}=\left(\reals_{\le0}\right)^n$.
Thus, up to permuting columns, $C_{t_0}^{-B_t;t}$ is the negative of the identity matrix.
We see that $\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda-\tB_0\alpha:\alpha\in\reals^n,\alpha\ge0}$.

Since also $\P^{\tB_0}_{\lambda,\emptyset}=\set{\lambda+\tB_0\alpha:\alpha\in\reals^n,\alpha\ge0}$, and since $\tB_0$ is salient, we conclude that $\P^{\tB_0}_\lambda=\set{\lambda}$.
\end{proof}

Thinking quickly, one might guess that if an exchange matrix $B$ is salient, then every extension $\tB$ of $B$ is also salient.
This is not true, but the failure of this implication does place some constraints on $B$.

\begin{lemma}\label{extend salient}
If $B$ is a salient exchange matrix and $\tB$ is an extension of $B$ that is not salient, then the kernel of $B$ contains two independent vectors, each with nonnegative entries.
\end{lemma}
\begin{proof}
Suppose $B$ is salient and suppose $x$ and $-x$ are nonzero elements of the nonnegative span of the columns of $\tB$.
Then $\Proj_n(x)$ and $\Proj_n(-x)$ are elements of the nonnegative span of the columns of $B$.
These are therefore both zero, because $B$ is salient.
Thus $x=\tB y$ for some nonzero $y\in\ker B$ with nonnegative entries and $x=\tB z$ for some nonzero $z\in\ker B$ with nonnegative entries.
Since both $y$ and $z$ are nonzero and have nonnegative entries, if they and linearly dependent, then they are related by a positive scaling, contradicting the fact that $x=\tB y\neq0$ and $x=\tB z\neq0$.
Thus $y$ and $z$ are linearly independent.
\end{proof}

An exchange matrix $B=[b_{ij}]$ determines a directed graph with vertices $\set{1,\ldots,n}$ and edges $i\to j$ whenever $b_{ij}>0$.
We say that $B$ is \newword{bipartite} if there is a bipartition $\set{1,\ldots,n}=P\cup N$ such that $i\to j$ implies $i\in P$ and $j\in N$.

\begin{lemma}\label{bip sal}
Suppose $B$ is a bipartite exchange matrix.
Then $B$ is salient.
Furthermore, if $B$ has at most one column that is zero, any extension $\tB$ of $B$ is also salient.
\end{lemma}
\begin{proof}
Any vector $x\in\reals^n$ that is positive in positions $P$ and negative in positions $N$ has strictly positive dot product with every nonzero column of $B$.
Thus $B$ is salient.

Suppose $y$ is an element of the kernel of $B$ with nonnegative entries.
The fact that $B$ is bipartite means that every row of $B$ has either nonnegative entries (if the index of the row is in~$P$) or nonpositive entries (if the index of the row is in~$N$).
Positions where the row is nonzero thus specify positions that must be zero in~$y$.
If $B$ has at most one zero column, then $y$ is zero in every position except in that column (if it exists).
Now \cref{extend salient} implies that any extension $\tB$ is salient.
\end{proof}




\begin{example}\label{0000 bad}
The finite-type exchange matrix $B=\begin{bsmallmatrix}0&0\\0&0\end{bsmallmatrix}$ is salient, but the extended exchange matrix $\tB=\begin{bsmallmatrix*}[r]0&0\\0&0\\1&-1\end{bsmallmatrix*}$ is not, because $\begin{bsmallmatrix*}[r]0&0\\0&0\\1&-1\end{bsmallmatrix*}\begin{bsmallmatrix}1\\0\end{bsmallmatrix}=\begin{bsmallmatrix}0\\0\\1\end{bsmallmatrix}$ and $\begin{bsmallmatrix*}[r]0&0\\0&0\\1&-1\end{bsmallmatrix*}\begin{bsmallmatrix}0\\1\end{bsmallmatrix}=\begin{bsmallmatrix*}[r]0\\0\\-1\end{bsmallmatrix*}$.
FOLLOWING SENTENCE FALSE:
One can check that $\P_\lambda^\tB=\settt{\lambda+a\begin{bsmallmatrix*}[r]0\\0\\1\end{bsmallmatrix*}:a\in\reals}$ for all $\lambda\in\reals^3$.
BECAUSE THAT SENTENCE IS FALSE, that's probably 


\end{example}



\begin{theorem}\label{finite P point}  
Suppose $B$ is an $n\times n$ exchange matrix of finite type.
Then 
\begin{enumerate}[\quad\bf1.]
\item \label{finite P point coeff-free}
$\P^B_\lambda=\set{\lambda}$ for all $\lambda\in\reals^n$.
\item \label{finite P point coeff-ful}
If $B$ has at most one zero column, then $\P^\tB_\lambda=\set{\lambda}$ for any $m\times n$ extension~$\tB$ of $B$ and any $\lambda\in\reals^m$.
\end{enumerate}
\end{theorem}
\begin{proof}
Since $B$ is of finite type, it is mutation-equivalent to an exchange matrix whose graph is an orientation of a Dynkin diagram of finite type and in particular an oriented forest.
Source-sink mutations on this forest yield a bipartite exchange matrix $B_0$, so that $B_0$ is salient by \cref{bip sal}.
Suppose $\kk$ is a sequence of indices such that $\mu_\kk(B_0)=B$.
For any $\lambda\in\reals^n$, \cref{shift} says that $\P_\lambda^B$ is $\eta_\kk^{B_0^T}(\P_{\lambda'}^{B_0})$ for $\lambda'=\eta_\kk^{B_0^T}(\lambda)$.
Every exchange matrix of finite type admits a maximal red sequence, so \cref{P point} applies to say that $\P_{\lambda'}^{B_0}=\set{\lambda'}$.
Therefore $\P_\lambda^B=\set{\lambda}$.

If $B$ has at most one column of zeros, then also $B_0$ has at most one column of zeros.
Let $\tB$ be any extension of $B$ and let $\tB_0=\mu_\kk(\tB)$, which is salient by \cref{bip sal}.
For any $\lambda\in\reals^m$, \cref{shift extended} says that $\P_\lambda^\tB$ is $\eta_\kk^{\BB_0^T}(\P_{\lambda'}^{\tB_0})$ for $\lambda'=\eta_\kk^{\BB_0^T}(\lambda)$.
Again, \cref{P point} applies to say that $\P_{\lambda'}^{\tB_0}=\set{\lambda'}$, so $\P_\lambda^\tB=\set{\lambda}$.
\end{proof}

The hypothesis of \cref{finite P point}.\ref{finite P point coeff-ful} holds, for example, whenever the Cartan matrix associated to $B$ is irreducible and of finite type.

%\section{Affine type}
%Let $B_0$ be acyclic of affine type, indexed so that entries above the diagonal are nonnegative.
%%Or is this backwards?:
%%Then $n(n-1)\cdots1$ is a maximal green sequence for $B_0$ and $12\cdots n$ is a maximal red sequence for $B_0$.
%Take $\lambda$ in the imaginary cone.
%Let $t$ be any seed such that $\Cone_t^{B_0;t_0}$ has $n-2$ rays on the boundary of the imaginary wall $\d_\infty$ such that $\lambda$ is in the imaginary cone spanned by those $n-2$ rays and the imaginary ray.
%Let $\kk=k_m\cdots k_1$ be a sequence such that $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
%%(Probably we need to address greenness or redness of this sequence, which we can presumably do pretty easily with the sortable elements stuff.)
%
%Let $\tB_0$ be an extension of $B_0$ that has linearly independent columns.
%Computations show that  \margin{Probably need more specific notation about $\d_\infty$.}
%
%\[\P_{\lambda,\kk}^{\tB_0}\cap\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0}\cap\d_\infty=\set{\lambda+x\tB_0\delta:x\in\reals}\cap\d_\infty.\]
%
%Let $u$ be the seed reached from $t_0$ by the sequence $n(n-1)\cdots1$ and let $\lambda_u$ be $\eta_{n(n-1)\cdots1}^{\BB_0^T}(\lambda)$.
%\cref{shift extended} says that $\eta_{n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk}^{\tB_0})=\P_{\lambda_u,\kk12\cdots n}^{\tB_u}$ and
%\[\eta_{n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0})=\P_{\lambda_u,\kk n\cdots11\cdots n}^{\tB_u}=\P_{\lambda_u,\kk}^{\tB_u}.\]
%The map $\eta_{n(n-1)\cdots1}^{\BB_0^T}$ is linear on $\d_\infty$ and maps $-\tB_0\delta$ to $-\tB_u\delta$, so it maps the set $\set{\lambda+x\tB_0\delta:x\in\reals}\cap\d_\infty$ to $\set{\lambda_u+x\tB_u\delta:x\in\reals}\cap\d_\infty$.
%Thus we will show that 
%\[\P_{\lambda_u,\kk}^{\tB_u}\cap\P_{\lambda_u,\kk12\cdots n}^{\tB_u}\cap\d_\infty=\set{\lambda_u+x\tB_u\delta:x\in\reals}\cap\d_\infty.\]
%
%Let $t'$ be the seed reached from $t_0$ by the sequence $\kk n(n-1)\cdots1$ or in other words, the seed reached from $u$ by the sequence $\kk$.
%Leaving out repetitions of ``$\alpha\ge0$'' for reasons of space,
%\begin{multline*}
%\P_{\lambda_u,\kk}^{\tB_u}
%\cap
%\P_{\lambda_u,\kk12\cdots n}^{\tB_u}
%\cap
%\d_\infty\\
%\begin{aligned}
%&=
%\left(\eta_{\kk}^{\BB_u^T}\right)^{-1}\!\!\set{\eta_{\kk}^{\BB_u^T}(\lambda_u)+\tB_{t'}\alpha}
%\cap
%\left(\eta_{\kk1\cdots n}^{\BB_u^T}\right)^{-1}\!\!\set{\eta_{\kk1\cdots n}^{\BB_u^T}(\lambda)+\tB_t\alpha}
%\cap
%\d_\infty\\&=
%\eta_{\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk}^{\BB_u^T}(\lambda_u)+\tB_{t'}\alpha}
%\cap
%\eta_{n\cdots1\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk1\cdots n}^{\BB_u^T}(\lambda_u)+\tB_t\alpha}
%\cap
%\d_\infty\\&=
%\eta_{\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk}^{\BB_u^T}(\lambda_u)+\tB_{t'}\alpha}
%\cap
%\eta_{n\cdots1}^{\BB_0^T}\left(\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk1\cdots n}^{\BB_u^T}(\lambda_u)+\tB_t\alpha}\right)
%\cap
%\d_\infty %\\&=
%\end{aligned}
%\end{multline*}
%
%
%
%
%
%Now, writing $\tB_u=\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}$, we have 
%\margin{If we use this, we need to put the $\GG B C$ thing in the background.}
%\begin{align*}
%\tB_t=\mu_\kk(\mu_{1\cdots n}(\tB_u))=\mu_\kk(\tB_0)
%&=\mu_\kk\left((\GG_{t_0}^{B_u;u})^{-1}\tB_uC_{t_0}^{B_u;u}\right)\\
%&=\mu_\kk\left(\begin{bsmallmatrix}G_{t_0}^{B_u;i}&0\\ H_{t_0}^{\tB_u;u}&I_{m-n}\end{bsmallmatrix}^{-1}\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}C_{t_0}^{B_u;u}\right)\\
%&=\mu_\kk\left(\begin{bsmallmatrix}(G_{t_0}^{B_u;u})^{-1}&0\\-H_{t_0}^{\tB_u;u}(G_{t_0}^{B_u;u})^{-1}&I_{m-n}\end{bsmallmatrix}\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}C_{t_0}^{B_u;u}\right)\\
%&=\mu_\kk\left(\begin{bsmallmatrix}B_0\\-H_{t_0}^{\tB_u;u}B_0+E_uC_{t_0}^{B_u;u}\end{bsmallmatrix}\right)\\
%\end{align*}
%We compute that $B_0=B_u$ and that $C_{t_0}^{B_u;u}=-I_n$.
%So $\tB_t=\mu_\kk\left(\begin{bsmallmatrix}B_u\\-H_{t_0}^{\tB_u;u}B_u-E_u\end{bsmallmatrix}\right)$.
%On the other hand, $\tB_{t'}=\mu_\kk(\tB_u)=\mu_\kk\left(\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}\right)$.
%
%
%KEY POINT:  On $\d_\infty$, the map $\eta_{n\cdots1}^{\BB_0^T}$ is linear, and agrees with $c$ or $c^{-1}$ or something.
%So there is just a chance that we know something.
%
%
%
%\subsection{Other ideas}
%
%%Let $t'$ be the seed reached from $t$ by the sequence $\kk n(n-1)\cdots1$ and let $u$ be the seed reached from $t_0$ by the sequence $n(n-1)\cdots1$.
%%Leaving out repetitions of ``$\alpha\ge0$'' for reasons of space,
%%\begin{multline*}
%%\P_{\lambda,\kk}^{\tB_0}
%%\cap
%%\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0}\\
%%\begin{aligned}
%%&=
%%\left(\eta_{\kk}^{\BB_0^T}\right)^{-1}\!\!\set{\eta_{\kk}^{\BB_0^T}(\lambda)+\tB_t\alpha}
%%\cap
%%\left(\eta_{\kk n\cdots1}^{\BB_0^T}\right)^{-1}\!\!\set{\eta_{\kk n\cdots1}^{\BB_0^T}(\lambda)+\tB_{t'}\alpha}
%%\\&=
%%\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk}^{\BB_0^T}(\lambda)+\tB_t\alpha}
%%\cap
%%\eta_{1\cdots n\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1}^{\BB_0^T}(\lambda)+\tB_{t'}\alpha}
%%\\&=
%%\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk}^{\BB_0^T}(\lambda)+\tB_t\alpha}
%%\cap
%%\eta_{1\cdots n\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1}^{\BB_0^T}(\lambda)+\tB_{t'}\alpha}
%%\end{aligned}
%%\end{multline*}
%%
%%Now, writing $\tB_0=\begin{bsmallmatrix}B_0\\E_0\end{bsmallmatrix}$, we have 
%%\margin{If we use this, we need to put the $\GG B C$ thing in the background.}
%%\begin{align*}
%%\tB_{t'}=\mu_\kk(\mu_{n\cdots1}(\tB_0))=\mu_\kk(\tB_u)
%%&=\mu_\kk\left((\GG_u^{B_0;t_0})^{-1}\tB_0C_u^{B_0;t_0}\right)\\
%%&=\mu_\kk\left(\begin{bsmallmatrix}G_u^{B_0;t_0}&0\\ H_u^{\tB;t_0}&I_{m-n}\end{bsmallmatrix}^{-1}\begin{bsmallmatrix}B_0\\E_0\end{bsmallmatrix}C_u^{B_0;t_0}\right)\\
%%&=\mu_\kk\left(\begin{bsmallmatrix}(G_u^{B_0;t_0})^{-1}&0\\-H_u^{\tB;t_0}(G_u^{B;t_0})^{-1}&I_{m-n}\end{bsmallmatrix}\begin{bsmallmatrix}B_0\\E_0\end{bsmallmatrix}C_u^{B_0;t_0}\right)\\
%%&=\mu_\kk\left(\begin{bsmallmatrix}B_u\\-H_u^{\tB;t_0}B_u+E_0C_u^{B_0;t_0}\end{bsmallmatrix}\right)\\
%%\end{align*}
%%We compute that $C $
%
%
%
%
%
%
%%What does \cref{P in B0C extended} say?
%%Assuming the appropriate sequences are red, and that $\lambda$ is in the right domain of definition, 
%%\[\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda+\GG_t^{\BB_0;t_0}\tB_t\alpha:\alpha\ge0}=\set{\lambda+\tB_0C_t^{B_0;t_0}\alpha:\alpha\ge0}\]
%%\[\P^{\tB_0}_{\lambda,\kk n(n-1)\cdots1}\subseteq\set{\lambda+\GG_{t'}^{\BB_0;t_0}\tB_{t'}\alpha:\alpha\ge0}=\set{\lambda+\tB_0C_{t'}^{B_0;t_0}\alpha:\alpha\ge0},\]
%%where $t'$ is the seed obtained from $t_0$ by $\kk n(n-1)\cdots1$.
%
%%Let $\lambda'=\eta_{\kk n(n-1)\cdots1}^{\BB_0^T}$.
%%\cref{shift extended} says that $\eta_{\kk n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk}^{\tB_0})=\P_{\lambda',\kk12\cdots n\kk^{-1}}^{\tB_{t'}}$ and
%%\[\eta_{\kk n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0})=\P_{\lambda',\kk n\cdots11\cdots n \kk^{-1}}^{\tB_{t'}}=\P_{\lambda',\emptyset}^{\tB_{t'}}=\set{\lambda'+\tB_{t'}\alpha:\alpha\ge0}.\]
%
%Let $\lambda_0=\eta_\kk^{\BB_0^T}$.
%\cref{shift extended} says that  
%\[\eta_\kk^{\BB_0^T}(\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0})=\P_{\lambda_0,\kk n(n-1)\cdots1\kk^{-1}}^{\tB_t}=\left(\eta^{\BB_t}_{\kk n\cdots1\kk^{-1}}\right)^{-1}\set{\eta^{\BB_t}_{\kk n\cdots1\kk^{-1}}(\lambda_0)+\tB_{t'}\alpha:\alpha\ge0}.\]
%Also, 
%\[\eta_\kk^{\BB_0^T}(\P_{\lambda,\kk}^{\tB_0})=\P_{\lambda_0,\kk\kk^{-1}}^{\tB_t}=\P_{\lambda_0,\emptyset}^{\tB_t}=\set{\lambda_0+\tB_t\alpha:\alpha\ge0}.\]
%
%We believe we could prove that $\P_{\lambda,\kk}$ only depends on the seed that $\kk$ leads to, not the specific $\kk$.
%So does it make sense to consider the sequence (maximal red or maximal green) that connects $t$ and $t'$.
%
%
%%Let $t'$ be the seed reached from $t$ by the sequence $\kk n(n-1)\cdots1\kk^{-1}$ and let $u$ be the seed reached from $t_0$ by the sequence $n(n-1)\cdots1$.
%%Now, leaving out repetitions of ``$\alpha\ge0$'' for reasons of space,
%%\begin{multline*}
%%\P_{\lambda_0,\kk n(n-1)\cdots1\kk^{-1}}^{\tB_t}\cap\P_{\lambda_0,\kk\kk^{-1}}^{\tB_t}\\
%%\begin{aligned}
%%&=\left(\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}\right)^{-1}\!\!\set{\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_{t'}\alpha}
%%\cap
%%\left(\eta_{\kk\kk^{-1}}^{\BB_t^T}\right)^{-1}\!\!\set{\eta_{\kk\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_t\alpha}\\
%%&=\eta_{\kk1\cdots n\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_{t'}\alpha}
%%\cap
%%\eta_{\kk\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_t\alpha}\\
%%&=\eta_{\kk1\cdots n}^{\BB_{u}^T}\left(\eta_{\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_{t'}\alpha}\right)
%%\cap
%%\eta_{\kk}^{\BB_0^T}\left(\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_t\alpha}\right)\\
%%\end{aligned}
%%\end{multline*}
%


\section{Dominance regions in affine type}
An exchange matrix is of \newword{affine type} if it is mutation-equivalent to an acyclic exchange matrix whose underlying Cartan matrix is of affine type.
We will prove the following theorems. \sayN{Probably should state versions for extended exchange matrices here.  If we get everything we want, we can say that these theorems completely characterize dominance regions in affine type.}

\sayN{Need to say something about rank2 affine.  Are we leaving it out or are we going to deal with it?  The coefficient-free case is in Rupel-Stella.}


\sayN{Need to add a note somewhere that affine implies not block-decomposable.}

\begin{theorem}\label{affine P point} \sayN{Separate this into two statements like \cref{finite P point}?}  
Suppose $\tB$ is an extended exchange matrix whose underlying exchange matrix $B$ is of affine type.
%Suppose $B$ is an exchange matrix of affine type and suppose $\tB$ is any extension of $B$.
If $\lambda\in\Cone^{\tB;t_0}_t$ for some $t$, then $\P^\tB_\lambda=\set{\lambda}$.
\end{theorem}

\begin{theorem}\label{affine main}
Suppose $B$ is an exchange matrix of affine type.
If $\lambda$ is contained in the imaginary wall~$\d^B_\infty$, then the dominance region $\P^B_\lambda$ is the line segment parallel to the imaginary ray, with one endpoint at $\lambda$ and the other endpoint on the relative boundary of $\d^B_\infty$.
\end{theorem}

To prove these theorems, we first gather some assorted tools, and then quote and develop the relevant theory for different classes of affine-type seeds, namely acyclic seeds, arbitrary seeds, and neighboring seeds.

\subsection{Some tools}
In this section, we gather some tools that we will use later to show that certain exchange matrices are not of affine type and to determine certain exchange matrices.

\subsubsection{Mutation-finite exchange matrices}\label{mut fin sec}
An exchange matrix $B$ is \newword{mutation-finite} if only finitely many different matrices can be obtained from $B$ by arbitrary sequences of mutations.
The following \lcnamecref{mut fin 2x2} is \cite[Theorem~2.8]{FeShTu}.  \sayN{Cluster algebras of finite mutation type via unfoldings}

\begin{theorem}\label{mut fin 2x2}
An $n\times n$ exchange matrix $B$ with $n\ge3$ is mutation-finite if and only if, for every sequence $\kk$ of indices in $\set{1,\ldots,n}$, the exchange matrix $B'=\mu_\kk(B)$ satisfies $b'_{ij}b'_{ji}\ge-4$ for all indices $i$ and $j$.
\end{theorem}

This theorem is useful to us because all cluster algebras of affine type are mutation-finite.
In fact, the following stronger statement holds \cite[Theorem~3.5]{Seven} \sayN{Cluster algebras and semipositive symmetrizable matrices}

\begin{theorem}\label{acyc mut fin}
An acyclic $n\times n$ exchange matrix with $n\ge3$ is mutation-finite if and only if its underlying Cartan matrix is of finite or affine type.
\end{theorem}

We also point out the following well known fact,  \sayN{Surely this is well known!  Reference?}
which holds because, for any subset $I$ of $\set{1,\ldots,n}$, and any sequence $\kk$ of indices in $I$, if $B'=[b'_{ij}]=\mu_\kk(B)$, then $\mu_\kk([b_{ij}]_{i,j\in I})=[b'_{ij}]_{i,j\in I}$.
%(The analogous fact for 

\begin{proposition}\label{mut fin sub}
Suppose $B$ is an $n\times n$ exchange matrix.
If $B$ is mutation-finite then, for every subset $I$ of $\set{1,\ldots,n}$, the submatrix $[b_{ij}]_{i,j\in I}$ is mutation-finite.
\end{proposition}
%\begin{proof}
%Suppose $B'=[b_{ij}]_{i,j\in I}$ is not mutation-finite.
%Then \cref{mut fin 2x2} says that there is some sequence $\kk$ of indices in $I$ such that $\mu_\kk(B')$ has a $2\times2$ submatrix failing the condition of \cref{mut fin 2x2}.
%Then the same submatrix occurs in $\mu_\kk(B)$, and thus $B$ is not mutation-finite.
%\end{proof}

\subsubsection{Growth of cluster algebras}\label{growth sec}
The \newword{growth} of a cluster algebra is the asymptotic behavior of the function that counts the number of seeds within a given mutation distance from some initial seed.
Some facts about growth will be useful.

First, beyond rank $2$, the notion of affine type has the following intrinsic characterization.
Combining \cite[Theorem~3.5]{Seven} and \cite[Theorem~1.1]{FeShThTu12}, we see that a cluster algebra of rank at least $3$ is of affine type if and only if it is not of finite type but has linear growth.
%Growth rate of cluster algebras
%Anna Felikson, Michael Shapiro, Hugh Thomas and Pavel Tumarkin

Second, given an $n\times n$ exchange matrix $B=[b_{ij}]$ and a subset $I\subseteq\set{1,\ldots,n}$, if the submatrix $[b_{ij}]_{i,j\in I}$ has exponential growth, it is immediate that that $B$ has exponential growth as well.

Finally, a non-acyclic skew-symmetrizable $3\times3$ exchange matrix with $b_{12}b_{21}=b_{13}b_{31}=b_{23}b_{32}=-4$ has exponential growth.
To see why, one can check that such matrices have the property that every single-step mutation coincides with negation of the matrix, and thereby apply \cite[Theorem~1.1]{FeShThTu12}.

\subsubsection{Exchange relations in the principal coefficients case}\label{exch rel sec}
%We pause to prove a necessary fact about exchange relations, assuming basic background from \cite[Sections~2 \&~3]{ca4}.
%For a moment we work in the most general setting of (normalized) cluster algebras in the sense of \cite[Definition~2.3]{ca4}.
%(See also \cite[Remark~2.7]{ca4}.)
Two cluster variables~$x$ and $x'$ in a cluster pattern of rank $n$  are \newword{exchangeable} if there is no cluster containing both $x$ and $x'$ but there exists a set $\Gamma$ of $n-1$ cluster variables such that $\Gamma\cup\set{x}$ is a cluster and $\Gamma\cup\set{x'}$ is a cluster.
The exchange relation relating $x$ and $x'$ might, in principle, depend on $\Gamma$, but we show that, in the case of principal coefficients, it only depends on $x$ and $x'$.

\begin{lemma}\label{exch ind}  \sayN{More generally, we may as well state this for signed nondegenerating coefficients, if we define that term.}
Suppose $x$ and $x'$ are exchangeable cluster variables in a cluster pattern with principal coefficients.
Then every exchange relation for $x$ and $x'$ is the same:
It involves the same two cluster monomials with the same coefficients.
\end{lemma}
\begin{proof}
Let $\g(x)$ and $\g(x')$ be the $\g$-vectors of $x$ and $x'$.
Sign-coherence of $C$-vectors (explained earlier in \cref{def sec}) implies that every exchange relation for $x$ and $x'$ has a monomial $M$, involving cluster variables but not coefficient variables, whose $\g$-vector is $\g(x)+\g(x')$.
This is a cluster monomial, and it is known that different cluster monomials have different $\g$-vectors (as conjectured in \cite[Conjecture~7.10]{ca4} and proved as \cite[Theorem~2.13]{GHKK}).
We see that $M$ is the same cluster monomial in every exchange relation for $x$ and~$x'$.
Any exchange relation for $x$ and $x'$ writes $xx'$ as $M$ plus another monomial $N$.
But then $N=xx'-M$, so every exchange relation for $x$ and $x'$ also has the same~$N$.
Now $N$ is a monomial in the coefficient variables times a cluster monomial, and again, it is the same cluster monomial in any exchange relation and the same coefficient monomial.
\end{proof}

\begin{remark}\label{4.3}
It appears that, to extend \cref{exch ind} to arbitrary coefficients, one needs \cite[Conjecture~4.3]{ca4}, which says that the exchange graph does not depend on the choice of coefficients.
\sayN{As of 2012 (and maybe 2019), this was known for skew-symmetric geometric type (Linear independence of cluster monomials for skew-symmetric cluster algebras, by Giovanni Cerulli Irelli, Bernhard Keller, Daniel Labardini-Fragoso, Pierre-Guy Plamondon).   Also, for nondegenerate $B$ (M. Gekhtman, M. Shapiro, A. Vainshtein, On the properties of the exchange graph of a cluster algebra, Math. Res. Lett. 15 (2) (2008) 321--330.) and of course finite type (ca2).  Is it known more generally now? (Is it worth trying to prove it for affine type??)}
\end{remark}


\subsection{Acyclic seeds of affine type}\label{acyc sec}
Most of what we know about cluster algebras of affine type rests on the fact that every cluster algebra of affine type has an acyclic seed whose exchange matrix is an orientation of a Cartan matrix of affine type.
(Indeed, the analogous observation is true for cluster algebras of finite type.)
The associated Cartan matrix of affine type allows us to model $\dd$-vectors, $\g$-vectors, and the mutation fan by almost-positive Schur roots, sortable elements and (doubled) Cambrian fans.
We now quote some useful results in this setting and prove some additional facts.
More details can be found in \cite{affdenom,framework,afframe,affscat}.

Let $B_0$ be an acyclic $n\times n$ exchange matrix in a cluster algebra of affine type, indexed so that $b_{ij}\ge0$ whenever $i<j$.
%Let $\tB_0$ be an extension with linearly independent columns.
Let $A_0$ be the Cartan matrix underlying $B_0$, so that $A_0$ is of affine type.
Let $\delta$ be the shortest positive imaginary root in the root system $\RS$ associated to $A$.
Let $\RSpos$ be the set of positive roots in $\RS$ and let $\Simples=\set{\alpha_1,\ldots,\alpha_n}$ be the simple roots of $\RS$.
The \newword{support} of a root is the set of simple roots appearing in an expression for the roots as a linear combination of simple roots.

As before, write $d_1,\ldots,d_n$ for the skew-symmetrizing constants of $B_0$ (meaning that the entries $b_{ij}$ of $B_0$ satisfy $d_i b_{ij}=-d_j b_{ji}$ for all $i,j$).
These constants also symmetrize~$A_0$ (meaning that the entries $a_{ij}$ of $A_0$ satisfy $d_i a_{ij}=d_j a_{ji}$ for all $i,j$).
The \newword{simple co-roots} are $\alpha_i\ck= d_i^{-1} \alpha_i$. 
Thus the diagonal matrix $\Sigma$ with diagonal entries $d_1,\ldots,d_n$, applied to the simple-root coordinates of a vector, gives the simple co-root coordinates of that vector.
%Furthermore, the matrix $\Sigma B_0\Sigma^{-1}$ has entries $d_ib_{ij}d_j^{-1}=-b_{ji}$, so $\Sigma B_0\Sigma^{-1}=-B_0^T$.
%Also, $(E_{\ep,k}^B)^T=F_{\ep,k}^{-B_0^T}=\Sigma F_{\ep,k}^B\Sigma^{-1}$.

Let $V$ be the real vector space spanned by the roots, let $V^*$ be the dual space, and let $\br{\,\cdot\,,\,\cdot\,}:V^*\times V$ be the natural pairing between them.
The simple co-roots~$\Simples\ck$ are also in $V$ (each being a positive scaling of the corresponding simple root).
We take as a basis for $V^*$ the \newword{fundamental weights}, defined as the dual basis to~$\Simples\ck$.
Given $\beta\in V$, the notation $\beta^\perp$ indicates the set of vectors in $V^*$ that pair to $0$ with~$\beta$. 




\begin{remark}\label{danger: co-roots}  \sayN{Added this to ward off confusion, especially in \cref{delta is the man}.}
The purpose of using $\Simples$ as the basis for $V$ but using the dual basis to $\Simples\ck$ as the basis for $V^*$ is to incorporate the symmetrizability of $A$ (and therefore the skew-symmetrizability of $B$) seamlessly into the constructions.
However, some care must be taken when we use these constructions to understand the vectors in~$\reals^n$ that appears elsewhere in the paper.
We identify $V^*$ with the vector space $\reals^n$ by identifying the \emph{fundamental weights} with the standard unit basis vectors of $\reals^n$ and also identify $V$ with the vector space $\reals^n$ by identifying the \emph{simple roots} with the standard unit basis vectors.
With these identifications, multiplying a vector in~$V$ by a matrix takes simple root coordinates to fundamental weight coordinates.
For the same reason, a matrix defines a bilinear form on $V$ in terms of simple co-root coordinates on the left and simple root coordinates on the right.
The expressions~$\beta^\perp$ must be treated with some care when we have passed to vectors in $\reals^n$:
Integer vectors in $\beta^\perp$ do not necessarily have zero dot product with the simple-root coordinates of~$\beta$, but, rather, they have zero dot product with the simple co-root coordinates of $\beta$.
\end{remark}

The Cartan matrix $A_0$ is the matrix of a bilinear form on $V$, in the simple-roots basis on the right and the simple-co-roots basis on the left.
Each real root defines a reflection that negates the root and fixes the hyperplane that is orthogonal to the root (with orthogonality defined in terms of $K$).
Let $W$ be the Weyl group generated by these reflections (an affine Coxeter  group with simple reflections $s_1,\ldots,s_n$ corresponding to the simple roots $\alpha_1,\ldots,\alpha_n$).
The action of $W$ on $V$ fixes~$\delta$.
Let $c$ be the Coxeter element that can be read from $B_0$ by multiplying the simple reflections with $s_i$ preceding $s_j$ whenever $b_{ij}>0$.
Because we have indexed $B_0$ so that its $ij$-entry is nonnegative whenever $i<j$, we have $c=s_1\cdots s_n$.
Define
\begin{align}
\label{rep->}
\TravProj{c}&:=\set{\alpha_1,s_1\alpha_2,\ldots,s_1\cdots s_{n-1}\alpha_n},\\
\label{rep<-}
\TravInj{c}&:=\set{\alpha_n,s_n\alpha_{n-1},\ldots,s_n\cdots s_2\alpha_1},
\end{align}%
Continuing under the assumption that $A_0$ is of affine type, the sets $\TravProj{c}$ and $\TravInj{c}$ are disjoint, and the union $\TravProj{c}\cup\TravInj{c}$ contains one representative of each of the $2n$ infinite $c$-orbits of roots.
(See \cite[Chapter~1]{Dlab-Ringel} or \cite[Theorem~1.2]{afforb}.)
The following lemma is \cite[Lemma~1.6]{Dlab-Ringel} or \cite[Lemma~4.1]{afforb}.

\begin{lemma}\label{c to pos}
Suppose $\RS$ is a root system of affine type and $\beta$ is a positive root in~$\RS$. 
Then $c\beta$ is negative if and only if $\beta\in\TravInj{c}$, in which case $-c\beta\in\TravProj{c}$.
Also $c^{-1}\beta$ is negative if and only if $\beta\in\TravProj{c}$, in which case $-c^{-1}\beta\in\TravInj{c}$.
\end{lemma}

\cref{c to pos} leads immediately to the following result.
\begin{proposition}\label{who is pos}
Suppose $\RS$ is a root system of affine type and $\beta\in\RS$ is in an infinite $c$-orbit.
Then $c^i\beta$ is a positive root for all $i\ge0$ if and only if $\beta=c^p\gamma$ for some $\gamma\in\TravProj{c}$ and $p\ge0$.
\end{proposition}

We define a bilinear form $\omega_c$ on $V$ whose matrix, in simple co-root coordinates on the left and simple root coordinates on the right is~$B$.
This bilinear form is skew-symmetric because $B$ is skew-symmetrizable and the skew-symmetrizing constants are also the scaling factors between simple roots and simple co-roots.
The following lemma is a version of \cite[Lemma~3.8]{typefree}.

\begin{lemma}\label{omega s}
$\omega_{s_1cs_1}(s_1x,s_1y)=\omega_c(x,c)=\omega_{s_ncs_n}(s_nx,s_ny)$ for any $x$ and $y$ in~$V$.
\end{lemma}

Applying \cref{omega s} $n$ times, we obtain the following lemma.

\begin{lemma}\label{omega c}
$\omega_c(x,y)=\omega_c(cx,cy)$ for any $x$ and $y$ in $V$.
\end{lemma}

\begin{proposition}\label{om del}
Suppose $\RS$ is of affine type.
If $\beta=c^p\gamma$ for some $\gamma\in\TravProj{c}$ and $p\ge0$, then $\omega_c(\beta,\delta)\ge0$. 
\end{proposition}
\begin{proof}
The hypothesis is that $\beta=(s_1\cdots s_n)^ps_1\cdots s_{k-1}\alpha_k$ for some $k\in\set{1,\ldots,n}$ and $p\ge0$.
We argue by induction on $np+k$.
If $p=0$ and $k=1$, then $\beta=\alpha_1$ and $\omega_c(\alpha_1,\delta)\ge0$ because every entry of $B$ in row $1$ is nonnegative with at least one positive entry and $\delta$ has all simple root coordinates positive.
Otherwise ${s\beta=(s_2\cdots s_ns_1)^ps_2\cdots s_{k-1}\alpha_k}$ if $k>1$ or $s\beta=(s_2\cdots s_ns_1)^{p-1}s_2\cdots s_n\alpha_1$ if $k=1$.
In either case, $s_1\beta$ is of the form $(s_1cs_1)^{p'}\gamma'$ for $\gamma'\in\TravProj{s_1cs_1}$.
By induction, $\omega_{s_1cs_1}(s_1\beta,\delta)\ge0$, and since $s_1\delta=\delta$, \cref{omega s} completes the proof.
\end{proof}

%The following \lcnamecref{om del fin} is an immediate consequence of \cite[Proposition~2.16]{affdenom}, because the bilinear form $E_c$ in \cite[Proposition~2.16]{affdenom} satisfies $\omega_c(\beta,\beta')=E_c(\beta,\beta')-E_c(\beta',\beta)$.

\begin{proposition}\label{om del fin}
Suppose $\RS$ is of affine type and suppose $x\in V$.
Then $x$ is in a finite $c$-orbit if and only if $\omega_c(x,\delta)=0$.
\end{proposition}
\begin{proof}
Suppose the orbit of $x$ has $k$ elements.
The sum $x+cx+\cdots+c^{k-1}x$ of all vectors in the $c$-orbit of $x$ is fixed by the action of $c$ and thus is a scalar multiple of $\delta$, since $\delta$ spans the $1$-eigenspace of $c$.
Because $\omega_c$ is skew-symmetric, $\omega_c(x+cx+\cdots+c^{k-1}x,\delta)=0$.
Now \cref{omega c} says that $k\omega_c(x,\delta)=0$.
The subspace consisting of vectors $x$ such that $\omega_c(x,\delta)=0$ has dimension $n-1$, and the subspace consisting of vectors in finite $c$-orbits also has dimension $n-1$ (for example, it contains $n-1$ linearly independent roots).
\end{proof}


Every finite $c$-orbit of roots consists entirely of positive roots or entirely of negative roots.
(See \cite[Chapter~1]{Dlab-Ringel} or \cite[Theorem~1.2(5)]{afforb}.)
Let $\RST{c}$ be the set of roots in~$\RS$ that are in finite $c$-orbits.
These form a subsystem of $\RS$, in the sense that the set of real roots in $\RST{c}$ are closed under the reflections they define, and all imaginary roots are in $\RST{c}$.
But $\RST{c}$ might not be a root system in the usual (Kac-Moody) sense.
Rather, it is the product of $1$, $2$, or $3$ affine root systems of type $\afftype{A}$, living in a vector space that may be smaller than the sum of the ranks of the irreducible factor ($0$, $1$, or $2$ dimensions smaller):
Each factor has a shortest positive imaginary root, and all of these are identified with $\delta$.
(Thus $\RST{c}$ is a Kac-Moody root system only in the case where it has one of these components.)
The set of simple roots of $\RST{c}$ is written as $\SimplesT{c}$.
Any real root $\beta\in\RST{c}$ admits a unique expression as a linear combination of elements of $\SimplesT{c}$ and thus a well defined support $\SuppT(\beta)$ as a subset of $\SimplesT{c}$.
The total number of roots in $\SimplesT{c}$ is $n-2$ plus the number of factors of $\RST{c}$.
If $\beta_0,\ldots,\beta_k$ are the simple roots in one of the factors of $\RST{c}$, then $\beta_0+\cdots+\beta_k=\delta$.
These simple roots can be indexed so that $c\beta_{i-1}=\beta_i$ for $i=1,\ldots,k$ and $c\beta_k=\beta_0$.

Let $\APTre{c}$ be the set of positive roots in $\RST{c}$ whose support is strictly smaller than $\Simples$.
Let $\APT{c}=\APTre{c}\cup\set\delta$.
The set $\AP{c}$ of \newword{almost positive Schur roots} is ${-\Simples\cup(\RSpos\setminus\RST{c})\cup\APT{c}}$.
The set $\APre{c}=\AP{c}\setminus\set\delta$ of real roots in $\AP{c}$ is precisely the set of denominator vectors of cluster variables \cite[Theorem~1.2]{affdenom}, with respect to the initial exchange matrix $B_0$.
There is a notion \cite[Definition~4.3]{affdenom} of \newword{$c$-compatibility} of almost positive Schur roots (similar in spirit to the compatibility of almost positive roots in~\cite{ga}) such that a set of almost positive real Schur roots is pairwise $c$-compatible if and only if the corresponding cluster variables are contained in a common cluster.
Thus almost positive real Schur roots and $c$-compatibility model the $\dd$-vector fan (the fan obtained by replacing each cluster with the nonnegative linear span of its $\dd$-vectors).
In addition to these $\dd$-vector cones, there are cones spanned by pairwise $c$-compatible sets that contain~$\delta$.
The set of all these cones is a complete fan $\Fan_c(\RS)$ in $V$ called the \newword{affine generalized associahedron fan}.
%Just as in finite type \cite{ga}, the notion of $c$-compatibility derives from a more detailed notion of $c$-compatibility degree, which assigns an integer to any pair of almost-positive Schur roots.
%(Two roots are $c$-compatible if and only if their $c$-compatibility degree is~$0$.)
%We will not need the definition of $c$-compatibility, but we give some of its crucial properties, particularly as it relates to roots in $\APT{c}$.
The following proposition is \cite[Proposition~5.6]{affdenom}.

\begin{proposition}\label{delta c compat}
A root $\alpha\in\APre{c}$ is $c$-compatible with~$\delta$ if and only if $\alpha\in\APTre{c}$.
\end{proposition}

Two roots $\alpha,\beta\in\APTre{c}$ are called \newword{nested} if $\SuppT(\alpha)\subseteq\SuppT(\beta)$ or if $\SuppT(\beta)\subseteq\SuppT(\alpha)$ or \newword{spaced} if $[\SuppT(c^{-1}\alpha)\cup\SuppT(\alpha)\cup\SuppT(c\alpha)]\cap\SuppT(\beta)=\emptyset$.
The following proposition is \cite[Proposition~5.12]{affdenom}.

\begin{proposition}\label{compatible in tubes}
Two distinct roots $\alpha$ and $\beta$ in $\APTre{c}$ are $c$-compatible if and only if they are nested or spaced.
\end{proposition}

Thus roots $\alpha,\beta\in\APTre{c}$ are $c$-compatible if they are in different factors of $\RST{c}$, while
$c$-compatibility of roots in the same factor can be understood by picturing their supports on a cycle.
(The cycle consists of the simple roots the factor with edges connecting each $\beta$ to $c\beta$.)
The roots appear as ``tubes'' in the cycle (in the sense of graph associahedra), and $c$-compatibility is the usual compatibility of tubes:
Either the tubes are nested or there is at least one vertex between them on both sides.
%However, the representation theory literature uses the term ``tubes'' for the components
However, this terminology conflicts with the representation theory literature use of the term ``tubes'', so we will avoid the term altogether.

%There is a piecewise-linear map $\tau_c$ that induces an automorphism of $\Fan_c(\RS)$.
%The map $\tau_c$ that agrees with $c$ on all but finitely many almost positive Schur roots and in particular agrees with $c$ on all roots in $\APT{c}$.
%For every almost positive Schur root $\beta$ not in $\APT{c}$ (i.e.\ every $\beta\in{-\Simples\cup(\RSpos\setminus\RST{c})}$), $\tau_c^k\beta$ approaches the direction of $\delta$ as $k\to\infty$.
%More precisely, as $k\to\infty$, $\tau_c^k\beta$ is eventually of the form $\beta'+m\delta$, where $\beta'\in\RSfin$ continues to vary with $k$ and $m\to\infty$ as $k\to\infty$.

A maximal set of pairwise $c$-compatible roots in $\AP{c}$ is called a \newword{$c$-cluster}.
A \newword{real $c$-cluster} is a $c$-cluster that does not contain $\delta$.
Real $c$-clusters have $n$ roots and span maximal cones of the $\dd$-vector fan.
An \newword{imaginary $c$-cluster} is a $c$-cluster that contains $\delta$.
Imaginary $c$-clusters have $n-1$ roots and span cones outside the $\dd$-vector fan.

There is a piecewise linear homeomorphism $\nu_c$ from $V$ to $V^*$ that is linear on every cone of $\Fan_c(\RS)$ and thus defines a fan $\nu_c(\Fan_c(\RS))$ in $V^*$.
The fan $\nu_c(\Fan_c(\RS))$ is the mutation fan for $B_0^T$, and the map $\nu_c$ restricts to an isomorphism from the $\dd$-vector fan of $B_0$ to the $\g$-vector fan \cite[Theorem~2.9]{affscat}.
The cones of $\Fan_c(\RS)$ spanned by $\APT{c}$ map to the \newword{imaginary cones} of $\nu_c(\Fan_c(\RS))$.
These cover the closure of the complement of the $\g$-vector fan, a codimension-$1$ cone that we call that \newword{imaginary wall}~$\d_\infty$.
The imaginary wall is contained in $\delta^\perp$.
The vector $\nu_c(\delta)$ spans a ray called the \newword{imaginary ray} that is in every imaginary cone.
This is the only ray of $\nu_c(\Fan_c(\RS))$ that is not spanned by the $\g$-vector of a cluster variable.
The following \lcnamecref{nu delta} is \cite[Lemma~5.9]{affscat}, written in different notation.
(The statement in \cite{affscat} is phrased in terms of $\omega_c(\,\cdot\,,\delta)$.)

\begin{lemma}\label{nu delta}  
Suppose $B_0$ is an acyclic exchange matrix of affine type.
The vector~$\nu_c(\delta)$ spanning the imaginary ray is $-\frac12B_0\delta$.
\end{lemma}

Since $\mu_{12\cdots n}(B)=B$, the map $\eta^{B_0^T}_{12\cdots n}$ is an automorphism of the fan and separately of the $\g$-vector fan.
The following propositions are part of \cite[Proposition~7.31(4)]{affscat} and \cite[Proposition~7.32]{affscat}

\begin{proposition}\label{eta on dinf}
Suppose $B_0$ is an acyclic $n\times n$ exchange matrix of affine type.
The map $\eta^{B_0^T}_{12\cdots n}$ fixes $\d_\infty$ as a set, agrees with $c$ on $\d_\infty$, and has finite order on $\d_\infty$.
\end{proposition}

\sayN{I inserted the following (and something else, I can't remember) into \{affscat\} after we submitted it to CR.  It costs us just over a page there but would be a huge pain to do it here.  Hopefully the CR people don't get bent out of shape about it, or maybe they reject it before we even tell them.  :)}


\begin{proposition}\label{delta limit eta}
Suppose $x\in V^*\setminus\d_\infty$.
Then for large enough $i$, the action of $\eta_{12\cdots n}^{B^T}$ on $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i(x)$ is the same as the action of~$c$.
Also, $\br{\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i(x),\delta}>0$ for large enough $i$.
There exist integers $m$ and $I$ and a positive real number $a$ such that $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^{i+m}(x)=\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i(x)+a\nu_c(\delta)$ when $i\ge I$.
%Thus 
%\[\lim_{i\to\infty}\frac{\bigl(\eta_{12\cdots n}^{B_0^T}\bigr)^i(x)}{\bigl|\bigl(\eta_{12\cdots n}^{B_0^T}\bigr)^i(x)\bigr|}=\frac{\nu_c(\delta)}{|\nu_c(\delta)|}\]
\end{proposition}

We now prove our first preliminary result about dominance regions in the affine case.

\begin{lemma}\label{P in dinf}
Suppose $B_0$ is acyclic of affine type.
If $\lambda\in\d_\infty$, then $\P^{B_0}_\lambda\subseteq\d_\infty$.
\end{lemma}
\begin{proof}
By definition, $\P^{B_0}_\lambda\subseteq\P^{B_0}_{\lambda,\emptyset}=\set{\lambda+B_0\alpha:\alpha\ge0}$.
Write the entries of $\lambda$ as $(\lambda_1,\ldots,\lambda_n)$.
Since $B_0$ is acyclic and its $1\st$ row has nonnegative entries, in particular every point in $\P^{B_0}_\lambda$ has $1\st$ coordinate greater than or equal to $\lambda_1$.
Also, since $\delta$ has all entries positive and the first row of $B_0$ is nonzero, $B_0\delta$ has $1\st$ coordinate strictly positive.

In \cref{delta limit eta}, we can take $m$ such that $\bigl(\eta^{B_0^T}_{12\cdots n}\bigr)^m$ fixes $\d_\infty$ pointwise (replacing $m$ with the least common multiple of $m$ and the order of $c$ on $\d_\infty$).
Since $\mu_{12\cdots n}(B_0)=B_0$, \cref{shift}.\ref{shift all} says that $\bigl(\eta^{B_0^T}_{12\cdots n}\bigr)^m(\P_\lambda^{B_0})=P_\lambda^{B_0}$.

Now suppose $\P^{B_0}_\lambda$ contains a point $x\not\in\d_\infty$.
Applying \cref{delta limit eta}, we find a sequence of points in $\P^{B_0}_\lambda$ that approach the direction of $\nu_c(\delta)$ with the component in the direction of $\nu_c(\delta)$ increasing without bound.
By \cref{nu delta}, and the fact that $B_0\delta$ has $1\st$ coordinate strictly positive, we conclude that $\P^{B_0}_\lambda$ contains points with $1\st$ coordinate strictly \emph{negative}.
This contradiction shows that $\P^{B_0}_\lambda\subseteq\d_\infty$.
\end{proof}

We conclude this section with a brief discussion of $c$-sortable elements.  
Recalling that $c=s_1\cdots s_n$, write $c^\infty$ for the infinite word $s_1\cdots s_n|s_1\cdots s_n|s_1\cdots s_n|\cdots$.
(The symbols ``$|$'' are not considered as letters of the word, but are \newword{dividers} placed in the word for convenience.)
Every element $w$ of the group $W$ can be written (in many ways) as a subword of $c^\infty$.
Each subword is specified by a finite sequence of positions in $c^\infty$.
Out of all those subwords, the \newword{$c$-sorting word} for $w$ is the subword with the \emph{lexicographically leftmost} sequence of positions.
The $c$-sorting word for $w$ is also specified by a finite sequence of nonempty subsets of $\set{s_1,\ldots,s_n}$, namely the set of letters of the $c$-sorting word that are before the first divider, the set of letters between the first and second dividers, between the second and third dividers, etc.
The element $w$ is \newword{$c$-sortable} if this sequence of subsets is weakly decreasing.
(In other words, working from left to right, once a letter $s_i$ of $c^\infty$ is skipped in the $c$-sorting word, it never appears afterwards.)

The following lemma is immediate from the definition.
\begin{lemma}\label{any prefix}
If $a_1\ldots a_m$ is the $c$-sorting word for a $c$-sortable element, then for any $k\in\set{0,1,\ldots,m}$, the prefix $a_1\cdots a_k$ is the $c$-sorting word for a $c$-sortable element.
\end{lemma}


The $c$-sortable elements and their $c$-sorting words contain a lot of combinatorial data about the mutation fan.
%Each $c$-sortable element determines an $n$-dimensional cone $\Cone_c(v)$ in $V^*$ whose inward-facing normals are certain roots or co-roots in $\RS\subset V$ determined by the $c$-sorting word as follows.
Given a $c$-sortable element $v$ and an index $i\in\set{1,\ldots,n}$, let $a_1\cdots a_k$ be the prefix of the $c$-sorting word for $v$ consisting of all the letters before the first place where $s_i$ is skipped in the $c$-sorting word.
Define $C_c^i(v)$ to be the root $a_1\cdots a_k\alpha_i$ and define $C_c^i(v)\ck$ to be the corresponding co-root $a_1\cdots a_k\alpha\ck_i$.
The skip of $s_i$ in the $c$-sorting word is \newword{unforced} if $a_1\cdots a_ks_i$ is a reduced word.
Otherwise, it is \newword{forced}.
The root $C_c^i(v)$ is positive if and only if the skip is unforced.

For each $c$-sortable element $v$, define a cone  
\[\Cone_c(v)=\bigcap_{\beta \in C_c(v)}\set{x\in V^*:\br{x,\beta} \geq 0}.\]
%(Since each $C_c^i(v)\ck$ is a scaling of $C_c^i(v)$, we could equally well write $\bigcap_{\beta\ck\in C_c(v)\ck}\set{x\in V^*:\br{x,\beta\ck} \geq 0}$).
In general acyclic (but not necessarily affine) type, the union of these cones $\Cone_c(v)$ and their faces, over all $c$-sortable elements $v$, is a fan called the \newword{$c$-Cambrian fan}, which is a subfan of the $\g$-vector fan in $V^*$.
In particular, each $c$-sortable element~$v$ specifies a unique seed.
The $C$-vectors of this seed are encoded in the $c$-sorting word for~$v$.
Specifically, the $C$-vector in position $i$ is the integer vector given by the simple-root coordinates of $C_c^i(v)$.
(See \cite[Therem~1.1]{framework} and \cite[Theorem~5.12]{framework}.)
The simple co-root coordinates of $C_c^i(v)\ck$ give the $C$-vector of the seed specified by~$v$ in the cluster algebra with initial exchange matrix $-B^T$.
Thus by \cite[Theorem~1.1]{framework}, the exchange matrix at the seed associated to $v$ has $ij$-entry $\omega_c(C_c^i(v)\ck,C_c^j(v))$.

When $B$ is of finite type (equivalently, when $W$ is finite), the $c$-Cambrian fan coincides with the $\g$-vector fan, but otherwise (when $W$ is infinite), the $c$-Cambrian fan is not complete and is a proper subfan of the $\g$-vector fan.
When $B$ is of affine type (equivalently, when $W$ is of affine type), the $\g$-vector fan is the \newword{doubled $c$-Cambrian fan}: the union of the \mbox{$c$-Cambrian} fan and the image of the \mbox{$c^{-1}$-Cambrian} fan under the antipodal fan. % (more briefly, the Cambrian fan and the \newword{opposite Cambrian fan}).
In particular, the $c$-Cambrian fan covers the open halfspace $\set{x\in V^*:\br{x,\delta}>0}$.

\begin{proposition}\label{tack on c}
Suppose $W$ is of affine type and suppose $v$ is a $c$-sortable element whose $c$-sorting word starts with $s_1\cdots s_n$.
Then $cv$ is $c$-sortable and its $c$-sorting word is $s_1\cdots s_n$ followed by the $c$-sorting word for~$v$.
\end{proposition}
\begin{proof}
Let $a_1\cdots a_m$ be the $c$-sorting word for $v$.
We first show that $s_na_1\cdots a_m$ is reduced.
The means showing that $\alpha_n\not\in\set{a_1\cdots a_{k-1}\alpha(a_k):k=1,\ldots,m}$.
Suppose to the contrary that $\alpha_n=a_1\cdots a_{k-1}\alpha(a_k)$ for some~$k$, where $\alpha(a_k)$ is the simple root associated to the simple reflection~$a_k$.
Since $\set{a_1\cdots a_{k-1}\alpha(a_k):k=1,\ldots,n}$ equals $\TravProj{c}$ and $\alpha_n\in \TravInj{c}$, we must have $k>n$.
But then \cref{c to pos} says that $a_{n+1}\cdots a_{k-1}\alpha(a_k)$ is negative, contradicting the fact that $a_{n+1}\cdots a_m$ is reduced.

We conclude that $s_na_1\cdots a_m$ is reduced.
Therefore, we see that $s_na_1\cdots a_m$ is the $(s_ncs_n)$-sorting word for $s_nv$ and that $s_nv$ is $(s_ncs_n)$-sortable.
Applying that fact $n$ times implies the proposition.
\end{proof}

\begin{proposition}\label{sort green}
Suppose $B_0$ is an acyclic $n\times n$ exchange matrix of affine type.
Let $v$ be a $c$-sortable element and let $\kk=k_m\cdots k_1$ be the \emph{reverse} of the sequence of indices in the $c$-sorting word for $v$.
Then $\kk$ is a green sequence for $B_0$.
\end{proposition}
\begin{proof}
Write $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
\cref{any prefix} implies that each $t_i$ is associated to a $c$-sortable element.
Each $t_{i+1}$ is obtained from $t_i$ by mutating in position $k_{i+1}$, the last letter of the $c$-sorting word associated to $t_{i+1}$.
Since the $c$-sorting word associated to $t_{i+1}$ is reduced, the skip of $s_{k_{i+1}}$ is unforced, and thus the $C$-vector at $t_i$ indexed by $k_{i+1}$ is positive.
\end{proof}




\begin{proposition}\label{any B is sort}
Suppose $B_0$ is an acyclic exchange matrix of affine type and suppose $B$ is mutation-equivalent to $B_0$.
Then there exists a $c$-sortable element~$v$ whose associated seed $t$ has exchange matrix $B$.
For $\kk$ the \emph{reverse} of the sequence of indices in the $c$-sorting word for~$v$, write ${t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t}$.
Then~$v$ and~$t$ can be chosen so that $\bigl(G_{t_j}^{-B_0^T;t_0}\bigr)^T\delta$ has nonnegative entries for all ${j=0,\ldots,m}$.
\end{proposition}
\begin{proof}
Since $B$ is mutation-equivalent to $B_0$, taking $B_0$ to be the exchange matrix at $t_0$, there exists a seed $t'$ with $B$ as exchange matrix.
Taking $x$ in the interior of $\Cone^{B_0;t_0}_{t'}$, \cref{delta limit eta} says that $\br{\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i(x),\delta}>0$  for large enough~$i$.
Thus $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i(x)$ is in the $c$-Cambrian fan, and thus $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i\bigl(\Cone^{B_0;t_0}_{t'}\bigr)$ is a cone $\Cone^{B_0;t_0}_t$ for some seed $t$ associated to a $c$-sortable element $v$.
The automorphism $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i$ of $\nu_c(\Fan_c(\RS))$ maps $\Cone^{B_0;t_0}_t$ to $\Cone^{B_0;t_0}_{t'}$.
The map $\bigl(\eta_{12\cdots n}^{B^T}\bigr)^i$ acts as an initial seed mutation $\mu_{(12\cdots n)^i}$, which fixes $B_0$.
If $\ll$ is the sequence of indices that mutates $t_0$ to $t'$, then $\ll(12\cdots n)^i$ mutates $t_0$ to $t$.
We see that the exchange matrix at $t$ is $\mu_{\ll(12\cdots n)^i}(B_0)=\mu_\ll(B_0)=B$.

Increasing $i$ in the previous paragraph means using \cref{tack on c} to replace~$v$ by a $c$-sortable element~$c^pv$.
It is known \cite[Proposition~4.6]{afframe} that there are only finitely many $c$-sortable elements $v$ such that the nonnegative span of $G_{t_j}^{-B_0^T;t_0}$ intersects the halfspace $\set{x\in V^*:\br{x,\delta<0}}$. 
Thus we choose $i$ large enough (i.e. choose $p$ large enough) so that $\bigl(G_{t_j}^{-B_0^T;t_0}\bigr)^T\delta$ has nonnegative entries for all $j\ge pn$.
But for $0\le j\le pn$, the the prefix $a_1\cdots a_j$ of the $c$-sorting word for $v$ is a prefix of $c^\infty$.
By the characterization of $C$-vectors in terms of skips, we see that the columns of $C_{t_j}^{B_0;t_0}$ are the inward-facing normals of the cone obtained by applying $a_1\cdots a_j$ to the positive cone.
The resulting cone $\Cone^{B_0;t_0}_{t_j}$ is therefore contained in the Tits cone, so that every nonzero vector in $\Cone^{B_0;t_0}_{t_j}$ pairs strictly positively with $\delta$.
The cone $\Cone^{B_0;t_0}_{t_j}$ is the nonnegative span of the rows of $\bigl(C_t^{B_0;t_0}\bigr)^{-1}$, which equals $\bigl(G_t^{-B_0^T;t_0}\bigr)^T$ by \cite[Theorem~1.2]{NZ}.
We see that the columns of $G_{t_j}^{-B_0^T;t_0}$ pair strictly positively with $\delta$.
\end{proof}




\subsection{General seeds of affine type}
We now prove some facts about exchange matrices of affine type, without the requirement of acyclicity.
\sayN{Also write what parts of the theorems we pro

ve here... Not doing it yet because I hope there is still a coefficientful version of the line segment result.}

Let $B$ be an exchange matrix of affine type.
Then $B$ is mutation-equivalent to some acyclic exchange matrix $B_0$ of affine type.
Mutation maps $\eta_\kk^{B^T}$ are homeomorphisms $V^*$ and isomorphisms of mutation fans and also constitute initial seed mutations of $\g$-vectors.
Thus the following structure carries over from the acyclic seed:
There is an \newword{imaginary ray}, the unique ray of the mutation fan that is not spanned by the $\g$-vector of a cluster variable.
An \newword{imaginary cone} of $\F_{B^T}$ is a cone containing the imaginary ray.
The maximal imaginary cones have codimension~$1$.
The mutation fan $\F_{B^T}$ consists of the $\g$-vector fan of $B$ and the imaginary cones.
Write $\d^B_\infty$ for the union of the imaginary cones.
This is a finite union of cones of codimension $1$, and we will show that it is contained in a hyperplane.
Thus we will again call $\d^B_\infty$ the \newword{imaginary wall}.
Since $\d^B_\infty$ is the image of $\d_\infty$ under the fan isomorphism $\eta_\kk^{B^T}$, the following lemma is immediate from \cref{shift,P in dinf}.

\begin{lemma}\label{P in dBinf}
Suppose $B$ is of affine type.
If $\lambda\in\d^B_\infty$, then $\P^B_\lambda\subseteq\d^B_\infty$.
\end{lemma}

Let $B_0$ be an acyclic exchange matrix that is mutation-equivalent to $B$ and consider the cluster pattern with $B_0$ the seed at $t_0$.
We will construct a particular seed $t$ with exchange matrix $B$ and a green sequence $\kk=k_m\cdots k_1$ for $B_0$ that mutates $t_0$ to $t$ and then establish some additional properties of $\kk$.

\cref{any B is sort} says there exists a $c$-sortable element $v$ whose associated seed has exchange matrix $B$.
Let $\kk$ be the reverse of the sequence of indices in the $c$-sorting word for $v$ and write $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
\cref{any B is sort} says further that $v$ can be chosen so that $\bigl(G_{t_j}^{-B_0^T;t_0}\bigr)^T\delta$ has nonnegative entries for all $j=0,\ldots,m$.
\cref{sort green} says that $\kk$ is a green sequence for $B_0$.
For such a choice of $v$ and $t$, define $\delta^B$ to be $\bigl(G_t^{-B_0^T;t_0}\bigr)^T\delta$.  The following proposition in particular implies that $\delta^B$ is well defined, in the sense that it depends only on $B$ and not on the specific choices we made.


\begin{proposition}\label{delta is the man}
Suppose $B$ is an exchange matrix of affine type.
\begin{enumerate}[\quad\bf1.]
\item \label{im ray pos}
$\delta^B$ has nonnegative entries.
\item \label{im ray}
$-\frac12B\delta^B$ is the shortest integer vector in the imaginary ray of $\F_{B^T}$.
\item \label{im hyp}
All imaginary cones in $\F_{B^T}$ are contained in $(\delta^B)^\perp$.
\end{enumerate}
\end{proposition}

We refer to \cref{danger: co-roots} for clarification on how to interpret \cref{delta is the man}.\ref{im hyp} in specific examples.
See also the last part of the following proof.

\begin{proof}%[Proof of \cref{delta is the man}]
Assertion~\ref{im ray pos} is true because we chose $v$ and $t$ so that $\bigl(G_t^{-B_0^T;t_0}\bigr)^T\delta$ has nonnegative entries.

Suppose $a_1\cdots a_m$ is the $c$-sorting word for $v$.
\cref{any prefix} says that for any ${\ell\in\set{0,1,\ldots,m}}$, the prefix $a_1\cdots a_\ell$ is the $c$-sorting word for another $c$-sortable element $v_\ell$.
The corresponding postfix of $\kk$ and the seed $t_\ell$ have the same properties we used to define $\delta^B$, so writing $B_\ell$ for the exchange matrix at $t_\ell$, we can use $t_\ell$ to define $\delta^{B_\ell}$.
Thus we argue by induction on $m$ for the remaining assertions.

Assertion~\ref{im ray} is true by \cref{nu delta} when $m=0$ (so that $t=t_0$ and $B=B_0$) because then $\delta^B=\delta$.
By induction, $-B_{m-1}\delta^{B_{m-1}}$ is the shortest integer vector in the imaginary ray of $\F_{B_{m-1}^T}$.
The mutation map $\eta_{k_m}^{B_{m-1}^T}$ takes the shortest integer vector in the imaginary ray of $\F_{B_{m-1}^T}$ to the shortest integer vector in the imaginary ray of $\F_{B^T}$. 
Thus we check that $\eta_{k_m}^{B_{m-1}^T}(-B_{m-1}\delta^{B_{m-1}})=-B\delta^{B}$.

We first appeal to \cite[Proposition~1.3]{NZ} to say that 
\[\delta^B=\bigl(G_t^{-B_0^T;t_0}\bigr)^T\delta=\bigl(G_{t_{m-1}}^{-B_0^T;t_0}E_{+,k_m}^{B_{m-1}^T}\bigr)^T\delta=F_{-,k_m}^{B_{m-1}}\bigl(G_{t_{m-1}}^{-B_0^T;t_0}\bigr)^T\delta,\]
because $\kk$ is a green sequence, so that column $k_m$ of $C_{t_{m-1}}^{-B_0^T;t_0}$ is positive.
Now \cref{EBF trick} says that $B=E_{-,k_m}^{B_{m-1}}B_{m-1}F_{-,k_m}^{B_{m-1}}$.
Thus since $F_{-,k_m}^{B_{m-1}}$ is its own inverse, 
\[-B\delta^B=-E_{-,k_m}^{B_{m-1}}B_{m-1}\bigl(G_{t_{m-1}}^{-B_0^T;t_0}\bigr)^T\delta=E_{-,k_m}^{B_{m-1}}(-B_{m-1}\delta^{B_{m-1}}).\]
Showing that $\eta_{k_m}^{B_{m-1}^T}(-B_{m-1}\delta^{B_{m-1}})=E_{-,k_m}^{B_{m-1}}(-B_{m-1}\delta^{B_{m-1}})$ amounts to showing that the $k_m$-entry of $-B_{m-1}\delta^{B_{m-1}}$ is nonpositive.

We use \cref{BGCB} to rewrite $-B_{m-1}\delta^{B_{m-1}}=-B_{m-1}\bigl(G_{t_{m-1}}^{-B_0^T;t_0}\bigr)^T\delta$ as $\bigl(C_{t_{m-1}}^{-B_0^T;t_0}\bigr)^T(-B_0\delta)$.
Thus we must show that $\omega_c(C^{k_m}_c(v_{m-1})\ck,\delta)\ge0$. 
Since $C^{k_m}_c(v_{m-1})\ck$ is a positive scalar multiple of $C^{k_m}_c(v_{m-1})$, we need to check that $\omega_c(C^{k_m}_c(v_{m-1}),\delta)\ge0$. 

In the definition of $\delta^B$, we replaced a $c$-sortable element $v$ by the $c$-sortable element~$c^iv$ for large enough $i$.
Since we can increase $i$ arbitrarily without disturbing the properties we have established for $\kk$, we know that $c^jC^{k_m}_c(v_{m-1})=C^{k_m}_c(c^jv_{m-1})$ is a positive root for any $j\ge0$.
Thus \cref{who is pos} says that either $C^{k_m}_c(v_{m-1})$ is $c^p\gamma$ for some $\gamma\in\TravProj{c}$ and $p\ge0$ or $C^{k_m}_c(v_{m-1})$ is in a finite $c$-orbit.
Thus \cref{om del} or \cref{om del fin} says that $\omega_c(C^{k_m}_c(v_{m-1}),\delta)\ge0$. 
We have proved Assertion~\ref{im ray}.

Assertion~\ref{im hyp} is true when $m=0$.
If $m>0$, then by induction, all imaginary cones in $\F_{B_{m-1}^T}$ are contained in $(\delta^{B_{m-1}})^\perp$.
The imaginary cones in $\F_{B^T}$ are the images, under $\eta^{B^T_{m-1}}_{k_m}$, of the imaginary cones in $\F_{B_{m-1}^T}$.
Every imaginary cone in $\F_{B_{m-1}^T}$ contains the imaginary ray, spanned by $-B_{m-1}\delta^B_{m-1}$.
We have previously shown that the $k_m$ entry of $-B_{m-1}\delta^B_{m-1}$ is nonpositive.
%More specifically, we showed that it is strictly negative unless $C^{k_m}_c(v_{m-1})$ is in a finite $c$-orbit.
Let $\lambda$ be any vector in an imaginary cone of $\F_{B_{m-1}^T}$.

If the $k_m$ entry of $-B_{m-1}\delta^B_{m-1}$ is strictly negative, then the $k_m$ entry of $\lambda$ is nonpositive, and thus $\eta^{B^T_{m-1}}_{k_m}$ acts on $\lambda$ (just as is acts on $-B_{m-1}\delta^B_{m-1}$) by the map given in the basis of fundamental weights by the matrix $E_{-,k_m}^{B_{m-1}}$.
The map taking $\delta^{B_{m-1}}$ to $\delta^B$ is given, in the basis of simple roots, by the matrix $F_{-,k_m}^{B_{m-1}}$.
The fundamental weights are dual to the simple \emph{co-roots} and the diagonal matrix $\Sigma$ of symmetrizing constants, applied to the simple-root coordinates of a vector, gives the simple co-root coordinates of that vector.
Recalling the identity $(E_{\ep,k}^B)^T\Sigma=\Sigma F_{\ep,k}^B$ from \cref{acyc sec}, we compute
\begin{align*}
\br{\eta^{B^T_{m-1}}_{k_m}(\lambda),\delta^B}
&=\br{E_{-,k_m}^{B_{m-1}}\lambda,F_{-,k_m}^{B_{m-1}}\delta^{B_{m-1}}}\\
&=(E_{-,k_m}^{B_{m-1}}\lambda)^T\Sigma F_{-,k_m}^{B_{m-1}}\delta^{B_{m-1}}\\
&=\lambda^T(E_{-,k_m}^{B_{m-1}})^T\Sigma F_{-,k_m}^{B_{m-1}}\delta^{B_{m-1}}\\
&=\lambda^T\Sigma F_{-,k_m}^{B_{m-1}}F_{-,k_m}^{B_{m-1}}\delta^{B_{m-1}}=\lambda^T\Sigma\delta^{B_{m-1}}=\br{\lambda,\delta^{B_{m-1}}}=0.
\end{align*}

If instead the $k_m$ entry of $-B_{m-1}\delta^B_{m-1}$ is zero then either linear map associated to $\eta^{B^T_{m-1}}_{k_m}$ takes $-B_{m-1}\delta^B_{m-1}$ to $-B\delta^B$, namely $E_{\ep,k_m}^{B_{m-1}}$ for $\ep\in\set{+,-}$.
Taking $\ep$ to be the sign of the $k_m$ entry of $\lambda$, we compute as above with the sign $\ep$ replacing the sign~$-$, to obtain $\br{\eta^{B^T_{m-1}}_{k_m}(\lambda),\delta^B}=0$.
\end{proof}


%\begin{proposition}\label{dBinf conv}
%If $B$ is of affine type, then $\d^B_\infty$ is a convex cone.
%\end{proposition}
%\begin{proof}
%CAN WE DO THIS?  Not sure.
%
%DO WE NEED TO DO THIS?  Not sure.
%
%But it probably will be useful somewhere.
%
%If we don't do it, update the prose above.
%\end{proof}4

\cref{delta is the man}.\ref{im hyp} establishes that $\d^B_\infty$ is contained in a hyperplane.
We will use that fact to prove a \lcnamecref{aff red} and then use that \lcnamecref{aff red} to prove \cref{affine P point}.

\begin{proposition}\label{aff red}
Suppose $B$ is an exchange matrix of affine type, then $B$ admits a maximal green sequence and a maximal red sequence.
\end{proposition}
\begin{proof}
The matrix $B$ is of affine type if and only if $-B$ is of affine type, so it is enough to prove that every $B$ of affine type admits a maximal green sequence.
The imaginary wall $\d^B_\infty$ is a union of convex cones, is contained in the hyperplane $(\delta^B)^\perp$, but is strictly smaller than that hyperplane.
(If the imaginary wall were the whole hyperplane, the $\g$-vector fan would be contained in a halfspace, but in the case where $B$ is acyclic, the $\g$-vector fan is dense in the whole space, and that property is preserved by mutation maps.) 
For that reason, we can construct a maximal green sequence as follows:
For any point $x$ in the interior of the positive cone, we can move the line segment from $x$ to~$-x$ an arbitrarily small amount to obtain a line segment from the positive cone to the negative cone that is contained in the interior of the $\g$-vector fan and doesn't pass through any codimension-$2$ faces of the $\g$-vector fan.
The $\g$-vector cones visited by this define a maximal green sequence.
\end{proof}


\begin{proof}[Proof of \cref{affine P point}]
We first construct an exchange matrix $B_0$ that is mutation-equivalent to $B$ such that any extension of $B_0$ is salient.

Since $B$ is of affine type, it is mutation-equivalent to an exchange matrix whose graph is an orientation of a Dynkin diagram of affine type.
When this graph is an oriented tree, source-sink moves make a bipartite exchange matrix $B_0$ mutation-equivalent to $B$.
Since Cartan matrices of affine type have connected Dynkin diagrams, $B$ has no zero columns, and therefore $B_0$ has no zero columns.
\cref{bip sal} says that any extension of $B_0$ is salient.

Otherwise, the graph is a cycle, and source-sink moves lead to an exchange matrix $B'_0$ whose oriented graph has exactly one source and exactly one sink.
For convenience, we can reindex $B'_0$ so that its source is $1$, the sink is $n$, and the directed graph for $B'_0$ has ${1\to2\to\cdots\to k\to n}$ and $1\to k+1\to k+2\to\cdots\to n$ for some $k$ with $1\le k<n$.
The matrix $B'_0$ is salient except when $n$ and $k$ are both even, but we perform one mutation on $B'_0$ to obtain a matrix that is salient for all $n$ and~$k$.

Let $B_0$ be obtained from $B_0'$ by mutating at the sink.
Thus the directed graph for $B_0$ has ${1\to2\to\cdots\to k\leftarrow n}$ and $1\to k+1\to k+2\to\cdots\to n-1\leftarrow n$ for some $k$ with $1\le k<n$.
(If $k=1$ or $k=n-1$, then the  arrows are $1\leftarrow n$ and $1\to2\to\cdots\to n-1\leftarrow n$.)

To show that $B_0$ is salient, we wish to find a vector $x\in\reals^n$ whose dot product is strictly positive with each column of $B_0$.
Each column of $B_0$ contains exactly two nonzero entries, so each column constrains $x$ by a strict inequality relating two of its entries.
%Thus the desired vector $x$ exists if and only if these strict inequalities do contradict each other.
The inequalities that appear depend on the parity of $k$ and of $n-k$.
\begin{align*}
k\text{ odd:}\qquad
&x_1>x_3>\cdots>x_k>-x_{n-1}\\
&-x_{k+1}>x_2>x_4>\cdots>x_{k-1}>-x_n\\
k\text{ even:}\qquad
&x_1>x_3>\cdots>x_{k-1}>-x_n\\
&-x_{k+1}>x_2>x_4>\cdots>x_k>-x_{n-1}\\
n-k\text{ odd:}\qquad
&x_1>x_{k+2}>x_{k+4}\cdots>x_{n-1}>-x_k\\
&-x_2>x_{k+1}>x_{k+3}>\cdots>x_{n-2}>-x_n\\
n-k\text{ even:}\qquad
&x_1>x_{k+2}>x_{k+4}\cdots>x_{n-2}>-x_n\\
&-x_2>x_{k+1}>x_{k+3}>\cdots>x_{n-1}>-x_k
\end{align*}
We see that for any combination of the parities of $k$ and $n-k$, the inequalities can all be satisfied, and we conclude that $B_0$ is salient for every choice of $n$ and~$k$.

Since $B_0$ is skew-symmetric, the equations describing its kernel are exactly the same as the inequalities above, but with equality replacing strict inequality everywhere.
By inspection of those equations, we see that every nonzero element of the kernel of $B_0$ has entries with opposite signs.
Thus no nonzero element of the kernel has nonnegative entries, so \cref{extend salient} says that every extension of $B_0$ is salient.

We now conclude the proof exactly as the proof of \cref{finite P point}, appealing to \cref{aff red} for the existence of a maximal red sequence.
\end{proof}

\cref{delta is the man} also allows us to prove one containment in \cref{affine main}.

\begin{proposition}\label{affine main partial}
Suppose $B$ is an exchange matrix of affine type.
If $\lambda$ is contained in the imaginary wall~$\d^B_\infty$, then the dominance region $\P^B_\lambda$ \emph{contains} the line segment parallel to the imaginary ray, with one endpoint at $\lambda$ and the other endpoint on the relative boundary of $\d^B_\infty$.
\end{proposition}
\begin{proof}
Suppose $\kk$ is a sequence of indices in $\set{1,\ldots,n}$ and let $B'=\mu_\kk(B)$.
Then the mutation map $\eta^{B^T}_\kk$ takes the imaginary ray in~$\F_{B^T}$ to the imaginary ray in~$\F_{(B')^T}$ and takes the imaginary cone of $\F_{B^T}$ containing $\lambda$ to an imaginary cone in~$\F_{(B')^T}$.
Since $\delta^{B'}$ has nonnegative entries, the ray $\set{\eta^{B^T}_\kk(\lambda)+aB'\delta^{B'}:a\ge0}$ is contained in $\set{\eta^{B^T}_\kk(\lambda)+B'\alpha:\alpha\ge0}$.
Since $(\eta_\kk^{B^T})^{-1}$ is linear on each imaginary cone and takes the direction $-B'\delta^{B'}$ of the imaginary ray in $\F_{(B')^T}$ to the direction $-B\delta^B$ of the imaginary ray in $\F_{B^T}$, we see that 
\begin{multline*}
\P^B_{\lambda,\kk}=(\eta_\kk^{B^T})^{-1}\set{\eta^{B^T}_\kk(\lambda)+B'\alpha:\alpha\ge0}\\
\supseteq(\eta_\kk^{B^T})^{-1}\set{\eta^{B^T}_\kk(\lambda)+aB'\delta^{B'}:a\ge0}\cap\d^B_\infty\\
=\set{\lambda+aB\delta^B:a\ge0}\cap\d^B_\infty. 
\end{multline*}
This is true for all $\kk$, so $\P^B_\lambda\supseteq\set{\lambda+aB\delta^B:a\ge0}\cap\d^B_\infty$, as desired.
\end{proof}

%\subsection{Neighboring exchange matrices}
\subsection{Neighboring seeds}
In a cluster pattern of affine type, a seed \newword{neighboring the imaginary wall} (or simply a \newword{neighboring seed}) is a seed that has $n-2$ of its $\g$-vectors contained in the imaginary wall.

\begin{lemma}\label{neigh B only}
The property that a seed $t$ is neighboring depends only on the exchange matrix at $t$, not on the specific seed $t$ or on the choice of initial seed $t_0$.
\end{lemma}
\begin{proof}
Suppose $t_0$ and $t$ are seeds in a cluster pattern of affine type, choose $\kk$ such that $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
Let $B_0$ be the exchange matrix at $t_0$ and let $B$ be the exchange matrix at $t$.
Then $\eta^{B^T}_\kk$ is an isomorphism from $\F_{B_0^T}$ to $\F_{B^T}$ sending $\Cone_t^{B_0;t_0}$ to $\Cone_t^{B;t}$, which is the positive cone.
Thus $B$ is neighboring if and only if the positive cone in $\F_{B^T}$ shares $n-2$ rays with the imaginary wall in $\F_{B^T}$.
This is a property that depends only on~$B$.
\end{proof}

In this section, we characterize \newword{neighboring exchange matrices}, the exchange matrices of neighboring seeds.

\begin{lemma}\label{neigh neg and T}
The property of an exchange matrix being of affine type and being neighboring is preserved by transpose and preserved by negating the matrix.
\end{lemma}
\begin{proof}
An exchange matrix $B$ of affine type has $-B$ and $B^T$ of affine type because mutation commutes with transpose and negation, because $A$ is of affine type if and only if $A^T$ is, and because $B$ and $-B$ have the same underlying Cartan matrix.

We have already seen that $-B^T$ is a rescaling of $B$, so \cite[Proposition~7.8(3)]{universal} says that $\F_B$ and $\F_{B^T}$ are related by a linear map.
Also, \cite[Proposition~7.1]{universal} says that $\F_B$ and $\F_{-B}$ are related by the antipodal map.
These linear maps preserve the property that the positive cone shares $n-2$ rays with the imaginary wall.
\end{proof}

We call column $i$ of an exchange matrix $B$ a \newword{quasi-leaf} if column $i$ has at most two nonzero entries and, if $b_{ij}$ and $b_{ik}$ are both nonzero, then the restriction of $B$ to rows and columns $i$, $j$, and $k$ is $\pm\begin{bsmallmatrix*}[r]0&1&-1\\-1&0&1\\1&-1&0\end{bsmallmatrix*}$.
In describing block decompositions of matrices, we allow \newword{empty blocks}, meaning blocks with $0$ columns and/or $0$ rows.
Thus, for example, a ``$0\times 2$ block'' or a ``$0\times 0$ block''.

\begin{theorem}\label{neigh B}
Suppose $B$ is an exchange matrix of affine type.
Then the following conditions are equivalent.
\begin{enumerate}[\quad\rm(i)]
\item \label{neigh}
$B$ is a neighboring exchange matrix.
\item \label{aff 2}
There exist indices $i$ and $j$ such that $\begin{bsmallmatrix}0&b_{ij}\\b_{ji}&0\end{bsmallmatrix}$ is of affine type.
\item \label{neigh detailed}
Up to relabeling, $B$ is
    $
      \begin{bsmallmatrix}
        B_{11} 	& 0 		& 0 		& B_{14} \\ 
        0 		& B_{22} 	& 0 		& B_{24} \\
        0 		& 0 		& B_{33} 	& B_{34} \\
        B_{41} 	& B_{42} 	& B_{43} 	& B_{44}
      \end{bsmallmatrix},
    $ 
where $B_{44}$ is a rank-$2$ exchange matrix of affine type and the following conditions hold for ${\ell\in\set{1,2,3}}$:
\begin{itemize}
\item
$B_{\ell\ell}$ is either a $0\times0$ block or an exchange matrix of finite type A; % (indexed with $B_{11}$, $B_{22}$, and $B_{33}$ in order of increasing size),
\item
If $B_{\ell\ell}$ is nonempty, then its last column is a quasi-leaf of $B_{\ell\ell}$;
\item
If $B_{\ell4}$ is nonempty, then it is nonzero only in its last row;
\item
If $B_{4\ell}$ is nonempty, then it is nonzero only in its last column; and
\item
If $B_{\ell4}$ is nonempty (equivalently if $B_{4\ell}$ is nonempty), then the nonzero rows and columns in $\begin{bsmallmatrix} 0 & B_{\ell4} \\ B_{4\ell} & B_{44} \end{bsmallmatrix}$ form one of the matrices in \cref{submat tab}.
\end{itemize}
The blocks $B_{11}$, $B_{22}$, and $B_{33}$ are in order of increasing size. 
If a submatrix in \cref{submat tab} of type $A_{4}^{(2)}$, $G_{2}^{(1)}$, or $D_{4}^{(3)}$ appears, then $B_{11}$ and $B_{22}$ are empty.
\end{enumerate}
\end{theorem}
	\begin{table}
	\caption{Possible submatrices}
	\label{submat tab}	
%	\begin{center}
	\begin{tabular}{|cc|cc|}
	Type & matrix & Type & matrix \\
	\hline & & & \\[-1ex]
	$A_{2}^{(1)}$ & $\begin{bsmallmatrix*}[r]
	0 & 1 & -1 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
	\end{bsmallmatrix*}$ & & \\[4ex]
	$C_{2}^{(1)}$ & $\begin{bsmallmatrix*}[r]
	0 & 2 & -2 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
	\end{bsmallmatrix*}$ &
	$D_{3}^{(2)}$ & $\begin{bsmallmatrix*}[r]
	0 & 1 & -1 \\
	-2 & 0 & 2 \\
	2 & -2 & 0
	\end{bsmallmatrix*}$ \\[4ex]
	$G_2^{(1)}$ & $\begin{bsmallmatrix*}[r]
	0 & 3 & -3 \\
	-1 & 0 & 2 \\
	1 & -2 & 0
	\end{bsmallmatrix*}$ &
	$D_4^{(3)}$ & $\begin{bsmallmatrix*}[r]
	0 & 1 & -1 \\
	-3 & 0 & 2 \\
	3 & -2 & 0
	\end{bsmallmatrix*}$ \\[4ex]
	$A_{4}^{(2)}$ & $\begin{bsmallmatrix*}[r]
	0 & 1 & -2 \\
	-2 & 0 & 4 \\
	1 & -1 & 0
	\end{bsmallmatrix*}$ &
	$A_{4}^{(2)}$ & $\begin{bsmallmatrix*}[r]
	0 & 2 & -1 \\
	-1 & 0 & 1 \\
	2 & -4 & 0
	\end{bsmallmatrix*}$ 
	\end{tabular}
%	\end{center}	
	\end{table}

%We now prepare to prove \cref{neigh B} as a series of propositions.  
In what follows, fix an acyclic $n\times n$ exchange matrix $B_0$ of affine type that is mutation-equivalent to $B$ and let $c$ be the corresponding Coxeter element.
It is apparent that condition~\eqref{neigh detailed} implies condition~\eqref{aff 2} in \cref{neigh B}, and the following proposition proves that condition~\eqref{aff 2} implies condition~\eqref{neigh}.
\begin{proposition}\label{aff 2 block}
If $B$ is of affine type and there are indices $i$ and $j$ such that $\begin{bsmallmatrix}0&b_{ij}\\b_{ji}&0\end{bsmallmatrix}$ is of affine type, then $B$ is a neighboring exchange matrix.
\end{proposition}

\begin{proof}
We use \cite[Corollary~4.18]{afframe}, which applies to the acyclic affine exchange matrix $B_0$.
The notation $\DF_c$ in \cite{afframe} refers to the doubled Cambrian fan, which coincides with the $\g$-vector fan \cite[Corollary~1.3]{afframe}.
As part of \cite[Corollary~4.18]{afframe}, if an $(n-2)$-dimensional cone $F$ of the $\g$-vector fan of $B_0$ is contained in infinitely many maximal cones of the $\g$-vector fan, then $F$ is in the boundary of the support of the $\g$-vector fan.
Thus $F$ is also in the imaginary wall $\d$, and since $F$ is a cone in the $\g$-vector fan, it is in the boundary of~$\d$.

Now, suppose some $\begin{bsmallmatrix}0&b_{ij}\\b_{ji}&0\end{bsmallmatrix}$ is of affine type (and in particular of infinite type).
Then, in the $\g$-vector fan of $B$, there are infinitely many maximal cones of the $\g$-vector fan that contain the cone $F$ spanned by the $\g$-vectors $\set{\e_k:k\not\in\set{i,j}}$.
There is a mutation map $\eta$ that is an isomorphism from the $\g$-vector fan for $B$ to the $\g$-vector fan for $B_0$.
The image of the positive cone under $\eta$ is a maximal cone in the $\g$-vector fan for $B_0$ whose associated seed $t$ has exchange matrix $B$.
The image of $F$ under $\eta$ is an $(n-2)$-dimensional cone in the $\g$-vector fan for $B_0$ that is contained in infinitely many maximal cones of the $\g$-vector fan.
Thus the image of $F$ is in the boundary of the imaginary wall.
Since the image of the positive cone contains the image of $F$, we conclude that $t$ is a neighboring seed, so that $B$ is a neighboring exchange matrix.
\end{proof}

It remains to show that condition~\eqref{neigh} implies condition~\eqref{neigh detailed}.
Assume that ${B=[b_{ij}]}$ is neighboring.
We will establish some notation and then prove the various parts of condition~\eqref{neigh detailed} as a series of propositions.

The fact that $B$ is neighboring means there is a seed $t$ with exchange matrix~$B$ such that $n-2$ of the columns of $G^{B_0,t_0}_t$ are in the boundary of $\d$.
We use \cref{any B is sort} to choose a seed $t$ that corresponds to a $c$-sortable element~$v$.
We reindex the rows and columns of $B_0$ so that columns $1,\ldots,n-2$ of $G^{B_0,t_0}_t$ are in the boundary of $\d$ and columns $n-1,n$ are not.
If necessary, we swap indices $n-1$ and~$n$ so that $b_{(n-1)n}\ge0$.
With this indexing, we write $\xi_1,\ldots,\xi_n$ for the columns of $G^{B_0,t_0}_t$.
The vectors $\set{\xi_i:i\in[1,n-2]}\cup\set{-B_0\delta}$ span an imaginary cone in $\F_{B_0^T}$.

\begin{proposition}\label{B44 aff}
The submatrix of $B$ with rows and columns indexed by $n-1,n$ is a rank-$2$ exchange matrix of affine type.
\end{proposition}
\begin{proof}
Consider the set of full-dimensional cones of $\F_{B_0^T}$ that have rays spanned by the vectors $\xi_1,\ldots,\xi_{n-2}$. 
This set is infinite.
(If the set is finite, the union of these full-dimensional cones intersects the relative interior of an imaginary cone, contradicting the fact that $\F_{-B_0^T}$ is a fan.)
Thus ${b_{(n-1)n}b_{n(n-1)}\le-4}$.
But also, $B$ is mutation-finite, so $b_{(n-1)n}b_{n(n-1)}\ge-4$ by \cref{mut fin 2x2}.
\end{proof}




There are vectors $\gamma_1,\ldots,\gamma_n\in\APre{c}$ such that $\nu_c(\gamma_i)=\xi_i$ for $i=1,\ldots,n$.
These roots are pairwise $c$-compatible and $\set{\gamma_1,\ldots,\gamma_{n-2}}\cup\set{\delta}$ is an imaginary $c$-cluster.
In particular,  $\gamma_1,\ldots,\gamma_{n-2}$ are all in $\APTre{c}$ and thus in~$\RST{c}$.
Choose integers ${0\leq p_1\leq p_2 \leq p_3}$ such that for each nonzero $p_\ell$ there is a component of $\RST{c}$ of rank $p_\ell+1$.
As a consequence of \cref{compatible in tubes}, we see that $\set{\gamma_1,\ldots,\gamma_{n-2}}$ consists of $p_\ell$ roots from the $\ell\th$ component for each $\ell=1,2,3$.
Thus ${p_1+p+2+p_3=n-2}$.
Define $I_1=\set{1,\ldots,p_1}$, $I_2=\set{p_1+1,\ldots,p_1+p_2}$, and $I_3=\set{p_1+p_2+1,\ldots,n-2}$ and further reindex the rows and columns of $B$ so that each subset $\set{\gamma_i:i\in I_\ell}$ consists of consists of roots from the same component of $\RST{c}$, for $\ell=1,2,3$.
Furthermore, if $p_1>0$, then there is a unique root in $\set{\gamma_i:i\in I_1}$ that is a sum of $p_1$ elements of $\SimplesT{c}$, and we give this the index $p_1$.
Similarly, if $p_2>0$, we make $p_1+p_2$ the index of the unique root in $\set{\gamma_i:i\in I_2}$ that is a sum of $p_2$ elements of $\SimplesT{c}$.
In any case, we make $p_1+p_2+p_3=n-2$ the index of the root in $\set{\gamma_i:i\in I_3}$ that is a sum of $p_3$ elements of $\SimplesT{c}$.
After this reindexing, we consider the decomposition of $B$ into blocks $B_{ij}$ given by the composition $(p_1+p_2+p_3+2)$ of $n$.

A \newword{special index} is the index of the last column/row of $B$ (if $B_{\ell\ell}$ is nonempty) for $\ell=1,2,3$.
There are $1$, $2$, or $3$ special indices, namely whichever of $p_1$, $p_1+p_2$ and $p_1+p_2+p_3=n-2$ are nonzero.
The two indices of $B_{44}$ are the \newword{affine indices} and $B_{44}$ is the \newword{affine submatrix} of~$B$.  \sayS{I think that affine indices is a misleading terminology: it can be confusing
with the affine index of the root system.  \rn{But its fits so well here.  Maybe we can get by with just a comment asking the reader not to be confused?}}
%The \newword{tube components} of $\set{1,\ldots,n}$ are whichever of the sets $\set{1,\ldots,p_1}$, $\set{p_1+1,\ldots,p_1+p_2}$, and $\set{p_1+p_2+1,\ldots,n-2}$ are nonempty.

\begin{proposition}\label{A and q-l}
For $\ell=1,2,3$, if $p_\ell>0$, then $B_{\ell\ell}$ is an exchange matrix of finite type $A_{p_\ell}$ and the special index in $B_{\ell\ell}$ is a quasi-leaf.
\end{proposition}
\begin{proof}
Write $\beta_0,\beta_1,\ldots,\beta_{p_\ell}$ for the simple roots of the component of $\RST{c}$ associated to $B_{\ell\ell}$, numbered so that $\beta_0$ is the unique one of these roots that does not occur in the support of $\gamma_i$ for $i\in I_\ell$.
For $1\le q\le r\le p_\ell$, write $\beta_{qr}$ for $\beta_q+\beta_{q+1}+\cdots+\beta_{r}$ so that each $\gamma_i$ is~$\beta_{q_ir_i}$ for some $q_i$ and $r_i$.
If $i$ is the special index, then $\gamma_i$ is~$\beta_{1p_\ell}$.
%\cref{compatible in tubes} says that roots
We can represent each $\gamma_i$ pictorially as in \cref{tube fig},  \sayN{Need picture.  I can probably modify a PS file from \{affdenom\}.}
by circling the nodes $q_i$ through~$r_i$ on a cycle with nodes labeled $0,1,\ldots,p_\ell$.

%We write $x_{qr}$ for the principal-coefficients cluster variable with denominator vector $\beta_{qr}$.
%The denominator vector of $x_{qr}$ is $\beta_{qr}$, so the $\g$-vector of $x_{qr}$ is $\nu_c(\beta_{qr})$.
%The map $\nu_c$ is linear on the subspace spanned by roots in finite orbits.
%\cref{compatible in tubes} says that two of these cluster variables can be in the same cluster if and only if the corresonding roots are nested or spaced.
%There is a similar characterization of which pairs of these cluster variables are exchangeable \cite[Theorem~7.2]{affdenom}:
%The cluster variables associated to non-special indices in $B_{\ell\ell}$ are exchangeable if and only if the corresponding roots are adjacent or have overlapping support but are not nested.
Write $x_{qr}$ for the principal-coefficients cluster variable with denominator vector~$\beta_{qr}$ and $\g$-vector $\nu_c(\beta_{qr})$.
\cref{compatible in tubes} says cluster variables $x_{qr}$ and $x_{q'r'}$ can be in the same cluster if and only if the $\beta_{qr}$ and $\beta_{q'r'}$ are nested or spaced.
%The map $\nu_c$ is linear on the cone spanned by positive roots.
Similarly, \cite[Theorem~7.2]{affdenom} says that cluster variables $x_{qr}$ and $x_{q'r'}$ are exchangeable if and only if $\beta_{qr}$ and $\beta_{q'r'}$ either have disjoint support but are not spaced or have overlapping support but are not nested.
(In interpreting \cite[Theorem~7.2]{affdenom}, the key fact is that $q\ge1$ and $q'\ge 1$, so neither of these roots has $\beta_0$ in its support.)
Thus two exchangeable cluster variables $x_{qr}$ and $x_{q'r'}$ have, without loss of generality, $q<q'<r$, $q<r<r'$, and $q'\le r+1$.
If they are exchangeable, then \cref{exch ind} implies that the only cluster variables that can occur in the exchange relation for $x_{qr}$ and $x_{q'r'}$ are the cluster variables that are in \emph{every} set $\Gamma$ of $n-1$ cluster variables such that $\Gamma\cup\set{x_{qr}}$ is a cluster and $\Gamma\cup\set{x_{q'r'}}$ is a cluster.
These are the cluster variables $x_{qr'}$, $x_{q'r}$, $x_{q(q'-2)}$, and $x_{(r+2)r'}$, except that the last three only exist if, respectively, $q'\le r$, $q\le q'-2$, or $r'\ge r+2$.

In the proof of \cref{exch ind}, we saw that the exchange relation for $x_{qr}$ and $x_{q'r'}$ has a term with no coefficient variables whose $\g$-vector is the sum of the $\g$-vectors of $x_{qr}$ and $x_{q'r'}$.
Since $\nu_c$ is linear on the cone spanned by positive roots, that monomial must be $x_{qr'}x_{q'r}$.
The other term in the exchange relation is therefore $y^\phi x_{q(q'-2)}^ax_{(r+2)r'}^b$ for some $a\ge0$ and $b\ge0$, where $y^\phi$ is the monomial in the coefficient variables with exponent vector $\phi$.
The $\g$-vector of $y^\phi$ is $B_0\phi$.
The $\g$-vectors of all cluster variables in the exchange relation are in $\delta^\perp$ and the exchange relation is homogeneous in the $\g$-vector grading, so $B_0\phi$ is also in $\delta^\perp$.
In other words, $\omega_c(\delta,\phi)=0$, so \cref{om del fin} says that $\phi$ is in a finite $c$-orbit.

Write $E_c$ for the $n\times n$ matrix whose $ij$-entry is~$(B_0)_{ij}$ if $i>j$, is $1$ if $i=j$, or is~$0$ if $i<j$.
Similarly, write $E_{c^{-1}}$ for the $n\times n$ matrix whose $ij$-entry is $0$ if $i>j$, is $1$ if $i=j$, and is $-b_{ij}$ if $i<j$.
Then $B_0=E_c-E_{c^{-1}}$.
The piecewise linear map $\nu_c$, when applied to vectors with nonnegative entries, is defined by the matrix $E_c$ (taking simple-root coordinates to fundamental-weight coordinates).
Furthermore, a result of Howlett \cite[Theorem~2.1]{Howlett} (see also \cite[Theorem~2.6]{affdenom}) says that the matrix for $c$ on the basis of simple roots is~$-E_{c^{-1}}^{-1}E_c$.
Thus the $\g$-vector of $y^\phi$ is 
\[B_0\phi=(E_c-E_{c^{-1}})\phi=(E_c+E_cc^{-1})\phi=\nu_c(1+c^{-1})\phi.\]
Thus because the exchange relation is homogeneous, the vector $(1+c^{-1})\phi$ equals $\beta_{qr}+\beta_{q'r'}-a\beta_{q(q'-2)}-b\beta_{(r+2)r'}$.
We conclude that $a=b=1$ and that $\phi=\beta_{q'(r+1)}$.

We have written the exchange relation between for $x_{qr}$ and $x_{q'r'}$ as 
\begin{equation}\label{exch rel eq}
x_{qr}x_{q'r'}=x_{qr'}x_{q'r}+y^{\beta_{q'(r+1)}}x_{q(q'-2)}x_{(r+2)r'}.
\end{equation}
Up to a global change of sign, these exchange relations determine the entries $b_{ij}$ of~$B_{\ell\ell}$ for non-special~$j$.
Furthermore, appealing to \cref{neigh neg and T}, we know that $B^T$ is also neighboring.
In particular, the argument to this point implies that the entries of $B_{\ell\ell}^T$ in non-special columns are $0$, $1$, or $-1$.
We see in particular that $B_{\ell\ell}$ has only entries $0$, $1$, or $-1$ and thus is skew-symmetric. %, so we have also determined the entries $b_{ij}$ of $B_{\ell\ell}$ for special $j$.

%we can write down the entries $b_{ij}$ of $B_{\ell\ell}$ for non-special~$j$:
%For $\gamma_j=\beta_{q_jr_j}$, among roots $\gamma_k\neq\gamma_j$ with $q_k\le q_j\le r_j\le r_k$, there is a unique $k$ that minimizes $r_k-q_k$, and for this $k$, either $q_k=q_j\le r_j< r_k$ or $q_k<q_j\le r_j=r_k$.
% ...
%there is a unique $m_j\in[q_j,r_j]$ such that every $\gamma_k$ with $\SuppT(\gamma_k)\subseteq\SuppT(\gamma_j)$ has $\beta_{m_j}\not\in\SuppT(\gamma_k)$.
%There is also a unique largest $m'_j$ such that there exists $\gamma_k=\beta_{(r_j+2)m'_j}$ (allowing $m'_j=r_j+1$ so that no such $\gamma_k$ exists).
%Then the entry $b_{ij}$ is $1$ if $\gamma_i=\beta_{q_j(m_j-1)}$ or $\gamma_i=\beta_{(r_j+2)m'_j}$, and $b_{ij}$ is $-1$ if $\gamma_i=\beta_{q_jm'_j}$ or $\gamma_i=\beta{(m_j+1)r_j}$, and otherwise $b_{ij}=0$.
%Now, appealing to \cref{neigh neg and T}, we know that $B^T$ is also neighboring.
%In particular, the argument to this point implies that the entries of $B_{\ell\ell}^T$ in non-special columns are $0$, $1$, or $-1$.
%We see in particular that $B_{\ell\ell}$ has only entries $0$, $1$, or $-1$ and thus is skew-symmetric.
%If $j$ is special, then $q_j=1$ and $r_j=p_\ell$.
%Defining $m_j$ as before (and ignoring $m'_j$), we see that $b_{ij}$ is $1$ if $\gamma_i=\beta_{q_j(m_j-1)}$, and $b_{ij}$ is $-1$ if $\gamma_i=\beta{(m_j+1)r_j}$, and otherwise $b_{ij}=0$.
%In particular, we have shown that the special index $j$ is a quasi-leaf.
%
%
%BLACK=NEGATIVE if I take the LEFT RED!

On the other hand, consider a $(p_\ell+3)$-gon with vertices labeled $0,1,\ldots,p_\ell+2$ and as initial triangulation $T_0$, take all diagonals with an endpoint at $p_\ell+2$.
The roots~$\gamma_i$ specify a triangulation $T$ that doesn't contain any of these initial arcs.
The root $\gamma_i=\beta_{q_i,r_i}$ corresponds to the diagonal from $q_i-1$ to $r_i+1$, i.e.\ the diagonal that crosses the initial diagonals with endpoints $q_i,q_i+1,\ldots,r_i$ and no other initial diagonals.  
The diagonal associated to $\gamma_i$ is contained in a quadrilateral in~$T$, as illustrated in \cref{tri fig}.  \sayN{Need picture.  I can easily supply PS.}
If the other diagonal is specified by $\gamma'_i$, then up to swapping primed and unprimed indices, the exchange relation is exactly what was given above.
We conclude that, up to a global sign, the entries of  $B_{\ell\ell}$ in non-special columns agree with the entries of the signed adjacency matrix of $T$.
Since both matrices are skew-symmetric, they are therefore equal up to a sign.
We see that $B_{\ell\ell}$ is of type $A_{p_\ell}$.

In the triangulation $T$, there are two triangles containing the diagonal associated to $\beta_{1,p_\ell}$.
One of these triangles also contains two segments of the boundary (the segments connecting $0$ to $1$ and $0$ to $p_\ell+2$).
The other triangle contains zero, one, or two diagonals (non-boundary segments).
Thus the column associated to the special index has at most two nonzero entries, and if there are two, then those two, together with the special index, induce the submatrix of $B$ that appears in the definition of a quasi-leaf.
\end{proof}

The following \lcnamecref{mostly zeros} is immediate from the fact that the exchange relations \eqref{exch rel eq} only involve cluster variables associated to indices in $B_{\ell\ell}$.

\begin{proposition}\label{mostly zeros}
For  $k\neq\ell\in\set{1,2,3}$, the block $B_{k\ell}$ is zero, except possibly the entry $b_{ij}$ where $i$ is the special index in $I_k$ and $j$ is the special index in $I_\ell$.
For $\ell\in\set{1,2,3}$ and for $i$ the special index in $I_\ell$, the block $B_{\ell4}$ is zero except in row~$i$ and the block $B_{4\ell}$ is zero except in column~$i$.
\end{proposition}
%\begin{proof}
%Since the exchange relations to exchange out the cluster variable with non-special indices in $B_{\ell\ell}$ only involve cluster variables associated to indices in $B_{\ell\ell}$, we see that $b_{ij}=0$ whenever one of $i$ or $j$ is a non-special index in $B_{\ell\ell}$ and the other is not an index in $B_{\ell\ell}$.
%In other words, the blocks $B_{k\ell}$ with $k\neq\ell\in[1,2,3]$ can only have a non-zero entry in the bottom right corner (indexed by the two special indices).
%Similarly blocks $B_{\ell4}$ for $\ell\in[1,2,3]$ can only be non-zero in the last row and blocks $B_{4\ell}$ for $\ell\in\set{1,2,3}$ can only be non-zero in the last column.
%\end{proof}

\begin{proposition}\label{not all zeros}
For $\ell\in\set{1,2,3}$, if $B_{\ell4}$ is nonempty, then it is nonzero, and equivalently $B_{4\ell}$ is nonzero.
\end{proposition}
\begin{proof}
Let $k$ be the special index in $I_\ell$.
Continuing notation from the proof of \cref{A and q-l}, $\xi_k$ is not exchangeable with any other cluster variable whose $\g$-vector is in the imaginary wall $\d^B_\infty$.
(This can be seen from the characterization of exchangeability in \cite[Theorem~7.2]{affdenom}.
In the language of that theorem, if $\gamma_k$ is $c$-exchangeable with any other root in $\APTre{c}$, then that root has $\beta_0$ is its support.
Since $\SuppT(\gamma_k)$ is $\set{\beta_1,\ldots,\beta_{p_\ell}}$, the union of the supports of the two roots is $\set{\beta_0,\ldots,\beta_{p_\ell}}$, so the two roots are not $c$-real-exchangeable.)
Therefore the exchange relation for $x_{1p_\ell}$ exchanges it with a cluster variable whose $\g$-vector is not in $\d^B_\infty$.  
In particular, the right side of that exchange relation can't involve only cluster variables in indexed by non-affine indices, and therefore $B_{\ell4}$ has at least one nonzero entry in column $k$.
\end{proof}

\begin{proposition}\label{3x3 submat}
For $\ell\in\set{1,2,3}$, if $B_{\ell4}$ is nonempty, then the nonzero rows and columns in $\begin{bsmallmatrix} 0 & B_{\ell4} \\ B_{4\ell} & B_{44} \end{bsmallmatrix}$ form one of the matrices in \cref{submat tab}.
\end{proposition}
\begin{proof}
Let $k$ be the special index in $I_\ell$.
To make the notation more compact, we use $p$ to stand for $n-1$.
The submatrix with rows and columns indexed by $k,p,n$ has a pair of opposite off-diagonal entries whose product is $-4$, so its underlying Cartan matrix is not of finite or affine type.
(There are no $3\times3$ Cartan matrices of finite or affine type with opposite off-diagonal entries whose product is $4$.)
Thus \cref{acyc mut fin} says that the submatrix is not acyclic.
In other words, $b_{kp}>0$ and $b_{kn}<0$, and therefore $b_{pk}<0$ and $b_{nk}>0$.
By \cref{mut fin 2x2}, each of these entries has absolute value $1$, $2$, $3$, or~$4$.

We first deal with the case where the affine submatrix has entries $\pm2$.
Below, we display some mutations of the submatrix.
%We will derive some inequalities from the fact that each of the mutations must also be non-acyclic and have opposite off-diagonal entries with products at least $-4$.
\begin{multline*}
\begin{bsmallmatrix*}
	0 & b_{kp} & b_{kn} \\
	b_{pk} & 0 & 2 \\
	b_{nk} & -2 & 0
\end{bsmallmatrix*}
\overset{p}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -b_{kp} & 2b_{kp}+b_{kn} \\
	-b_{pk} & 0 & -2 \\
	2b_{pk}+b_{nk}& 2 & 0
\end{bsmallmatrix*}
\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & 3b_{kp}+2b_{kn} & -2b_{kp}-b_{kn} \\
	3b_{pk}+2b_{nk} & 0 & 2 \\
	-2b_{pk}-b_{nk} & -2 & 0
\end{bsmallmatrix*}\\
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -3b_{kp}-2b_{kn} & 2b_{kp}+b_{kn} \\
	-3b_{pk}-2b_{nk} & 0 & 2+(2b_{kp}+b_{kn})(3b_{pk}+2b_{nk}) \\
	2b_{pk}+b_{nk} & -2-(2b_{pk}+b_{nk})(3b_{kp}+2b_{kn})& 0
\end{bsmallmatrix*}
\end{multline*}
\begin{multline*}
\begin{bsmallmatrix*}
	0 & b_{kp} & b_{kn} \\
	b_{pk} & 0 & 2 \\
	b_{nk} & -2 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & b_{kp}+2b_{kn} & -b_{kn} \\
	b_{pk}+2b_{nk} & 0 & -2 \\
	-b_{nk} & 2 & 0
\end{bsmallmatrix*}\overset{p}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -b_{kp}-2b_{kn} & 2b_{kp}+3b_{kn} \\
	-b_{pk}-2b_{nk} & 0 & 2 \\
	2b_{pk}+3b_{nk} & -2 & 0
\end{bsmallmatrix*}
\end{multline*}
The fact that each of these matrices is non-acyclic determines the sign of each entry.
Two of these inequalities are shown here, with an inequality that follows:
\[
3b_{kp}+2b_{kn}>0,\quad 2b_{kp}+3b_{kn}<0\quad \implies\quad b_{kn}>-\frac32b_{kp}>\frac94b_{kn}\\
\]
This rules out all pairs $(b_{kp},b_{kn})$ except $(1,-1)$, $(2,-2)$, $(3,-3)$, $(3,-4)$, and $(4,-4)$.
If $b_{kn}=-4$, then $b_{nk}=1$, and also $b_{kp}\ge3$ so also $b_{pk}=-1$.
Skew-symmetrizability of $B$ implies that $\frac{b_{pk}}{b_{kp}}=\frac{b_{nk}}{b_{kn}}$, which rules out the possibility that $(b_{kp},b_{kn})=(3,-4)$.
Similar considerations starting with the inequalities $3b_{pk}+2b_{nk}<0$ and $2b_{pk}+3b_{nk}>0$ rule out all pairs $(b_{pk},b_{nk})$ except $(-1,1)$, $(-2,2)$, $(-3,3)$, and $(-4,4)$.

We see that, in every case, $b_{kp}=-b_{kn}>0$ and $b_{pk}=-b_{nk}<0$.
To verify that all possibilities are listed in \cref{submat tab}, it remains to rule out the cases where $(b_{kp},b_{kn},b_{pk},b_{nk})$ are $(1,-1,-4,4)$, $(2,-2,-2,2)$, or $(4,-4,-1,1)$.
None of these three cases are possible because, as explained in \cref{growth sec}, in these cases, the submatrix would  have exponential growth and thus contradict the fact that $B$ has linear growth.

Next, we deal with the case where the affine submatrix has entries $-1$ and $4$.
\begin{multline*}
\begin{bsmallmatrix*}
	0 & b_{kp} & b_{kn} \\
	b_{pk} & 0 & 4	 \\
	b_{nk} & -1 & 0
\end{bsmallmatrix*}
\overset{p}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -b_{kp} & 4b_{kp}+b_{kn} \\
	-b_{pk} & 0 & -4 \\
	b_{pk}+b_{nk}& 1 & 0
\end{bsmallmatrix*}
\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & 3b_{kp}+b_{kn}& -4b_{kp}-b_{kn} \\
	3b_{pk}+4b_{nk} & 0 & 4 \\
	-b_{pk}-b_{nk}& -1 & 0
\end{bsmallmatrix*}\\
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -3b_{kp}-b_{kn}& 4b_{kp}+b_{kn} \\
	-3b_{pk}-4b_{nk} & 0 & 4+(3b_{pk}+4b_{nk})(4b_{kp}+b_{kn}) \\
	b_{pk}+b_{nk}& -1-(b_{pk}+b_{nk})(3b_{kp}+b_{kn}) & 0
\end{bsmallmatrix*}
\end{multline*}
\begin{multline*}
\begin{bsmallmatrix*}
	0 & b_{kp} & b_{kn} \\
	b_{pk} & 0 & 4	 \\
	b_{nk} & -1 & 0
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & b_{kp}+b_{kn} & -b_{kn} \\
	b_{pk}+4b_{nk} & 0 & -4	 \\
	-b_{nk} & 1 & 0
\end{bsmallmatrix*}\overset{p}{\longrightarrow}
\begin{bsmallmatrix*}
	0 & -b_{kp}-b_{kn} & 4b_{kp}+3b_{kn} \\
	-b_{pk}-4b_{nk} & 0 & 4	 \\
	b_{pk}+3b_{nk} & -1 & 0
\end{bsmallmatrix*}
\end{multline*}
These matrices determine inequalities, including
\[
3b_{kp}+b_{kn}>0,\quad 4b_{kp}+3b_{kn}<0\quad \implies\quad b_{kn}>-\frac32b_{kp}>\frac94b_{kn},\\
\]
which rules out all pairs $(b_{kp},b_{kn})$ except $(1,-2)$, $(2,-3)$, and $(2,-4)$.
If $(b_{kp},b_{kn})=(2,-3)$, then $b_{pk}\in\set{-1,-2}$ and $b_{nk}=1$.
Skew-symmetrizability of $B$ implies that $\frac{b_{pk}}{b_{kp}}=4\frac{b_{nk}}{b_{kn}}$, which fails for either choice of $b_{pk}$, thus ruling out the possibility that $(b_{kp},b_{kn})=(2,-3)$.
Similar considerations starting with $b_{pk}+3b_{nk}>0$ and $3b_{pk}+4b_{nk}<0$ rule out all pairs $(b_{pk},b_{nk})$ except $(-2,1)$ and $(-4,2)$.
Thus $(b_{kp},b_{kn},b_{pk},b_{nk})$ is $(1,-2,-2,1)$, $(1,-2,-4,2)$, or $(2,-4,-2,1)$, but we rule out the second and third possibilities because they would imply exponential growth.

The last case, where the affine submatrix has entries $-4$ and $1$, is related to the previous case by passing to the negative transpose, so the same calculations lead to the desired result.
\end{proof}

\begin{proposition}\label{one component}
If the nonzero rows and columns in $\begin{bsmallmatrix} 0 & B_{\ell4} \\ B_{4\ell} & B_{44} \end{bsmallmatrix}$ are of type $A_{4}^{(2)}$, $G_{2}^{(1)}$, or $D_{4}^{(3)}$ appears, then $\ell=3$ and $B_{11}$ and $B_{22}$ are empty.
\end{proposition}
\begin{proof}
If the submatrix is of type $A_{4}^{(2)}$, then the underlying Cartan matrix defines a root system with 3 root lengths, so $\RS$ is of type $A_{2l}^{(2)}$ in the notation of \cite[Chapter~4]{Kac}.
In this case, the root system can be rescaled to be of type $C^{(1)}$, and therefore $\RST{c}$ has only one component (see \cite[Table 1]{affdenom}).
If the submatrix is of type $G_{2}^{(1)}$ or $D_{4}^{(3)}$, then there are two root lengths, related by a factor of $\sqrt{3}$, so $\RS$ is of type $G_{2}^{(1)}$ or~$D_{4}^{(3)}$.
Again $\RST{c}$ has one component (and indeed the submatrix is all of~$B$).
\end{proof}

\begin{proposition}\label{really zero}
The matrices $B_{12}$, $B_{13}$, $B_{23}$, $B_{21}$, $B_{31}$, and $B_{32}$ are zero.
\end{proposition}
\begin{proof}
There is nothing to prove unless the affine submatrix is $\begin{bsmallmatrix*}[r]0&2\\-2&0\end{bsmallmatrix*}$.
(If not, \cref{one component} says that $B_{12}$, $B_{13}$, $B_{23}$, $B_{21}$, $B_{31}$, and $B_{32}$ are all empty.)

By \cref{mostly zeros}, it only remains to show that, for any two distinct special indices $j$ and $k$, the entry $b_{jk}$ is zero.
Suppose to the contrary that distinct special indices $j$ and $k$ have $b_{jk}\neq0$.
We may as well choose $j$ and $k$ such that $b_{jk}>0$.
Again writing $p$ for $n-1$, we consider the $4\times4$ submatrix of $B$ with rows and columns indexed by $j,k,p,n$.
We have $b_{jp}=-b_{jn}>0$, $b_{kp}=-b_{kn}>0$, $b_{pj}=-b_{nj}<0$, and $b_{pk}=-b_{nk}<0$.
%WON'T BE NECESSARY:
%Also, skew-symmetrizability of $B$ implies that $b_{kj}=-\frac{b_{nj}b_{kn}}{b_{jn}b_{nk}}b_{jk}<0$.
We compute the mutation in direction $j$.
\begin{align*}
\begin{bsmallmatrix*}
0&b_{jk}&-b_{jn}&b_{jn}\\
b_{kj}&0&-b_{kn}&b_{kn}\\
-b_{nj}&-b_{nk}&0&2\\
b_{nj}&b_{nk}&-2&0
\end{bsmallmatrix*}
&\overset{j}{\longrightarrow}
\begin{bsmallmatrix*}
0&-b_{jk}&b_{jn}&-b_{jn}\\
-b_{kj}&0&-b_{kn}&b_{kn}-b_{kj}b_{jn}\\
b_{nj}&-b_{nk}&0&2+b_{jn}b_{nj}\\
-b_{nj}&b_{nk}+b_{nj}b_{jk}&-2-b_{jn}b_{nj}&0
\end{bsmallmatrix*}
\end{align*}
Since $b_{jk}$, $b_{nj}$, and $b_{nk}$ are all strictly positive, $b_{nk}+b_{nj}b_{jk}\ge2$.
Similarly, $b_{kn}$, $b_{kj}$, and $b_{jn}$ are strictly negative, so $b_{kn}-b_{kj}b_{jn}\le-2$.
Both of these inequalities must be equality, and we see that $b_{jk}=b_{nj}=b_{nk}=1$ and $b_{kn}=b_{kj}=b_{jn}=-1$.
We have determined all entries of the $4\times 4$ submatrix.
One easily verifies that the submatrix has the property that every mutation agrees with some permutation of the row/column indices.  
Therefore, we see that this submatrix is not on the list exchange matrices of sub-exponential growth in \cite[Theorem~1.1]{FeShThTu12}.
Therefore $B$ has exponential growth, and by this contradiction, we conclude that $b_{jk}=0$.
\end{proof}

We have proved \cref{neigh B}.
We conclude this section with some useful facts that can be proved using the \lcnamecref{neigh B}.

\begin{lemma}\label{affine mut}
Suppose $B$ is a neighboring exchange matrix and let $\kk$ be a $2$-element sequence consisting of the two affine indices. 
Then $\mu_\kk(B)=B$.
\end{lemma}
\begin{proof}
\cref{neigh B} implies that most entries of $B$ are fixed under mutation in an affine position.
The only entries that might not be fixed are entries $b_{ij}$ where one of $i$ or $j$ is affine and the other is affine or special.
Thus we only need to check the submatrices shown in \cref{submat tab}.
In each case, we check that mutation in one of the affine entries has the effect of negating the submatrix.
An additional mutation in the other affine index still fixes all entries of $B$ except those described above and again negates the submatrix, so that $\mu_\kk(B)=B$.  
%\begin{align*}
%&\begin{bsmallmatrix*}[r]
%	0 & 1 & -1 \\
%	-1 & 0 & 2 \\
%	1 & -2 & 0
%\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & -1 & 1 \\
%	1 & 0 & -2 \\
%	-1 & 2 & 0
%\end{bsmallmatrix*}\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & 1 & -1 \\
%	-1 & 0 & 2 \\
%	1 & -2 & 0
%\end{bsmallmatrix*}\\
%&\begin{bsmallmatrix*}[r]
%	0 & 2 & -2 \\
%	-1 & 0 & 2 \\
%	1 & -2 & 0
%\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & -2 & 2 \\
%	1 & 0 & -2 \\
%	-1 & 2 & 0
%\end{bsmallmatrix*}\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & 2 & -2 \\
%	-1 & 0 & 2 \\
%	1 & -2 & 0
%\end{bsmallmatrix*}\\
%&\begin{bsmallmatrix*}[r]
%	0 & 1 & -1 \\
%	-2 & 0 & 2 \\
%	2 & -2 & 0
%\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & -1 & 1 \\
%	2 & 0 & -2 \\
%	-2 & 2 & 0
%\end{bsmallmatrix*}\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & 1 & -1 \\
%	-2 & 0 & 2 \\
%	2 & -2 & 0
%\end{bsmallmatrix*}\\
%&\begin{bsmallmatrix*}[r]
%	0 & 3 & -3 \\
%	-1 & 0 & 2 \\
%	1 & -2 & 0
%\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & -3 & 3 \\
%	1 & 0 & -2 \\
%	-1 & 2 & 0
%\end{bsmallmatrix*}\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & 3 & -3 \\
%	-1 & 0 & 2 \\
%	1 & -2 & 0
%\end{bsmallmatrix*}\\
%&\begin{bsmallmatrix*}[r]
%	0 & 1 & -1 \\
%	-3 & 0 & 2 \\
%	3 & -2 & 0
%\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & -1 & 1 \\
%	3 & 0 & -2 \\
%	-3 & 2 & 0
%\end{bsmallmatrix*}\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & 1 & -1 \\
%	-3 & 0 & 2 \\
%	3 & -2 & 0
%\end{bsmallmatrix*}\\ 
%&\begin{bsmallmatrix*}[r]
%	0 & 1 & -2 \\
%	-2 & 0 & 4 \\
%	1 & -1 & 0
%\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & -1 & 2 \\
%	2 & 0 & -4 \\
%	-1 & 1 & 0
%\end{bsmallmatrix*}\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%	0 & 1 & -2 \\
%	-2 & 0 & 4 \\
%	1 & -1 & 0
%\end{bsmallmatrix*}\\ 
%&\begin{bsmallmatrix*}[r]
%	0 & 2 & -1 \\
%	-1 & 0 & 1 \\
%	2 & -4 & 0
%\end{bsmallmatrix*}\overset{n-1}{\longrightarrow} 
%\begin{bsmallmatrix*}[r]
%	0 & -2 & 1 \\
%	1 & 0 & -1 \\
%	-2 & 4 & 0
%\end{bsmallmatrix*}\overset{n}{\longrightarrow} 
%\begin{bsmallmatrix*}[r]
%	0 & 2 & -1 \\
%	-1 & 0 & 1 \\
%	2 & -4 & 0
%\end{bsmallmatrix*} \qedhere
%\end{align*}
\end{proof}

\begin{proposition}\label{neigh good stuff}
Suppose $B$ is a neighboring exchange matrix.
\begin{enumerate}[\quad\bf1.]
\item\label{neigh delta}
The vector $\delta^B$ is zero in all non-affine indices.
Its affine entries are
\begin{itemize}
\item
$1,1$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&2\\-2&0\end{bsmallmatrix*}$,
\item
$2,1$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&4\\-1&0\end{bsmallmatrix*}$, or
\item
$1,2$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&1\\-4&0\end{bsmallmatrix*}$.
\end{itemize}
%If the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&2\\-2&0\end{bsmallmatrix*}$, then $\delta^B$ has entries $1,1$ in the affine indices. 
%If the affine submatrix is $\begin{bsmallmatrix*}[r]0&4\\-1&0\end{bsmallmatrix*}$ or $\begin{bsmallmatrix*}[r]0&1\\-4&0\end{bsmallmatrix*}$, then $\delta^B$ has entries $2,1$ or $1,2$ respectively in the affine indices.
\item\label{neigh im ray}
The vector $-\frac12B\delta^B$ (the shortest integer vector that spans the imaginary ray) is zero in all non-affine entries.
Its affine entries are
\begin{itemize}
\item
$-1,1$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&2\\-2&0\end{bsmallmatrix*}$,
\item
$-2,1$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&4\\-1&0\end{bsmallmatrix*}$, or
\item
$-1,2$ if the affine submatrix of $B$ is $\begin{bsmallmatrix*}[r]0&1\\-4&0\end{bsmallmatrix*}$.
\end{itemize}
\end{enumerate}
\end{proposition}
\begin{proof}
We state these assertions about $\delta^B$ and $-\frac12B\delta^B$ together because the proofs of the assertions are intertwined.

Considering a cluster pattern with $B$ as the initial exchange matrix, the initial $\g$-vectors for non-affine indices are contained in the imaginary wall~$\d^B_\infty$.
Thus \cref{delta is the man}.\ref{im hyp} implies that $\delta^B$ is zero in all non-affine positions.
(More carefully, in light of~\cref{danger: co-roots}, the simple-co-root coordinates of $\delta^B$ are zero in non-affine positions, and therefore the same is true of the simple root coordinates that we take as the ``entries'' of $\delta^B$.)  \sayN{I don't think this proof runs afoul of \cref{danger: co-roots} because we don't use  \cref{delta is the man}.\ref{im hyp} anywhere but here.}
%NO. \cref{delta is the man}.\ref{im ray} DOES NOT give this minimality.
%Furthermore, \cref{delta is the man}.\ref{im ray} says that $\delta^B$ has nonnegative entries and is minimal among integer vectors obtained from it by positive integer scaling.

Before determining the entries of $\delta^B$ in the two affine positions, we determine some entries of $-\frac12B\delta^B$.
\cref{neigh B} and the fact that $\delta^B$ is zero in all non-affine positions implies that $-\frac12B\delta^B$ is zero in non-special non-affine positions.
Since~$-\frac12B\delta^B$ is the shortest integer vector in the imaginary ray, \cref{affine mut} implies that it is fixed under $\eta^{B^T}_\kk$, where $\kk$ is a $2$-element sequence consisting of the two affine indices. 
We use the fact that $-\frac12B\delta^B$ is fixed under $\eta^{B^T}_\kk$ to determine its affine entries $r_{n-1}$, and $r_n$.
%We need only consider the submatrices shown in \cref{submat tab}.
%The mutations are computed below, where the entries $r_k$ (for special~$k$), $r_{n-1}$, and $r_n$ denote entries in $-\frac12B\delta^B$.
We again use $p$ to stand for $n-1$.
We use the notation 
\[m(a,b)=\begin{cases}
ab&\text{if }\sgn(a)=\sgn(b)=+\\
-ab&\text{if }\sgn(a)=\sgn(b)=-\\
0&\text{otherwise}.
\end{cases}\]
To determine the affine entries, we need only the $2\times2$ affine submatrices of $B$.
In the most common case from \cref{submat tab}, the mutations are as follows:
\[
\begin{bsmallmatrix*}
0 & 2 & r_p\\
-2 & 0 & r_n
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}
0 & -2 & -r_p\\
2 & 0 & r_n+m(r_p,-2)
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
0 & 2 & -r_p+m(r_n+m(r_p,-2),-2)\\
-2 & 0 & -r_n-m(r_p,-2)
\end{bsmallmatrix*}
\]
We see that $r_n=-r_n-m(r_p,-2)$ and $r_p=-r_p+m(r_n+m(r_p,-2),-2)$, which can be rewritten $r_p=-r_p+m(-r_n,-2)$.
If $r_p$ is positive, then $r_n$ is zero, and thus also $r_p$ is zero.
By this contradiction, we see that $r_p$ is nonpositive.
Similarly, we see that $r_n$ is nonnegative.
%If $r_n$ is negative, then $r_p$ is zero, and thus also $r_n$ is zero, and we see that $r_n$ is nonnegative.
Thus both equations say $r_n=-r_p$.

Let $d_p$ and $d_n$ be the entries of $\delta^B$ in its affine positions.
Since $\delta^B$ is zero in its non-affine positions and since $r_p$ and $r_n$ are the affine entries of $-\frac12B\delta^B$, we have $r_p=-d_n$ and $r_n=d_p$.
Therefore $d_p=d_n$.
From there, we see that the special entries of $-\frac12B\delta^B$ are also zero, and since $-\frac12B\delta^B$ is the shortest integer vector in the ray is spans (\cref{delta is the man}.\ref{im ray}), $r_n=1=-r_p$.
Therefore also $d_p=d_n=1$.

In the next case from \cref{submat tab}, the mutations are
\[\begin{bsmallmatrix*}
0 & 4 & r_p\\
-1 & 0 &r_n
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}
0 & -4 & -r_p\\
1 & 0 &r_n+m(r_p,-1)
\end{bsmallmatrix*}\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
0 & 4 &-r_p+m(r_n+m(r_p,-1),-4)\\
-1 & 0 &-r_n-m(r_p,-1)
\end{bsmallmatrix*}
\]
%We see that $r_n=-r_n-m(r_p,-1)$ and $r_p=-r_p+m(r_n+m(r_p,-1),-4)$, which can be rewritten $r_p=-r_p+m(-r_n,-4)$.
%If $r_p$ is positive, then $r_n$ is zero, and thus also $r_p$ is zero.
%By this contradiction, we see that $r_p$ is nonpositive.
%Similarly, we see that $r_n$ is nonnegative.
%If $r_n$ is negative, then $r_p$ is zero, and thus also $r_n$ is zero, and we see that $r_n$ is nonnegative.
%One equation says $2r_n=-r_p$ and the other says $2r_p=-4r_n$.
Arguing as before, we see that $r_n$ is nonnegative and $2r_n=-r_p$.
Again taking $d_p$ and $d_n$ to be the affine entries of $\delta^B$, we also find that $d_p=2r_n=-r_p=2d_n$, so that the special entries of $-\frac12B\delta^B$ are zero, and thus $r_p=-2$ and $r_n=1$.
Therefore,
$d_p=2$ and $d_n=1$.
%Since $\delta^B$ is zero in its non-affine positions and since $r_p$ and $r_n$ are the affine entries of $-\frac12B\delta^B$, we compute that $r_p=-2d_n$ and $r_n=\frac12d_p$.
%Therefore $d_p=2r_n=-r_p=2d_n$,...

In the final case, the mutations are
\[\begin{bsmallmatrix*}
0 & 1 & r_p\\
-4 & 0 &r_n
\end{bsmallmatrix*}\overset{n-1}{\longrightarrow} 
\begin{bsmallmatrix*}
0 & -1 & -r_p\\
4 & 0 &r_n+m(r_p,-4)
\end{bsmallmatrix*}\overset{n}{\longrightarrow} 
\begin{bsmallmatrix*}
0 & 1 &-r_p+m(r_n+m(r_p,-4),-1)\\
-4 & 0 &-r_n-m(r_p,-4)
\end{bsmallmatrix*}
\]
We see that $r_n=-2r_p\ge0$ and $2d_p=r_n=-2r_p=d_n$, so that special entries of~$-\frac12B\delta^B$ are zero, and thus $r_p=-1$, $r_n=2$, $d_p=1$, and $d_n=2$.
%We see that $r_n=-r_n-m(r_p,-4)$ and $r_p=-r_p+m(r_n+m(r_p,-4),-1)$, which can be rewritten $r_p=-r_p+m(-r_n,-1)$.
%If $r_p$ is positive, then $r_n$ is zero, and thus also $r_p$ is zero.
%By this contradiction, we see that $r_p$ is nonpositive.
%Similarly, we see that $r_n$ is nonnegative.
%If $r_n$ is negative, then $r_p$ is zero, and thus also $r_n$ is zero, and we see that $r_n$ is nonnegative.
%One equation says $2r_n=-4r_p$ and the other says $2r_p=-r_n$.
%Since $\delta^B$ is zero in its non-affine positions and since $r_p$ and $r_n$ are the affine entries of $-\frac12B\delta^B$, we compute that $r_p=-\frac12d_n$ and $r_n=2d_p$.
%Therefore $2d_p=r_n=-2r_p=d_n$, ...
\end{proof}

\subsection{Type-C companions of neighboring exchange matrices}
To each neighboring exchange matrix $B$, we now associate an $(n-2)\times(n-2)$ exchange matrix called the \newword{type-C companion} of $B$ and written $\Comp(B)$.
The type-C companion $\Comp(B)$ agrees with the \emph{non-affine} rows and columns of $B$, except that all entries in \emph{special columns} are multiplied by $2$.
The type-C companion $\Comp(B)$ has a diagonal block decomposition with $1$, $2$, or $3$ diagonal blocks, having the same sizes as the blocks $B_{11}$, $B_{22}$, and/or $B_{33}$ of~$B$.
The following \lcnamecref{Comp is C} is immediate from \cref{neigh B} and \cite[Proposition~3.3]{NakanishiStella} and justifies the terminology.

\begin{proposition}\label{Comp is C}
Given a neighboring exchange matrix $B$, each diagonal block of $\Comp(B)$ is of finite type C.
\end{proposition}
%\begin{proof}
%This should just be Salvatore and Tomoki's characterizations of type C exchange matrices, together with the quasi-leaf part of \cref{neigh B}\eqref{neigh detailed}.
%
%\end{proof}

\sayN{Salvatore, you said you were confused about the proof of the following proposition, and you were right to be!  It was wrong, and in fact the statement was wrong.  I think this is now correct and everything still works, but it's slightly more complicated}
Let $\CompPlus(C)$ be the $n\times(n-2)$-matrix agrees with the non-affine \emph{columns} of $B$, except that all entries in \emph{special columns} are multiplied by $2$.
Although $\CompPlus(B)$ is an extension of $\Comp(C)$ in the sense of ``extended exchange matrices'', we don't want to think of it that way (and have chosen an overline in the notation rather than a tilde) because we will not always mutate it like an extended exchange matrix.
(See \cref{special mut}, below.)

%\begin{proposition}\label{Comp span}
%Suppose $B$ is a neighboring exchange matrix.
%Then the intersection of the hyperplane $(\delta^B)^\perp$ with the nonnegative linear span of the columns of $B$ is contained in the nonnegative linear span of $\frac12B\delta^B$ and the columns of $\Comp(B)$.
%\end{proposition}
%\begin{proof}
%By inspection of \cref{submat tab} and in light of \cref{neigh good stuff}, we observe the following facts:
%First, the vector $-\frac12B\delta^B$ spanning the affine ray is a \emph{negative} linear combination of the last two columns of $B$.
%Second, the linear span of the last two columns of $B$ intersected with the hyperplane $(\delta^B)^\perp$ is the line spanned by $\frac12B\delta^B$.
%Third, $\frac12B\delta^B$ has nonzero entries only in the affine indices, and those entries are $1,-1$ unless the affine submatrix is $\begin{bsmallmatrix*}[r]0&4\\-1&0\end{bsmallmatrix*}$ or $\begin{bsmallmatrix*}[r]0&1\\-4&0\end{bsmallmatrix*}$, in which case, those entries are $2,-1$ or $1,-2$ respectively.
%Fourth, each non-affine column of $B$ is half the corresponding column of $\Comp(B)$ plus a nonnegative multiple of $-\frac12B\delta^B$. 
%\saySS{I am confused here: why half? non-affine non-special columns are unchanged, no?}
%(This multiple is zero unless the column is a special column.)
%The proposition follows.
%\end{proof}

\begin{proposition}\label{Comp span}
Suppose $B$ is a neighboring exchange matrix.
Then the intersection of the hyperplane $(\delta^B)^\perp$ with the nonnegative linear span of the columns of $B$ is contained in the nonnegative linear span of $\frac12B\delta^B$ and the columns of $\CompPlus(B)$.
\end{proposition}
\begin{proof}
Suppose $\alpha$ has nonnegative entries and $B\alpha$ is in $(\delta^B)^\perp$.
Write $\alpha$ as $\beta+\gamma$ such that $\beta$ is a linear combination of simple roots in non-affine entries and $\gamma$ is a combination of simple roots in affine positions.
Then $B\beta$ is a nonnegative linear combination of the non-affine columns of $B$, and equivalently a nonnegative linear combination of the columns of $\CompPlus(B)$.
Also $B\beta$ is in $(\delta^B)^\perp$ because every non-affine column of $B$ is.
By \cref{neigh good stuff}, since $B\gamma\in(\delta^B)^\perp$, the vector $\gamma$ is a scalar multiple of $\delta^B$.
Thus $B\gamma$ is in the nonnegative span of $\frac12B\delta^B$.
\end{proof}

\begin{proposition}\label{nonspecial mut}
Suppose $B$ is a neighboring exchange matrix and $k$ is a non-special, non-affine index of~$B$.
Then 
\begin{itemize}
\item
$\mu_k(B)$ is neighboring,
\item
$\Comp(\mu_k(B))=\mu_k(\Comp(B))$,
\item
$\CompPlus(\mu_k(B))=\mu_k(\CompPlus(B))$, and these agree with $\CompPlus(B)$ in the affine rows,
\item
the mutation map $\eta^{B^T}_k$ fixes the imaginary ray pointwise, and 
\item
If $x$ is a vector whose affine entries are zero, then $\eta^{B^T}_k(x)=\eta^{\Comp(B)^T}_k(x)$.
\end{itemize}
\end{proposition}
\begin{proof}
Mutating at $k$ means replacing the root $\gamma_k$ in the real $c$-cluster $\set{\gamma_1,\ldots,\gamma_n}$.
Since $k$ is not special and not affine, $\gamma_k$ is replaced by another root in $\APTre{c}$.
Thus $\xi_k$ is replaced by another vector in $\d_\infty$.
We see that $\mu_k(B)$ is neighboring.

Furthermore, since $k$ is not special and not affine, row $k$ and column $k$ only have nonzero entries within one of the blocks $B_{\ell\ell}$ for $\ell\in\set{1,2,3}$.
Thus mutating~$B$ at~$k$ amounts to replacing $B_{\ell\ell}$ by $\mu_k(B_{\ell\ell})$.
Mutating $B_{\ell\ell}$ at $k$ commutes with applying a positive scaling to a column of $B_{\ell\ell}$ other than~$k$.
Thus $\Comp(\mu_k(B))=\mu_k(\Comp(B))$ and ${\CompPlus(\mu_k(B))=\mu_k(\CompPlus(B))}$.
Since $B$ has entries zero in affine entries of column $k$, the affine rows of $\CompPlus(B)$ are unchanged by mutation in position~$k$.

\cref{neigh good stuff}.\ref{neigh im ray} says that $\frac12B\delta^B$ is zero in all non-affine entries.
Since $k$ is a non-affine entry, we see that $\eta^{B^T}_k$ fixes $\frac12B\delta^B$ and thus fixes the imaginary ray pointwise.
Since $k$ is non-special, the non-affine entries of column $k$ of $B$ agree with column $k$ of $\Comp(B)$.
Thus if $x$ is a vector whose affine entries are zero (so that $\eta^{\Comp(B)^T}_k(x)$ makes sense) we have $\eta^{B^T}_k(x)=\eta^{\Comp(B)^T}_k(x)$.
\end{proof}

\begin{proposition}\label{special mut}
Suppose $B$ is a neighboring exchange matrix and $k$ is a special index of~$B$.
Then there exists a sequence $\kk$ of indices in $\set{k,n-1,n}$ such that 
\begin{itemize}
\item
$\mu_\kk(B)$ is neighboring,
\item
$\Comp(\mu_\kk(B))=\mu_k(\Comp(B))$,
\item
$\CompPlus(\mu_\kk(B))$ does not equal $\mu_k(\CompPlus(B))$, but $\CompPlus(\mu_\kk(B))$ agrees with $\CompPlus(B)$ in the affine rows,
\item
the mutation map $\eta^{B^T}_\kk$ fixes the imaginary ray pointwise, and 
\item
If $x$ is a vector whose affine entries are zero, then $\eta^{B^T}_\kk(x)=\eta^{\Comp(B)^T}_k(x)$.
\end{itemize}
\end{proposition}
\begin{proof}
The special index $k$, together with the affine indices, determines a $3\times3$ submatrix of $B$ that agrees with one of the entries in \cref{submat tab}.
For convenience in the rest of the proof, we call this $3\times3$ submatrix merely ``the submatrix'' and refer to the labels in the table as ``the type'' of the submatrix.
We argue separately for the different cases in the table, but are able to combine some cases that are related by scaling.

\medskip

\noindent
\textbf{Case 1.}
The submatrix is of type $A_2^{(1)}$.
The sequence $\kk$ is $k(n-1)knk$.
Recalling that mutations in the sequence are applied from right to left, mutation by this sequence acts on the submatrix as
%\[
%\begin{bsmallmatrix*}[r]
%0&1&-1\\
%-1&0&2\\
%1&-2&0
%\end{bsmallmatrix*}
%\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-1&\,\,\,1\\
%1&0&1\\
%-1&-1&0
%\end{bsmallmatrix*}
%\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&\,\,\,1&1\\
%-1&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}
%\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&2&-1\\
%-2&0&1\\
%1&-1&0
%\end{bsmallmatrix*}.
%\]
\begin{multline*}
\begin{bsmallmatrix*}[r]
0&1&-1\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,1\\
1&0&1\\
-1&-1&0
\end{bsmallmatrix*}
\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,-1\\
1&0&-1\\
1&1&0
\end{bsmallmatrix*}\\
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&1&\,\,\,1\\
-1&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,1\\
1&0&1\\
-1&-1&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&1&\,\,\,-1\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}.
\end{multline*}
In particular, $\mu_\kk(B)$ restricts, in its affine indices, to an affine exchange matrix of rank~$2$,
so \cref{neigh B} implies that it is neighboring.

To compute the remaining entries of $\mu_\kk(B)$, we consider each $4\times4$ submatrix whose rows are indexed by $i,k,n-1,n$ and whose columns are indexed by $j,k,n-1,n$, for arbitrary $i,j\not\in\set{k,n-1,n}$.
%To make the notation more compact, we use $p$ to stand for $n-1$.
%We use the notation 
%\[m(a,b)=\begin{cases}
%ab&\text{if }\sgn(a)=\sgn(b)=+\\
%-ab&\text{if }\sgn(a)=\sgn(b)=-\\
%0&\text{otherwise},
%\end{cases}\]
We continue the notation $m(a,b)$ from the proof of \cref{neigh good stuff} and we will use the identity $m(a,b)+m(a,-b)=ab$ for $b>0$ several times.
We will also use the facts (from \cref{neigh B}) that $b_{(n-1)j}=-b_{nj}\le0$, and $b_{i(n-1)}=-b_{in}\ge0$.
\begin{align*}
\begin{bsmallmatrix*}[r]
b_{ij}&b_{ik}&-b_{in}&b_{in}\\
b_{kj}&0&1&-1\\
-b_{nj}&-1&0&2\\
b_{nj}&1&-2&0
\end{bsmallmatrix*}
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & -b_{ik} & -b_{in}+m(b_{ik},1) & b_{in}+m(b_{ik},-1)\\
-b_{kj} & 0 & -1 & 1\\
-b_{nj}+m(b_{kj},-1) & 1 & 0 & 1\\
b_{nj}+m(b_{kj},1) & -1 & -1 & 0
\end{bsmallmatrix*}\\
&\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & b_{in}-m(b_{ik},1) & b_{ik} & -b_{in}-m(b_{ik},-1)\\
b_{nj}-m(b_{kj},-1) & 0 & -1 & -1\\
b_{kj} & 1 & 0 & -1\\
-b_{nj}-m(b_{kj},1) & 1 & 1 & 0
\end{bsmallmatrix*}\\
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & -b_{in}+m(b_{ik},1) & b_{in}+m(b_{ik},-1) & -b_{ik}\\
-b_{nj}+m(b_{kj},-1) & 0 & 1 & 1\\
b_{nj}+m(b_{kj},1)  & -1 & 0 & -1\\
-b_{kj}& -1 & 1 & 0
\end{bsmallmatrix*}\\
&\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & b_{ik} & -b_{in}-m(b_{ik},-1) & b_{in}-m(b_{ik},1)\\
b_{kj}& 0 & -1 & 1\\
-b_{nj}-m(b_{kj},1)& 1 & 0 & 1\\
b_{nj}-m(b_{kj},-1) & -1 & -1 & 0
\end{bsmallmatrix*}\\
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+2m(b_{ik},b_{kj}) & -b_{ik} & -b_{in} & b_{in}\\
-b_{kj} & 0 & 1 & -1\\
-b_{nj} & -1 & 0 & 2\\
b_{nj} & 1 & -2 & 0
\end{bsmallmatrix*}.
\end{align*}

%ASIDE:
%Trying this with fewer assumptions, specifically ditching the assumption that $b_{(n-1)j}=-b_{nj}\le0$ and using $p$ to stand for $n-1$.
%UPDATE:  It's awful, because you don't know what sign anything is...  
%ONLY CHANGED the first two matrices.
%\begin{align*}
%\begin{bsmallmatrix*}[r]
%b_{ij}&b_{ik}&b_{ip}&b_{in}\\
%b_{kj}&0&1&-1\\
%-b_{nj}&-1&0&2\\
%b_{nj}&1&-2&0
%\end{bsmallmatrix*}
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+m(b_{ik},b_{kj}) & -b_{ik} & b_{ip}+m(b_{ik},1) & b_{in}+m(b_{ik},-1)\\
%-b_{kj} & 0 & -1 & 1\\
%-b_{nj}+m(b_{kj},-1) & 1 & 0 & 1\\
%b_{nj}+m(b_{kj},1) & -1 & -1 & 0
%\end{bsmallmatrix*}\\
%&\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+m(b_{ik},b_{kj}) & b_{in}-m(b_{ik},1) & b_{ik} & -b_{in}-m(b_{ik},-1)\\
%b_{nj}-m(b_{kj},-1) & 0 & -1 & -1\\
%b_{kj} & 1 & 0 & -1\\
%-b_{nj}-m(b_{kj},1) & 1 & 1 & 0
%\end{bsmallmatrix*}\\
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+m(b_{ik},b_{kj}) & -b_{in}+m(b_{ik},1) & b_{in}+m(b_{ik},-1) & -b_{ik}\\
%-b_{nj}+m(b_{kj},-1) & 0 & 1 & 1\\
%b_{nj}+m(b_{kj},1)  & -1 & 0 & -1\\
%-b_{kj}& -1 & 1 & 0
%\end{bsmallmatrix*}\\
%&\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+m(b_{ik},b_{kj}) & b_{ik} & -b_{in}-m(b_{ik},-1) & b_{in}-m(b_{ik},1)\\
%b_{kj}& 0 & -1 & 1\\
%-b_{nj}-m(b_{kj},1)& 1 & 0 & 1\\
%b_{nj}-m(b_{kj},-1) & -1 & -1 & 0
%\end{bsmallmatrix*}\\
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%b_{ij}+2m(b_{ik},b_{kj}) & -b_{ik} & -b_{in} & b_{in}\\
%-b_{kj} & 0 & 1 & -1\\
%-b_{nj} & -1 & 0 & 2\\
%b_{nj} & 1 & -2 & 0
%\end{bsmallmatrix*}.
%\end{align*}


%\begin{align*}
%\begin{bsmallmatrix*}[r]
%a_{ij}&x_i&0&0\\
%y_j&0&1&-1\\
%0&-1&0&2\\
%0&1&-2&0
%\end{bsmallmatrix*}
%&\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j) & -x_i & m(x_i,1) & m(x_i,-1)\\
%-y_j & 0 & -1 & 1\\
%m(y_j,-1) & 1 & 0 & 1\\
%m(y_j,1) & -1 & -1 & 0
%\end{bsmallmatrix*}\\
%&\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j) & -x_i+m(x_i,-1) & x_i & -m(x_i,-1)\\
%-y_j+m(y_j,1) & 0 & -1 & -1\\
%y_j & 1 & 0 & -1\\
%-m(y_j,1) & 1 & 1 & 0
%\end{bsmallmatrix*}\\
%&\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j) & x_i-m(x_i,-1) & m(x_i,-1) & -x_i+m(x_i,-1)-m(x_i,-1)\\
%y_j-m(y_j,1) & 0 & 1 & 1\\
%m(y_j,1) & -1 & 0 & -1\\
%-y_j+m(y_j,1)-m(y_j,1) & -1 & 1 & 0
%\end{bsmallmatrix*}\\
%&\overset{2}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j) & x_i & -m(x_i,-1) & -x_i+m(x_i,-1)\\
%y_j & 0 & -1 & 1\\
%-m(y_j,1) & 1 & 0 & 1\\
%-y_j+m(y_j,1) & -1 & -1 & 0
%\end{bsmallmatrix*}\\
%&\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+2m(x_i,y_j) & -x_i & 0 & 0\\
%-y_j & 0 & 1 & -1\\
%0 & -1 & 0 & 2\\
%0 & 1 & -2 & 0
%\end{bsmallmatrix*}\\
%\end{align*}
%where we observe in the middle mutation in direction 1 that $-x_i+m(x_i,-1)$ is always non-positive and $-y_j+m(y_j,1)$ is always non-negative.

It is much easier to find the $ij$-entry of $\mu_k(\Comp(B))$.
We compute
\[
\begin{bsmallmatrix*}
b_{ij}&2b_{ik}\\
b_{kj}&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(2b_{ik},b_{kj})&-2b_{ik}\\
-b_{kj}&0
\end{bsmallmatrix*}
\]
and we conclude that $\Comp(\mu_\kk(B))=\mu_k(\Comp(B))$.
We see also that all entries in affine rows are preserved by $\mu_\kk$.
Thus $\CompPlus(\mu_\kk(B))$ agrees with $\CompPlus(B)$ in the affine rows.
However, $\CompPlus(\mu_\kk(B))\neq\mu_k(\CompPlus(B))$, because the latter disagrees with $\CompPlus(B)$ in the $k\th$ entries of the affine rows (differing by a sign).

The computations above can be reused to prove the assertions about $\eta^{B^T}_\kk$.
%also let us determine mutation maps by taking $j$ to index a new column that has been used to extend $B$ but still taking $i$ to be a non-affine index in~$\set{1,\ldots,n}$.
\cref{neigh good stuff}.\ref{neigh im ray} says that the vector $-\frac12B\delta^B$ that spans the imaginary ray is zero in non-affine entries and has affine entries $-1,1$.
This, the computations above are valid, replacing the $i\th$ column of $B$ in the calculations with $-\frac12B\delta^B$.
Specifically, we replace $b_{ij}$ and $b_{kj}$ with $0$ and replace $b_{nj}$ with $1$, and the computations show that $\eta^{B^T}_\kk$ fixes $-\frac12B\delta^B$.

Similarly, if $x$ is a vector whose affine entries are zero, it can take the place of the $j\th$ column of $B$ in the above calculations, and will satisfy the requirements $b_{(n-1)j}=-b_{nj}\le0$, and $b_{i(n-1)}=-b_{in}\ge0$ that were used in the calculation.  
Therefore, we see that $\eta^{B^T}_\kk(x)=\eta^{\Comp(B)^T}_k(x)$.
We have proved the \lcnamecref{special mut} in this case.

\noindent
\textbf{Case 2.}
The submatrix is of type $C_2^{(1)}$ or $A_4^{(2)}$.
We first consider the case of $C_2^{(1)}$.
The sequence $\kk$ is again $k(n-1)knk$, and mutation acts on the submatrix as shown here:
%Since $\mu_k(DBD^{-1})=D\mu_k(B)D^{-1}$ for any positive diagonal matrix $D$, it suffices to only consider the cases $C_2^{(1)}$, which implies both $A_4^{(2)}$ cases. % and $D_3^{(2)}$, and $G_2^{(1)}$, which implies $D_4^{(3)}$.
%(Let $B'=DBD^{-1}$, then $E'=DED^{-1}$ and $F'=DFD^{-1}$ so that $D(EBF)D^{-1}=E'B'F'$.)
\begin{multline*}
\begin{bsmallmatrix*}[r]
0&2&-2\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-2&\,\,\,2\\
1&0&0\\
-1&0&0
\end{bsmallmatrix*}
\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-2&\,\,\,-2\\
1&0&0\\
1&0&0
\end{bsmallmatrix*}\\
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&2&\,\,\,2\\
-1&0&0\\
-1&0&0
\end{bsmallmatrix*}
\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-2&\,\,\,2\\
1&0&0\\
-1&0&0
\end{bsmallmatrix*}
\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&2&-2\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}.
\end{multline*}
Again, \cref{neigh B} implies that $\mu_\kk(B)$ is neighboring.

We again compute the remaining entries of $\mu_\kk(B)$ by considering each $4\times4$ submatrix with rows indexed by $i,k,n-1,n$ and columns indexed by $j,k,n-1,n$, for arbitrary $i,j\not\in\set{k,n-1,n}$.
Again, we have $b_{(n-1)j}=-b_{nj}\le0$, and $b_{i(n-1)}=-b_{in}\ge0$.
We again use the identity $m(a,b)+m(a,-b)=ab$ for $b\ge0$ and now also use the identity $m(a,b)-m(a,-b)=|a|b$ for $b\ge0$ and $m(a,pb)=pm(a,b)$ for $p\ge0$.
\begin{align*}
\begin{bsmallmatrix*}[r]
b_{ij}&b_{ik}&-b_{in}&b_{in}\\
b_{kj}&0&2&-2\\
-b_{nj}&-1&0&2\\
b_{nj}&1&-2&0
\end{bsmallmatrix*}
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & -b_{ik} & -b_{in}+m(b_{ik},2) & b_{in}+m(b_{ik},-2)\\
-b_{kj} & 0 & -2 & 2\\
-b_{nj}+m(b_{kj},-1) & 1 & 0 & 0\\
b_{nj}+m(b_{kj},1) & -1 & 0 & 0
\end{bsmallmatrix*}\\
&\overset{n}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & b_{in}-|b_{ik}| & -b_{in}+m(b_{ik},2) & -b_{in}-m(b_{ik},-2)\\
2b_{nj}+|b_{kj}| & 0 & -2 & -2\\
-b_{nj}+m(b_{kj},-1) & 1 & 0 & 0\\
-b_{nj}-m(b_{kj},1) & 1 & 0 & 0
\end{bsmallmatrix*}\\
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & -b_{in}+|b_{ik}| & b_{in}+m(b_{ik},-2) & b_{in}-m(b_{ik},2)\\
-2b_{nj}+|b_{kj}| & 0 & 2 & 2\\
b_{nj}+m(b_{kj},1)  & -1 & 0 & 0\\
b_{nj}-m(b_{kj},-1)& -1 & 0 & 0
\end{bsmallmatrix*}\\
&\overset{n-1}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+m(b_{ik},b_{kj}) & b_{ik} & -b_{in}-m(b_{ik},-2) & b_{in}-m(b_{ik},2)\\
b_{kj}& 0 & -2 & 2\\
-b_{nj}-m(b_{kj},1)& 1 & 0 & 0\\
b_{nj}-m(b_{kj},-1) & -1 & 0 & 0
\end{bsmallmatrix*}\\
&\overset{k}{\longrightarrow}
\begin{bsmallmatrix*}
b_{ij}+2m(b_{ik},b_{kj}) & -b_{ik} & -b_{in} & b_{in}\\
-b_{kj} & 0 & 2 & -2\\
-b_{nj} & -1 & 0 & 2\\
b_{nj} & 1 & -2 & 0
\end{bsmallmatrix*}.
\end{align*}

%\begin{align*}
%\begin{bsmallmatrix*}[r]
%b_{ij}&b_{ik}&-b_{in}&b_{in}\\
%b_{kj}&0&2&-2\\
%-b_{nj}&-1&0&2\\
%b_{nj}&1&-2&0
%\end{bsmallmatrix*}
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j)&-x_i&m(x_i,2)&m(x_i,-2)\\
%-y_j&0&-2&2\\
%m(y_j,-1)&1&0&0\\
%m(y_j,1)&-1&0&0
%\end{bsmallmatrix*}\\
%&\overset{n}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j)&-x_i+m(x_i,-2)&m(x_i,2)&-m(x_i,-2)\\
%-y_j+2m(y_j,1)&0&-2&-2\\
%m(y_j,-1)&1&0&0\\
%-m(y_j,1)&1&0&0
%\end{bsmallmatrix*}\\
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j)&x_i-m(x_i,-2)&m(x_i,-2)&-2x_i+m(x_i,-2)\\
%y_j-2m(y_j,1)&0&2&2\\
%m(y_j,1)&-1&0&0\\
%-y_j+m(y_j,1)&-1&0&0
%\end{bsmallmatrix*}\\
%&\overset{n-1}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+m(x_i,y_j)&x_i&-m(x_i,-2)&-2x_i+m(x_i,-2)\\
%y_j&0&-2&2\\
%-m(y_j,1)&1&0&0\\
%-y_j+m(y_j,1)&-1&0&0
%\end{bsmallmatrix*}\\
%&\overset{k}{\longrightarrow}
%\begin{bsmallmatrix*}
%a_{ij}+2m(x_i,y_j)&-x_i&0&0\\
%-y_j&0&2&-2\\
%0&-1&0&2\\
%0&1&-2&0
%\end{bsmallmatrix*}
%\end{align*}
%where we observe in the middle mutation in direction 1 that $-x_i+m(x_i,-2)$ is always non-positive and $-y_j+2m(y_j,1)$ is always non-negative.

The computation of the $ij$-entry of $\mu_k(\Comp(B))$ is the same as in Case~1, and we again conclude that $\Comp(\mu_\kk(B))=\mu_k(\Comp(B))$.
The assertions about $\CompPlus(\mu_\kk(B))$ and about $\eta^{B^T}_\kk$ are proved exactly as in Case~1.
We have proved the \lcnamecref{special mut} in the case of type $C_2^{(1)}$.

The matrices of type $A_4^{(2)}$ shown in \cref{submat tab} are obtained from the matrix of type $C_1{(1)}$ by conjugating by a diagonal matrix (with diagonal entries either $1,2,1$ or $1,1,2$).
Since $\mu_\kk(D^{-1}BD)=D^{-1}\mu_\kk(B)D$ for any positive diagonal matrix~$D$ \cite[Proposition~4.5]{ca1}, we can reuse the computations above, conjugated by a diagonal matrix, to obtain the \lcnamecref{special mut} in the cases of type $A_4^{(2)}$.

\noindent
\textbf{Case 3.}
The submatrix is of type  $G_2^{(1)}$ or $D_4^{(3)}$.
In these cases, the associated root system has roots whose squared lengths are related by a factor of $3$.
The classification of affine root systems thus implies that $n=3$, so that $B$ in fact equals one of the submatrices shown in \cref{submat tab}.
For that reason, the computation is smaller in this case, and the indices $k,(n-1),n$ are $1,2,3$.

We begin with type $G_2^{(1)}$.
%The sequence $\kk$ is $132131$, and the mutation acts on $B$ as shown here.
%\begin{multline*}
%\begin{bsmallmatrix*}[r]
%0&3&-3\\
%-1&0&2\\
%1&-2&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-3&3\\
%1&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}\\
%\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&1\\
%1&-1&0
%\end{bsmallmatrix*}
%\overset{2}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&-1\\
%1&1&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&3\\
%0&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}\\
%\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&3&-3\\
%-1&0&1\\
%1&-1&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-3&3\\
%1&0&-2\\
%-1&2&0
%\end{bsmallmatrix*}.
%\end{multline*}
%The sequence $\kk$ is $13121231$, and the mutation acts on $B$ as shown here.
%\begin{multline*}
%\begin{bsmallmatrix*}[r]
%0&3&-3\\
%-1&0&2\\
%1&-2&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-3&3\\
%1&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}
%\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&1\\
%1&-1&0
%\end{bsmallmatrix*}\\
%\overset{2}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&-1\\
%1&1&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&3\\
%0&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}
%\overset{2}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&3\\
%0&0&1\\
%-1&-1&0
%\end{bsmallmatrix*}\\
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&0&-3\\
%0&0&1\\
%1&-1&0
%\end{bsmallmatrix*}
%\overset{3}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&-3&3\\
%1&0&-1\\
%-1&1&0
%\end{bsmallmatrix*}
%\overset{1}{\longrightarrow}
%\begin{bsmallmatrix*}[r]
%0&3&-3\\
%-1&0&2\\
%1&-2&0
%\end{bsmallmatrix*}.
%\end{multline*}
The sequence $\kk$ is $1313231$, and the mutation acts on $B$ as shown here.
\begin{multline*}
\begin{bsmallmatrix*}[r]
0&3&-3\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-3&3\\
1&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&-3\\
0&0&1\\
1&-1&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&3\\
0&0&1\\
-1&-1&0
\end{bsmallmatrix*}\\
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&-3\\
0&0&-1\\
1&1&0
\end{bsmallmatrix*}
\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&-3\\
0&0&1\\
1&-1&0
\end{bsmallmatrix*}
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-3&3\\
1&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&3&-3\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}.
\end{multline*}
Once again, \cref{neigh B} implies that $\mu_\kk(B)$ is neighboring.

There are no additional entries of $\mu_\kk(B)$ to compute, and $\Comp(B)$ is $[0]$, so we see that $\Comp(\mu_\kk(B))=\mu_k(\Comp(B))$.
The assertions about $\CompPlus(\mu_\kk(B))$ as proved as in previous cases.

To prove the assertions about mutation maps, we consider adding an extra column with entries $c_1,c_2,c_3$ with $c_3=-c_2\ge0$.
\begin{multline*}
\begin{bsmallmatrix*}
0&3&-3&c_1\\
-1&0&2&c_2\\
1&-2&0&-c_2
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
0&-3&3&-c_1\\
1&0&-1&c_2+m(c_1,-1)\\
-1&1&0&-c_2+m(c_1,1)
\end{bsmallmatrix*}\\
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
0&0&-3&-3c_2+|c_1|+m(c_1,1)\\
0&0&1&c_2+m(c_1,-1)\\
1&-1&0&c_2-m(c_1,1)
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
0&0&3&3c_2-|c_1|-m(c_1,1)\\
0&0&1&c_2+m(c_1,-1)\\
-1&-1&0&-2c_2+|c_1|
\end{bsmallmatrix*}\\
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
0&0&-3&-3c_2+2|c_1|-m(c_1,1)\\
0&0&-1&-c_2+m(c_1,1)\\
1&1&0&2c_2-|c_1|
\end{bsmallmatrix*}
\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}
0&0&-3&-3c_2+2|c_1|-m(c_1,1)\\
0&0&1&c_2-m(c_1,1)\\
1&-1&0&c_2+m(c_1,-1)
\end{bsmallmatrix*}\\
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
0&-3&3&c_1\\
1&0&-1&c_2-m(c_1,1)\\
-1&1&0&-c_2-m(c_1,-1)
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
0&3&-3&-c_1\\
-1&0&2&c_2\\
1&-2&0&-c_2
\end{bsmallmatrix*}.
\end{multline*}
The imaginary ray is spanned by the vector with entries $c_1=0$, $c_2=-1$, $c_3=1$, and we see that $\eta^{B^T}_\kk$ fixes that vector.
We also see that if $x$ is a vector with entries $c_1,0,0$, then $\eta_\kk^{B^T}(x)$ has entries $-c_1,0,0$.
We compute $\eta_1^{\Comp(B)^T}$ by mutating $[0\,\,c_1]\overset{1}{\longrightarrow}[0\,\,-c_1]$, so $\eta^{B^T}_\kk(x)=\eta^{\Comp(B)^T}_k(x)$ as desired.

The matrix of type $D_4^{(3)}$ in \cref{submat tab} is obtained from the matrix of type $G_2{(1)}$ by conjugating by a diagonal matrix with diagonal entries $3,1,1$.
Since $\mu_\kk(D^{-1}BD)=D^{-1}\mu_\kk(B)D$ for any positive diagonal matrix \cite[Proposition~4.5]{ca1}, we can reuse the computations above, conjugated by a diagonal matrix, to obtain the \lcnamecref{special mut} in the case of type $D_4^{(3)}$.
\end{proof}

Given a  is a sequence $\kk$ of non-affine indices, we define an \newword{expanded sequence}~$\ll$ where each \emph{special} index in $\kk$ is replaced with the corresponding sequence whose existence is proved in \cref{special mut}.
\cref{nonspecial mut,special mut} combine to say that 
\begin{itemize}
\item
$\Comp(\mu_\ll(B))=\mu_\kk(\Comp(B))$,
\item
the mutation map $\eta^{B^T}_\ll$ fixes the imaginary ray pointwise, and 
\item
If $x$ is a vector whose affine entries are zero, then $\eta^{B^T}_\ll(x)=\eta^{\Comp(B)^T}_\kk(x)$.
\end{itemize}
(The fact, in each of \cref{nonspecial mut,special mut}, that the mutated matrix is neighboring is crucial to concluding these facts for $\ll$.)

\begin{proposition}\label{neigh im wall}
Suppose $B$ is neighboring and is indexed as in \cref{neigh B}.
Then $\d^B_\infty$ is the half-hyperplane contained in $(\delta^B)^\perp$, containing the vector $-\frac12B\delta^B$, with relative boundary the codimension-$2$ space consisting of vectors that are zero in the affine indices.
\end{proposition}
\begin{proof}
Since $\Comp(B)$ is of finite type, $\F_{\Comp(B)^T}$ is finite and complete.
We consider the mutation fan $\F_{\Comp(B)^T}$ as a complete fan in the subspace of $\reals^n$ consisting of vectors whose affine entries are zero. 
(We refer to this subspace as $\reals^{n-2}$.)
Every maximal cone $U$ of $\F_{\Comp(B)^T}$ is the image of the positive cone in $\reals^{n-2}$ under some mutation map $\eta^{(B')^T}_{\kk^{-1}}$, where $B'=\mu_\kk(\Comp(B))$ and $\kk$ is a sequence of non-affine indices.  
Let $\ll$ be the expanded sequence, so that $B'=\Comp(\mu_\ll(B))$.
There is an imaginary cone in $\F_{\mu_\ll(B)^T}$ that is the nonnegative linear span of the imaginary ray and the positive cone in $\reals^{n-2}$.
Since $\eta^{\mu_\ll(B)^T}_{\ell^{-1}}$ is an isomorphism from $\F_{\mu_\ll(B)^T}$ to $\F_{B^T}$ that fixes the imaginary ray and sends the the positive cone in $\reals^{n-2}$ to $U$, the cone spanned by $U$ and the imaginary ray is an imaginary cone in $\F_{B^T}$.
We see that the imaginary cones in $\F_{B^T}$ fill the half-hyperplane described in the \lcnamecref{neigh im wall}.
Since the imaginary ray is in the relative interior of that half-hyperplane, there are no other imaginary cones.
\end{proof}

\begin{proposition}\label{factor eta}
Suppose $B$ is neighboring and $\lambda\in\d_\infty^B$.
Write $\lambda=\lambda_0+\lambda_\infty$ for $\lambda_0$ with affine entries $0$ and $\lambda_\infty$ a nonnegative scaling of $\delta^B$.
For $\kk$ a sequence of non-affine indices with expanded sequence $\ll$, ${\eta^{B^T}_\ll(\lambda)=\eta_\kk^{\Comp(B)^T}(\lambda_0)+\lambda_0}$.
\end{proposition}
\begin{proof}
\cref{neigh im wall} says that we can write $\lambda$ as $\lambda_0+\lambda_0$.
Since $\eta_\ll^{B^T}$ is linear on the imaginary cone containing $\lambda$, and since that cone contains the imaginary ray~$\delta^B$, the \lcnamecref{factor eta} follows from \cref{nonspecial mut,special mut}.
\end{proof}


We now prove the main theorem.

\begin{proof}[Proof of \cref{affine main}]
Take $\lambda\in\d_\infty^B$, consider any $B'$ mutation-equivalent to $B$, and specifically take any sequence $\ll$ of indices in $\set{1,\ldots,n}$ such that $B'=\mu_\ll(B)$.
Write $\lambda'$ for $\eta_\ll^{B^T}(\lambda)$.
The mutation map $\eta_\ll^{B^T}$ acting on $\F_{B^T}$ takes the imaginary ray of $\F_{B^T}$ to the imaginary ray of $\F_{\mu_\ll(B)^T}$, and more specifically takes the vector $-\frac12B\delta^B$ to the vector $-\frac12\mu_\ll(B)\delta^{\mu_\ll(B)}$.
Furthermore, $\eta_\ll^{B^T}$ is linear on each imaginary cone of $\F_{B^T}$, and thus takes the line segment that \cref{affine main} claims is equal to~$\P_\lambda^B$ to the line segment that is claimed to equal $\P_{\lambda'}^{B'}$.
Now \cref{shift} implies that it is enough to prove the \lcnamecref{affine main} for any one $B$ in the exchange pattern.

We choose $B$ to be a neighboring exchange matrix.
%\cref{affine main partial} says that~$\P_\lambda^B$ contains the line segment described in the \lcnamecref{affine main}.
%We need to show the opposite containment.
Suppose now that $\kk$ is a sequence of \emph{non-affine} indices, let~$\ll$ be the expanded sequence, and again take $B'=\mu_\ll(B)$ by \cref{factor eta}.
Write $\lambda$ as $\lambda_0+\lambda_\infty$, where $\lambda_0$ is zero in the affine indices and $\lambda_\infty$ is a nonnegative scaling of $-\frac12B\delta^B$, so that ${\eta^{B^T}_\ll(\lambda)=\eta_\kk^{\Comp(B)^T}(\lambda_0)+\lambda_0}$.
\cref{Comp span} implies that 
\begin{multline*}
\set{B'\alpha:\alpha\in\reals^n,\alpha\ge0}\cap(\delta^{B'})^\perp\\
\subseteq\set{\CompPlus(B')\alpha+B'\delta^{B'}a:\alpha\in\reals^{n-2},\alpha\ge0,a\ge0}.
\end{multline*}

Let $M$ be the matrix obtained from $\CompPlus(B)$ by replacing the first $n-2$ rows by zeros (leaving only the affine entries).
\cref{nonspecial mut,special mut} imply that $M$ is also the matrix obtained from $\CompPlus(B')$ by replacing the first $n-2$ rows by zeros.
\cref{neigh B,delta is the man} imply that every column of $M$ is parallel to~$B\delta^B$.
In the following formula, we will think of the columns of $\Comp(B')$ as vectors in $\reals^n$ with entries $0$ in the affine positions.

We see that 
\begin{multline*}
\set{\eta_\ll^{B^T}(\lambda)+B'\alpha:\alpha\in\reals^n,\alpha\ge0}\cap\d^{B'}_\infty\\
\begin{aligned}
&\quad\subseteq\set{\eta_\ll^{B^T}(\lambda)+\CompPlus(B')\alpha+B'\delta^{B'}a:\alpha\in\reals^{n-2},\alpha\ge0,a\ge0}\cap\d^{B'}_\infty\\
&\quad=\set{\eta_\kk^{\Comp(B)}(\lambda_0)+\Comp(B')\alpha+\lambda_\infty+M\alpha+B'\delta^{B'}a:\alpha\ge0,a\ge0}\cap\d^{B'}_\infty.\\
\end{aligned}
\end{multline*}
The projection of this set to $\reals^{n-2}$ is $\set{\eta_\kk^{\Comp(B)}(\lambda_0)+\Comp(B')\alpha:\alpha\ge0}$.

The set $\P^B_{\lambda,\ll}$ obtained by applying $(\eta_\ll^{B^T})^{-1}$ to $\set{\eta_\ll^{B^T}(\lambda)+B'\alpha:\alpha\in\reals^n,\alpha\ge0}$.
By \cref{factor eta}, if some point $x$ is $\P^B_{\lambda,\ll}$ for every $\ll$, then the projection of $x$ to $\reals^{n-2}$ is in $\P^{\Comp(B)}_{\lambda_0,\kk}$ for every $\kk$.
Therefore \cref{finite P point}.\ref{finite P point coeff-free} says that the projection of~$x$ is $\lambda_0$.
We have showed that $\P^B_\lambda$ is contained in the line through $\lambda$ in the direction of~$B\delta^B$.
But also $\P^B_\lambda$ is contained in $\d^B_\infty$, and, by considering $\P^B_{\lambda,\emptyset}$, we see that $\P^B_\lambda$ is contained in the set $\set{\lambda+B\delta^Ba:a\ge0}$.
We have shown that $\P^B_\lambda$ is the line segment parallel to the imaginary ray, with one endpoint at $\lambda$ and the other endpoint on the relative boundary of $\d^B_\infty$.
\end{proof}

\subsection{Extended exchange matrices of affine type}
In this section, we extend \cref{affine main} to arbitrary extensions of $B$.  \sayN{All this will be stated earlier if any of it works!}

Recall that $\Proj_n$ is the projection from $\reals^m$ to $\reals^n$ that ignores the last $m-n$ coordinates.
Recall also that the mutation fan for $\tB^T$ is the set $\F_{\tB^T}$ of cones $\Proj_n^{-1}C$ such that $C$ is a cone in the mutation fan $\F_{B^T}$.
If $B$ is of affine type, then the imaginary wall $\d_\infty^B$ is a union of cones of $\F_{B^T}$.
Accordingly, we define $\d_\infty^\tB$ to be $\Proj_n^{-1}\d_\infty^B$.

\begin{theorem}\label{affine main extended}
Suppose $B$ is an exchange matrix of affine type and $\tB$ is an extension of $B$.
If $\lambda$ is contained in~$\d^\tB_\infty$, then the dominance region $\P^\tB_\lambda$ is the line segment parallel to $\tB\delta^B$, with one endpoint at $\lambda$ and the other endpoint on the relative boundary of $\d^\tB_\infty$.
\end{theorem}


The imaginary ray in $\F_{B^T}$, spanned by $-\frac12B\delta^B$, corresponds to a $(1+m-n)$-dimensional cone in $\F_{\tB^T}$.
The statement of \cref{affine main extended} suggests that there is a special direction within that $(1+m-n)$-dimensional cone, spanned by $-\frac12\tB\delta^B$.
As a first step in the proof of \cref{affine main extended}, we point out one thing that is special about that direction, namely how it behaves under mutation maps.

\begin{proposition}\label{delta is the tilde man}
Suppose $B$ is an exchange matrix of affine type and $\tB$ is an extension of $B$.
Then $\eta^{\BB^T}_\kk(-\frac12\tB\delta^B)=-\frac12\mu_\kk(\tB)\delta^{\mu_\kk(B)}$ for any sequence $\kk$ of indices in $\set{1,\ldots,n}$.
\end{proposition}
The factors $\frac12$ in the statement are for aesthetic reasons, but the negative signs are crucial.
The analogous statement for $-\frac12B\delta^B$ is true because $-\frac12B\delta^B$ is the shortest integer vector in the imaginary ray.

\begin{proof}
It is enough to prove the \lcnamecref{delta is the tilde man} for $\kk$ a singleton sequence~$k$.


Recall that, for $B_0$ an acyclic exchange matrix of affine type and for a particular choice of $t$, the vector $\delta^B\in V$ is defined to be $\bigl(G_t^{-B_0^T;t_0}\bigr)^T\delta$.
Thus \cref{BGCB ext} rewrites $-\tB\delta^B=-\tB\bigl(G_t^{-B_0^T;t_0}\bigr)^T\delta$ as 
\[-\tB\delta^B=-\bigl(\CC_t^{-\BB_0^T;t_0}\bigr)^T\tB_0\delta=\begin{bsmallmatrix}-(C_t^{-B^T;t_0})^T&0\\-(D_t^{-\tB^T;t_0})^T&-I_{m-n}\end{bsmallmatrix}\tB_0\delta,\]
where the notation $-\tB^T$ means the first $n$ columns of $-\BB^T$: an extended exchange matrix whose first $n$ rows are $-B^T$ and whose remaining rows agree with~$\tB$.  \sayN{Or, of we can get away with it, do a star.}

****

This HAS TO be true (possibly with more hypotheses) if we're going to prove that the dominance region is a line segment.
So I will continue on as if it were proved.

****

\end{proof}

In light of \cref{delta is the tilde man}, we can prove one containment of \cref{affine main extended} (similarly to \cref{affine main partial}).

\begin{proposition}\label{affine main extended partial}
Suppose $B$ is an exchange matrix of affine type and $\tB$ is an extension of $B$.
If $\lambda$ is contained in the imaginary wall~$\d^\tB_\infty$, then the dominance region $\P^\tB_\lambda$ \emph{contains} the line segment parallel to $\tB\delta^B$, with one endpoint at $\lambda$ and the other endpoint on the relative boundary of $\d^\tB_\infty$.
\end{proposition}
\begin{proof}
Suppose $\kk$ is a sequence of indices in $\set{1,\ldots,n}$ and let $\tB'=\mu_\kk(B)$.
Since $(\eta_\kk^{\BB^T})^{-1}$ is linear on each imaginary cone and by \cref{delta is the tilde man}, we see that 
\begin{multline*}
\P^\tB_{\lambda,\kk}=(\eta_\kk^{\BB^T})^{-1}\set{\eta^{\BB^T}_\kk(\lambda)+\tB'\alpha:\alpha\ge0}\\
\supseteq(\eta_\kk^{\BB^T})^{-1}\set{\eta^{\BB^T}_\kk(\lambda)+a\tB'\delta^{B'}:a\ge0}\cap\d^\tB_\infty\\
=\set{\lambda+a\tB\delta^B:a\ge0}\cap\d^\tB_\infty. 
\end{multline*}
This is true for all $\kk$, so $\P^\tB_\lambda\supseteq\set{\lambda+a\tB\delta^B:a\ge0}\cap\d^\tB_\infty$, as desired.
\end{proof}

\begin{proposition}\label{lots of kernels}
Suppose $B$ is an exchange matrix of affine type and $\tB$ is an extension of $B$.
If $\lambda\in\d_\infty^\tB$, then 
\[\P^\tB_{\lambda}\subseteq\Bigl(\set{\lambda+\tB\delta^Ba:a\ge0}\cap\d_\infty^\tB\Bigr)+\bigcap_{\tB'}\sett{\tB'\beta:\beta\in\ker(B')},\]
where the intersection is over all $\tB'$ that obtained from $\tB$ by a sequence of mutations and each $B'$ is the exchange matrix that is the top of $\tB'$.
\end{proposition}
\begin{proof}
%%Let $U$ be the set of vectors in $\reals^m$ that project to the line segment in \cref{affine main}.
%%\cref{contains proj} combines with \cref{affine main} to say that $\P^\tB_\lambda\subseteq U$.
%%In particular, $\P^\tB_\lambda$ is contained in $\d_\infty^\tB$, and more specifically, $\P^\tB_\lambda$ is contained in a cone of $\F_{\tB^T}$.
%%Suppose $x\in\P^\tB_\lambda$.
%%Since $\P^\tB_\lambda\subseteq U$, for some $a\ge0$, $x$ is $\lambda+\tB\delta^Ba$ plus a vector that is zero in the first $n$ entries.
%%Since $\P^\tB_\lambda\subseteq\P^\tB_{\lambda,\emptyset}$, also $x=\lambda+\tB\alpha$ for some ${\alpha\ge0}$.
%%Thus $x-(\lambda+\tB\delta^Ba)$ is $\tB(\alpha-\delta^B)$ and $\alpha-\delta^B\in\ker B$.
%%We conclude that $\P^\tB_\lambda$ is contained in $\set{\lambda+\tB\delta^Ba+\tB\beta:a\ge0,\beta\in\ker B}$.
%%
%If $\tB'$ is mutation-equivalent to $\tB$, then there exists a sequence $\kk$ of indices in $\set{1,\ldots,n}$ with $\tB'=\mu_\kk(\tB)$ and therefore $B'=\mu_\kk(B)$.
%Let $\lambda'=\eta_\kk^{\BB^T}(\lambda)$.
%\cref{shift extended}.\ref{shift extended all} says that $\P^{\tB'}_{\lambda'}=\eta^{\BB^T}_\kk\!\!(\P^\tB_\lambda)$.
%Suppose $x\in\P^{\tB'}_{\lambda'}$.
%Then \cref{contains proj} combines with \cref{affine main} to say that, for some $a\ge0$, $x$ is $\lambda'+\tB'\delta^{B'}a$ plus a vector that is zero in the first $n$ entries.
%Also, since $\P^{\tB'}_{\lambda'}\subseteq\P^{\tB'}_{\lambda',\emptyset}$, also $x=\lambda'+\tB'\alpha$ for some ${\alpha\ge0}$.
%Thus $x-(\lambda'+\tB'\delta^{B'}a)$ is $\tB'(\alpha-a\delta^{B'})$ and $\alpha-a\delta^{B'}\in\ker(B')$.
%We conclude that $\P^{\tB'}_{\lambda'}$ is contained in $\set{\lambda'+\tB'\delta^{B'}a+\tB'\beta:a\ge0,\beta\in\ker(B')}$.
%
%Since this set is contained in a cone of the mutation fan for $\tB'$, the map $\bigl(\eta_\kk^{\BB^T}\bigr)^{-1}=\eta_{\kk^{-1}}^{(\BB')^T}$ is linear on it.
%Applying \cref{delta is the tilde man} and observing that $\eta_{\kk^{-1}}^{(\BB')^T}$ fixes vectors that are zero in the first $n$ entries, we see that
%\[\P^\tB_\lambda\subseteq\sett{\lambda+\tB\delta^Ba+\tB'\beta:a\ge0,\beta\in\ker(B')}.\]
%Thus
%\begin{multline*}
%\P^\tB_{\lambda}\subseteq\bigcap_{\tB'}\sett{\lambda+\tB\delta^Ba+\tB'\beta:a\ge0,\beta\in\ker(B')}\\=\sett{\lambda+\tB\delta^Ba:a\ge0}+\bigcap_{\tB'}\sett{\tB'\beta:\beta\in\ker(B')},
%\end{multline*}
%where the intersection is over all $\tB'$ that obtained from $\tB$ by a sequence of mutations.
%
%IS THAT LAST EQUALITY REALLY TRUE?
%
%Suppose $x\in\bigcap_{\tB'}\sett{\lambda+\tB\delta^Ba+\tB'\beta:a\ge0,\beta\in\ker(B')}$.
%We know $\delta^B\not\in\ker(B)$ (because $-B\delta^B$ spans the imaginary ray), so $x= $
%
%STARTING AGAIN, with improvements:
%
Let $U$ be the set of vectors in $\reals^m$ that project to the line segment in \cref{affine main} under the map $\Proj_n$ that ignores the last $m-n$ entries.
The set $U$ is contained in $\d_\infty^\tB$, and more specifically, contained in a cone of $\F_{\tB^T}$.
We see that every mutation map is linear on $U$.
\cref{contains proj} combines with \cref{affine main} to say that $\P^\tB_\lambda\subseteq U$, so $\P^\tB_\lambda$ is in a cone of $\F_{\tB^T}$ that is in $\d_\infty^\tB$.

Suppose $x\in\P^\tB_\lambda$.
Then $x\in U$, so for some $a\ge0$, $x$ is a vector $y=\lambda+\tB\delta^Ba$ plus a vector $z$ that is zero in the first $n$ entries.
For any $\tB'$ that is mutation-equivalent to $\tB$, there exists a sequence $\kk$ of indices in $\set{1,\ldots,n}$ with $\tB'=\mu_\kk(\tB)$ and therefore $B'=\mu_\kk(B)$.
Let $\lambda'=\eta_\kk^{\BB^T}(\lambda)$ and let $x'=\eta_\kk^{\BB^T}(x)$.
Applying \cref{delta is the tilde man}, since $\eta_\kk^{\BB^T}$ is linear on $U$, and observing that $\eta_\kk^{\BB^T}$ fixes vectors that are zero in the first $n$ entries, we see that $x'$ is a vector $y'=\lambda'+\tB'\delta^{B'}a$ (for the same $a\ge0$ as before) plus $z$.
\cref{shift extended}.\ref{shift extended all} implies that $x'\in\P^{\tB'}_{\lambda'}$, and since $\P^{\tB'}_{\lambda'}\subseteq\P^{\tB'}_{\lambda',\emptyset}$, we see that $x=\lambda'+\tB'\alpha$ for some ${\alpha\ge0}$.
Thus $z=x'-y'$ is $\tB'(\alpha-a\delta^{B'})$ and since the first $n$ entries of $z$ are zero, $\alpha-a\delta^{B'}$ is in the kernel of~$B'$.
We have written $x$ as a vector $\lambda+\tB\delta^Ba$ for some $a\ge0$ plus a vector $z$ that, for any $\tB'$ mutation-equivalent to $\tB$, is is of the form $\tB'\beta$ for some $\beta\in\ker(B')$.
\end{proof}

In light of \cref{affine main extended partial,lots of kernels}, the following \lcnamecref{int of kernels} completes the proof of \cref{affine main extended}.

\begin{proposition}\label{int of kernels}
Suppose $B$ is an exchange matrix of affine type and $\tB$ is an extension of $B$.
Then ${\bigcap_{\tB'}\sett{\tB'\beta:\beta\in\ker(B')}=\set{0}}$, where the intersection is over $\tB'$ mutation-equivalent to $\tB$.
\end{proposition}
\begin{proof}
Haven't thought about this AT ALL!

But these kernels are $0$, $1$, or $2$-dimensional, so this at least seems possible, maybe with conditions on the extension.
NO, also $3$-dimensional is possible.

Also, Dylan and Salvatore convinced me that every single subspace in this intersection is the same!!!

\end{proof}



\sayN{Note to self:  Fix alignment of all matrices, if necessary using [ and small and array instead of bsmallmatrix.}



% bibliography
\bibliographystyle{plain}
\bibliography{bibliography}
\vspace{-0.175 em}


\end{document}

