%!TEX TS-program =  pdflatex 

%%!TEX TS-program =  arara 
%% arara: pdflatex
%% arara: bibtex

%Changes since the initial arXiv submission are marked:
%SinceArXiv:  

\documentclass{amsart}
\usepackage{array}
\newcolumntype{P}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\usepackage{graphicx,verbatim, amsmath, amssymb, amsthm, amsfonts, epsfig, amsxtra,ifthen,mathtools,epstopdf,caption,enumerate,hhline,bbm,capt-of,longtable}	
\usepackage[bookmarks=true, bookmarksopen=false,%
    colorlinks=true,%
    linkcolor=darkblue,%
    citecolor=darkblue,%
    filecolor=darkblue,%
    menucolor=darkblue,%
%    linktoc=page,%
    linktoc=all,%
    urlcolor=darkblue
]{hyperref}
\usepackage[usenames]{xcolor}
\definecolor{darkblue}{cmyk}{1,0.3,0,0.1}  %blue
\usepackage[capitalize]{cleveref}
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widebar}{0}{mathx}{"73}
\epstopdfsetup{suffix=}
\DeclareGraphicsExtensions{.ps}
\DeclareGraphicsRule{.ps}{pdf}{.pdf}{`ps2pdf -dEPSCrop -dNOSAFER #1 \noexpand\OutputFile}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{thm}[proposition]{Theorem}
\newtheorem{conj}[proposition]{Conjecture}
\newtheorem{phen}{Phenomenon}

\renewcommand\thephen{\Roman{phen}}

\theoremstyle{definition}
\newtheorem{example}[proposition]{Example}
\newtheorem{definition}[proposition]{Definition}
\newtheorem{question}[proposition]{Question}

\theoremstyle{remark}
\newtheorem{remark}[proposition]{Remark}
\newtheorem{problem}[proposition]{Problem}

\numberwithin{equation}{section}


% This is for setting off words we define in a separate typeface.
\newcommand{\newword}[1]{\textbf{\emph{#1}}}

\newcommand{\integers}{\mathbb Z}
\newcommand{\rationals}{\mathbb Q}
\newcommand{\naturals}{\mathbb N}
\newcommand{\reals}{\mathbb R}

\newcommand{\edge}{\,\,\rule[2.7pt]{20pt}{0.5pt}\,\,}

\newcommand{\ep}{\varepsilon}
\newcommand{\thet}{\vartheta}
\newcommand{\col}{\operatorname{col}}
\newcommand{\cut}{\operatorname{cut}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\op}{\operatorname{op}}
\newcommand{\JIrr}{\operatorname{JIrr}}
\newcommand{\ji}{\operatorname{ji}}
\newcommand{\Sh}{\operatorname{Sh}}
\newcommand{\Wall}{\operatorname{Wall}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\cw}{\operatorname{cw}}
\newcommand{\ccw}{\operatorname{ccw}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\vsgn}{\mathbf{sgn}}
\newcommand{\Seed}{\operatorname{Seed}}
%\newcommand{\Sh}{{\mathcal Sh}}
\newcommand{\posspan}{\!\tiny\begin{array}{c}\mathbf{pos}\\\mathbf{span}\end{array}\!\!}
%\newcommand{\posspan}{\underset{\text{span}}{\overset{\text{pos}}{}}}
%\newcommand{\posspan}{\mathbf{span_+}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Int}{\operatorname{Int}}
\newcommand{\Fix}{\operatorname{Fix}}
\newcommand{\Stab}{\operatorname{Stab}}
\newcommand{\geom}{{\operatorname{geom}}}
\newcommand{\mon}{{\operatorname{mon}}}
\newcommand{\Ray}{{\operatorname{Ray}}}
\newcommand{\Ram}{{\operatorname{Ram}}}
\newcommand{\uf}{{\operatorname{uf}}}
\newcommand{\fr}{{\operatorname{fr}}}
\newcommand{\Geom}{{\operatorname{\textbf{Geom}}}}
\newcommand{\gFan}{\g\!\operatorname{Fan}}
\newcommand{\Cg}{\mbox{{\rm Cg}}}
\newcommand{\Con}{\mbox{{\rm Con}}}
\newcommand{\Irr}{\mbox{{\rm Irr}}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\fs}{\mathrm{fs}}
\newcommand{\ufs}{\mathrm{ufs}}
\newcommand{\covers}{{\,\,\,\cdot\!\!\!\! >\,\,}}
\newcommand{\covered}{{\,\,<\!\!\!\!\cdot\,\,\,}}
\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace}}
\newcommand{\pidown}{\pi_\downarrow}
\newcommand{\piup}{\pi^\uparrow}
\newcommand{\br}[1]{{\langle #1 \rangle}}
\newcommand{\brr}[1]{{\bigl\langle #1 \bigr\rangle}}
\newcommand{\brrr}[1]{{\Bigl\langle #1 \Bigr\rangle}}
\newcommand{\brrrr}[1]{{\biggl\langle #1 \biggr\rangle}}
\newcommand{\brrrrr}[1]{{\Biggl\langle #1 \Biggr\rangle}}
\newcommand{\A}{{\mathcal A}}
\newcommand{\GG}{{\mathbf G}}
\newcommand{\CC}{{\mathbf C}}
\newcommand{\EL}{{\mathcal L}}
\newcommand{\F}{{\mathcal F}}
\newcommand{\D}{{\mathfrak D}}
\newcommand{\N}{{\mathcal N}}
\newcommand{\p}{{\mathfrak p}}
\newcommand{\X}{{\mathcal X}}
\newcommand{\W}{{\mathcal W}}
\newcommand{\join}{\vee}
\newcommand{\meet}{\wedge}
\renewcommand{\Join}{\bigvee}
\newcommand{\Meet}{\bigwedge}
\newcommand{\bigmeet}{\Meet}
\newcommand{\bigjoin}{\Join}
\newcommand{\leftq}[2]{\!\!\phantom{.}^{#1} {#2}}
\newcommand{\closeleftq}[2]{\!\!\phantom{.}^{#1}\! {#2}}
\newcommand{\Pge}{{\Phi_{\ge -1}}}
%\newcommand{\ck}{^\vee}
%\newcommand{\ck}{^{\scalebox{0.5}[0.5]{$\vee$}}}
\newcommand{\ck}{\spcheck}
\newcommand{\letw}{\le_{\mathrm{tw}}}
\newcommand{\Alg}{\mathrm{Alg}}
\newcommand{\toname}[1]{\overset{#1}{\longrightarrow}}
\newcommand{\dashname}[1]{\overset{#1}{\mbox{---\!---}}}
\newcommand{\st}{^\mathrm{st}}
\renewcommand{\th}{^\text{th}}
\newcommand{\nd}{^\text{nd}}
\newcommand{\rd}{^\text{rd}}
\newcommand{\0}{{\mathbf{0}}}
\newcommand{\Vol}{\mathrm{Vol}}
\newcommand{\lleq}{\le\!\!\!\le}
\newcommand{\notlleq}{\le\!\!\!\!\not\,\le}
\newcommand{\ggeq}{\ge\!\!\!\ge}
\newcommand{\Cone}{\mathrm{Cone}}
\newcommand{\Star}{\mathrm{Star}}
\newcommand{\Lin}{\mathrm{Lin}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Proj}{\mathrm{Proj}}
\newcommand{\relint}{\mathrm{relint}}
\newcommand{\Clust}{\mathrm{Clust}}
\newcommand{\into}{\hookrightarrow}
\newcommand{\equivalent}{\Longleftrightarrow}
\newcommand{\onto}{\twoheadrightarrow}
\newcommand{\isomorph}{\cong}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\Asym}{A_{\mathrm{sym}}}
\newcommand{\Cox}{\mathrm{Cox}}
\newcommand{\Des}{\mathrm{Des}}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\inv}{inv}
\newcommand{\odd}{\mathrm{odd}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\newcommand{\m}{\mathbf{m}}
\renewcommand{\c}{\mathbf{c}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\k}{\mathbbm{k}}
\newcommand{\kk}{\mathbf{k}}
\renewcommand{\ll}{{\boldsymbol\ell}}
\newcommand{\ks}{\mathbf{k}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\renewcommand{\t}{\mathbf{t}}
\renewcommand{\v}{\mathbf{v}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\tB}{{\tilde{B}}}
\newcommand{\tM}{{\widetilde{M}}}
\newcommand{\tN}{{\tilde{N}}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\ZP}{\mathbb{ZP}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\BB}{\mathbf{B}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\U}{\mathcal{U}}
\renewcommand{\O}{\mathcal{O}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Rel}{\operatorname{Rel}}
\newcommand{\Trop}{\operatorname{Trop}}
\newcommand{\pr}{{\operatorname{pr}}}
\newcommand{\bB}{\widebar{B}}
\renewcommand{\S}{\mathbf{S}}
\newcommand{\Clear}{\operatorname{Clear}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\Scat}{\operatorname{Scat}}
\newcommand{\Fan}{\operatorname{Fan}}
\newcommand{\ScatFan}{\operatorname{ScatFan}}
\newcommand{\ClusFan}{\operatorname{ClusFan}}
\newcommand{\CambScat}{\operatorname{CambScat}}
\newcommand{\ChamberFan}{\operatorname{ChamberFan}}
\newcommand{\Nar}{\operatorname{Nar}}
\newcommand{\can}{\operatorname{can}}
\renewcommand{\mid}{\operatorname{mid}}
\newcommand{\re}{\mathrm{re}}
\newcommand{\im}{\mathrm{im}}
%\newcommand{\init}{\mathrm{in}}
\renewcommand{\d}{{\mathfrak d}}
%\newcommand{\f}{{\mathfrak f}}
\newcommand{\seg}[1]{\overline{#1}}
\newcommand{\hy}{\hat{y}}
\newcommand{\ab}{\uparrow}
\newcommand{\bel}{\downarrow}

\newcommand{\complexes}{\mathbb{C}}
\newcommand{\Up}{\Upsilon}
\newcommand{\tUp}{\widetilde\Upsilon}
\newcommand{\cm}[3]{(#1\Vert#2)_{#3}}
\newcommand{\Cm}[3]{\bigl(#1\big\Vert#2\bigr)_{#3}}
\newcommand{\CM}[3]{\Bigl(#1\Big\Vert#2\Bigr)_{#3}}
\newcommand{\cmrarrow}{{\small{\rightarrow}}}
\newcommand{\cmlarrow}{{\small{\leftarrow}}}
\newcommand{\cmcircarrow}{{\small{\circlearrowright}}}
\newcommand{\cmr}[3]{(#1\cmrarrow#2)_{#3}}
\newcommand{\Cmr}[3]{\bigl(#1\cmrarrow#2\bigr)_{#3}}
\newcommand{\CMr}[3]{\Bigl(#1\cmrarrow#2\Bigr)_{#3}}
\newcommand{\cml}[3]{(#1\cmlarrow#2)_{#3}}
\newcommand{\Cml}[3]{\bigl(#1\cmlarrow#2\bigr)_{#3}}
\newcommand{\CMl}[3]{\Bigl(#1\cmltarrow#2\Bigr)_{#3}}
\newcommand{\cmcirc}[3]{(#1\,\cmcircarrow\,#2)_{#3}}
\newcommand{\Cmcirc}[3]{\bigl(#1\,\cmcircarrow\,#2\bigr)_{#3}}
\newcommand{\CMcirc}[3]{\Bigl(#1\,\cmcircarrow\,#2\Bigr)_{#3}}
\newcommand{\intnum}[2]{(#1\,|\,#2)}
\newcommand{\Phire}{\Phi^{\operatorname{re}}}
\newcommand{\dist}{\operatorname{dist}}
\renewcommand{\c}{{\mathbf c}}
\newcommand{\dd}{{\mathbf d}}
\newcommand{\aff}{\mathrm{aff}}
\newcommand{\fin}{\mathrm{fin}}
\renewcommand{\th}{^\text{th}}
%\newcommand{\laff}{<}
%\newcommand{\gaff}{>}
\newcommand{\laff}{\triangleleft}
\newcommand{\gaff}{\triangleright}
\newcommand{\DF}{{\mathcal {DF}}}
\newcommand{\DCScat}{{\operatorname{DCScat}}}
\newcommand{\adj}[2]{\operatorname{adj}_{#1}(#2)}
\newcommand{\Lower}{\operatorname{Lower}}
\newcommand{\Upper}{\operatorname{Upper}}


% Notation mess
% the space of eigenvectors of c
\newcommand{\eigenspace}[1]{U^{#1}}
% the full root system
\newcommand{\RSChar}{\Phi}
\newcommand{\RS}{\RSChar}
\newcommand{\RSre}{\RS^\re}
\newcommand{\RSpos}{\RS^+}
\newcommand{\RSneg}{\RS^-}
\newcommand{\RSfin}{\RS_\fin}
\newcommand{\RSfinpos}{\RSfin^+}
\newcommand{\RSfinneg}{\RSfin^-}
% simples in \RS
\newcommand{\SimplesChar}{\Pi}
\newcommand{\Simples}{\SimplesChar}
\newcommand{\simple}{\alpha}
% the root subsystem in \eigenspace (T is for "tubes")
\newcommand{\RSTChar}{\Upsilon}
\newcommand{\RST}[1]{\RSTChar^{#1}}
\newcommand{\RSTfin}[1]{\RST{#1}_\fin}
% simples in \RST
\newcommand{\SimplesTChar}{\Xi}
\newcommand{\SimplesT}[1]{\SimplesTChar^{#1}}
\newcommand{\simpleT}{\beta}
% Supports
\newcommand{\Supp}{\operatorname{Supp}_\SimplesChar}
\newcommand{\SuppT}{\operatorname{Supp}_\SimplesTChar}
% traversals for \tau-orbits
\newcommand{\TravInfChar}{\Psi}
\newcommand{\TravInf}[1]{\TravInfChar^{#1}}
\newcommand{\proj}{\to}
\newcommand{\TravProj}[1]{\overrightarrow{\TravInfChar}^{#1}}
\newcommand{\inj}{\leftarrow}
\newcommand{\TravInj}[1]{\overleftarrow{\TravInfChar}^{#1}}
%\newcommand{\proj}{\medvertdot}
%\newcommand{\TravProj}[1]{\TravInfChar^{#1}_\proj}
%\newcommand{\inj}{\dotmedvert}
%\newcommand{\TravInj}[1]{\TravInfChar^{#1}_\inj}
\newcommand{\TravRegChar}{\Omega}
\newcommand{\TravReg}[1]{\TravRegChar^{#1}}
% Schur roots
\newcommand{\AP}[1]{\RS_{#1}}
\newcommand{\APre}[1]{\AP{#1}^\re}
%\newcommand{\APT}[1]{\RST{#1}_{#1}}           % THIS NEEDS A BETTER NOTATION possibly \Lambda_c
%\newcommand{\APTre}[1]{\RST{#1;\re}_{#1}}     % THIS NEEDS A BETTER NOTATION possibly \Lambda_c^\re
\newcommand{\APTChar}{\Lambda}
\newcommand{\APT}[1]{\APTChar_{#1}}      
\newcommand{\APTre}[1]{\APT{#1}^\re}     


\newcommand{\fakesubsec}[1]{\medskip\noindent\textbf{#1.}}  %unnumbered

%\allowdisplaybreaks

%  Uncomment the following to remove all figures (useful for checking how many pages are taken up by figures)
%\usepackage{comment}
%\excludecomment{figure}
%\let\endfigure\relax







% Commands for marginal notes below
\usepackage[draft]{say}
\newcommand{\sayS}[1]{\say[S]{#1}}
\newcommand{\sayN}[1]{\say[N]{#1}}
\newcommand{\sayD}[1]{\say[D]{#1}}
\newcommand{\margin}[1]{\say[N]{#1}}
% control the width of your comments
\addtolength{\marginparwidth}{3mm}

%  If you want to switch which margin you're using, do the command  \switchmargin before your marginal comment.
% But it won't let you switch which margin you use in the middle of a paragraph of the main text.
% Also, you can only switch if there is room on the other margin.  (I.e. if you switch too often, things may overlap
\makeatletter
\newcommand{\switchmargin}{
\if@reversemargin
\normalmarginpar
\else
\reversemarginpar
\fi
}
\makeatother

%\newcommand{\response}[2]{{\color{red}#1:--}#2{\color{red}--:#1}}
\newcommand{\response}[2]{ {\color{red}#1:}~#2}
\newcommand{\rn}[1]{\response{NR}{#1}}
\newcommand{\rs}[1]{\response{SS}{#1}}

\newcommand{\ok}[1]{ {\color{blue}#1: OK }}
\newcommand{\okn}{\ok{NR}}
\newcommand{\oks}{\ok{SS}}

% A quick way to get rid of the red text.
%\renewcommand{\textcolor}[2]{}


\author{Nathan Reading}
\author{Dylan Rupel}
\author{Salvatore Stella}
\title{Dominance Regions for Affine Cluster Algebras}
\address[N. Reading]{Department of Mathematics, North Carolina State University, Raleigh, NC, USA}
\address[D. Rupel]{NEED THIS}
\address[S. Stella]{NEED THIS}
%\keywords{}
\thanks{Nathan Reading was partially supported by the Simons Foundation under award number 581608 and by the National Science Foundation under award number DMS-2054489.
Dylan Rupel was partially supported by ????.  
Salvatore Stella was partially supported by ????.  }
%\received{}
%\revised{}
%\accepted{}

\begin{document}

\begin{abstract}
NEED THIS
\end{abstract}

\maketitle

\vspace{-8pt}

\setcounter{tocdepth}{2}
\tableofcontents





\section{Background}
We assume the basic definitions of exchange matrices and of matrix mutation.
Given a sequence $\kk=k_m\cdots k_1$ of indices in $\set{1,\ldots,n}$, we read the sequence from right to left for the purposes of matrix mutation.
That is, $\mu_\kk(B)$ means $\mu_{k_m}(\mu_{k_{m-1}}(\cdots(\mu_{k_1}(B))\cdots))$.
We write $\kk^{-1}$ for $k_1\cdots k_m$, the reverse of $\kk$.
Throughout, we will use without comment the fact that matrix mutation commutes with the maps $B\mapsto-B$ and $B\mapsto B^T$.

Given an exchange matrix $B$, the \newword{mutation map} $\eta^B_\kk:\reals^n\to\reals^n$ takes the input vector in $\reals^n$, places it as an additional row below $B$, mutates the resulting matrix according to the sequence $\kk$, and outputs the bottom row of the mutated matrix.
In this paper, it is convenient to think of vectors in $\reals^n$ as column vectors, and also, the mutation maps we need use transposes $B^T$ of exchange matrices.
Thus we write maps $\eta_\kk^{B^T}$.
This map takes a vector, places it as an additional \emph{column} to the right of $B$ (not $B^T$), does mutations according to $\kk$, and reads the rightmost column of the mutated matrix.

Given a vector $\lambda\in\reals^n$, define $\P^B_{\lambda,\kk}=\left(\eta_{\kk}^{B^T}\right)^{-1}\set{\eta_\kk^{B^T}(\lambda)+B_t\alpha:\alpha\in\reals^n,\alpha\ge0}$, where the symbol $\ge$ denotes componentwise comparison.
(Throughout the paper, we will define sets indexed by vectors $\alpha\in\reals^n$ with $\alpha\ge0$, or sometimes $\alpha\in\reals^m$ with $\alpha\ge0$.
When we can do so without confusion, we will omit the explicit statement that $\alpha\in\reals^n$ or $\alpha\in\reals^m$.)
Define the \newword{dominance region} of $\lambda$ with respect to $B$ to be $\P^B_\lambda=\bigcap_\kk\P^B_{\lambda,\kk}$, where the intersection is over all sequences~$\kk$.
\begin{lemma}\label{shift}
If $\lambda'=\eta^{B^T}_\kk(\lambda)$ and $B'=\mu_\kk(B)$, then 
\begin{enumerate}[\quad\bf1.]
\item \label{shift all}
$\eta^{B^T}_\kk\!\!(\P^B_\lambda)=\P^{B'}_{\lambda'}$.
\item \label{shift one}
$\eta^{B^T}_\kk\!\!(\P^B_{\lambda,\ll})=\P^{B'}_{\lambda',\ll\kk^{-1}}$ for any $\ll$.
\end{enumerate}
\end{lemma}
\begin{proof}
For any $\ll$,
\begin{align*}
\eta^{B^T}_\kk\!\!(\P^B_{\lambda,\ll})
&=\eta^{B^T}_\kk\!\!\left(\left(\eta_{\ll}^{B^T}\right)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\right)\\
%&=\left(\eta^{B^T}_\ll\!\!\left(\eta_{\kk}^{B^T}\right)^{-1}\right)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\\
&=\left(\eta^{B^T}_\ll\!\!\eta_{\kk^{-1}}^{\mu_\kk(B)^T}\right)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\\
%&=\left(\eta^{\mu_\kk(B)^T}_{\ll\kk^{-1}}\right)^{-1}\set{\eta_\ll^{B^T}(\lambda)+B_t\alpha:\alpha\ge0}\\
&=\left(\eta^{\mu_\kk(B)^T}_{\ll\kk^{-1}}\right)^{-1}\set{\eta^{\mu_\kk(B)^T}_{\ll\kk^{-1}}\left(\eta^{B^T}_\kk(\lambda)\right)+B_t\alpha:\alpha\ge0}\\
&=\P^{B'}_{\lambda',\ll\kk^{-1}}.
\end{align*}
Thus $\eta^{B^T}_\kk\!\!(\P^B_\lambda)=\bigcap_\ll\P^{B'}_{\lambda',\ll\kk^{-1}}=\P^{B'}_{\lambda'}$.
\end{proof}


For seeds $t_0$ and $t$ and an exchange matrix $B$, let $C_t^{B;t_0}$ be the matrix whose columns are the $C$-vectors at $t$ relative to the initial seed $t_0$ with exchange matrix~$B$.
Each column of $C_t^{B;t_0}$ is nonzero and all of its nonzero entries have the same sign.
(This is ``sign-coherence of $C$-vectors'', which was implicitly conjectured in \cite{FZ07} and proved as \cite[Corollary~5.5]{GHKK18}.)
Thus we will refer to the \newword{sign} of a column of $C_t^{B;t_0}$.
For $\kk=k_m\cdots k_1$, define seeds $t_1,\ldots,t_m$ by $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m$.
The sequence $\kk$ is a \newword{green sequence} for an exchange matrix $B$ if column $k_\ell$ of $C_{t_{\ell-1}}^{B;t_0}$ is \emph{positive} for all $\ell$ with $1\le\ell<m$.
A \newword{maximal green sequence} for $B$ is a green sequence that cannot be extended.
That is, the sequence $\kk$ is a maximal green sequence if every column of $C_{t_m}^{B;t_0}$ is \emph{negative}.
We will call $\kk$ a \newword{red sequence} for~$B$ if it is a green sequence for $-B$.
A \newword{maximal red sequence} is a red sequence that cannot be extended.
(A red sequence relates to antiprincipal coefficients: 
If we were to define the $C$-vectors recursively starting with the negative of the identity matrix, the requirement for a red sequence is that the $k_\ell$ column is negative at every step.)

Let $G_t^{B;t_0}$ be the matrix whose columns are the $\g$-vectors at $t$ relative to the initial seed $t_0$ with exchange matrix~$B$.
Let $\Cone^{B;t_0}_t$ be the nonnegative linear span of the columns of $G_t^{B;t_0}$.
For each $k\in\set{1,\ldots,n}$, the entries in the $k\th$ row of $G_t^{B;t_0}$ are not all zero and the nonzero entries have the same sign.
(This is ``sign-coherence of $\g$-vectors'', conjectured as \cite[Conjecture~6.13]{FZ07} and proved as \cite[Theorem 5.11]{GHKK18}.)
Thus all vectors in $\Cone^{B;t_0}_t$ all have weakly the same sign in the $k\th$ position.
The inverse of $G_t^{B;t_0}$ is $\bigl(C_t^{-B^T;t_0}\bigr)^T$.
(This is \cite[Theorem~1.2]{NZ12} or \cite[Theorem~1.1]{RS16} and \cite[Theorem~3.30]{RS16}.)
Thus $\Cone^{B;t_0}_t=\set{x\in\reals^n:x^TC_t^{-B^T;t_0}\ge0}$, where $0$ is a row vector and ``$\ge$'' means componentwise comparison. 

Given $\kk$ with $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m$, let $B_i$ be the exchange matrix at~$t_i$, so that in particular, $B_0=B$.
The map $\eta_{\kk}^{B^T}$ is ${\eta_{k_m}^{B_{m-1}^T}\circ\cdots\circ\eta_{k_2}^{B_1^T}\circ\eta_{k_1}^{B_0^T}}$.
The definition of each $\eta_{k_i}^{B_{i-1}^T}$ has two cases, separated by the hyperplane $x_{k_i}=0$.
Two vectors are in the same \newword{domain of definition} of $\eta_\kk^{B^T}$ if, at every step, the same case applies for the two vectors.
(Both cases apply on the hyperplane, so domains of definition are closed.)
In particular, $\eta_\kk^{B^T}$ is linear in each of its domains of definition, but the domains of linearity of $\eta_\kk^{B^T}$ can be larger than its domains of definition.

There is a fan $\F_{B^T}$ called the \newword{mutation fan} for $B^T$ \cite[Definition~5.12]{universal}.
We will not need the details of the definition, but roughly, the cones of $\F_{B^T}$ are the intersections of domains of definition of all mutation maps $\eta_\kk^{B^T}$, as $\kk$ varies.
Thus for each $\kk$, each cone of $\F_{B^T}$ is contained in a domain of definition of $\eta_\kk^{B^T}$, and the mutation map $\eta_\kk^{B^T}$ is linear on every cone of $\F_{B^T}$ \cite[Proposition~5.3]{universal}.
Every cone $\Cone^{B;t_0}_t$ is a maximal cone in the mutation fan $\F_{B^T}$ \cite[Proposition~8.13]{universal}.
Thus in particular, the mutation map $\eta_\kk^{B^T}$ is linear on every cone $\Cone^{B;t_0}_t$.
Furthermore, $\Cone_t^{B_m;t_m}=\eta_\kk^{B^T}\left(\Cone_t^{B;t_0}\right)$ for every seed~$t$.
(This amounts to the initial seed mutation formula for $\g$-vectors, conjectured as \cite[Conjecture~7.12]{FZ07} and shown in \cite[Proposition~4.2(v)]{NZ12} to follow from sign-coherence of $C$-vectors.
The restatement in terms of mutation maps is \cite[Conjecture~8.11]{universal}.)

\begin{remark}\label{conditional}
As written, \cite[Proposition~8.13]{universal} is conditional on ``sign-coherence of $C$-vectors'', which was a conjecture but is now a theorem \cite[Corollary~5.5]{GHKK18}.
\end{remark}

We will need to relate the cones $\Cone^{B;t_0}_t$ and $\Cone^{-B^T;t_0}_t$.
It is immediate from \cite[Proposition~7.5]{universal} and the skew-symmetry of $B$ that $-B^T$ is a \newword{rescaling} of $B$, meaning that there is a diagonal matrix $\Sigma$ with positive entries on the diagonal such that $-B^T=\Sigma^{-1}B\Sigma$.
Therefore, \cite[Proposition~8.20]{universal} says that the $i\th$ column of $G_t^{-B^T;t_0}$ is a positive scalar multiple of the $i\th$ column of $\Sigma G_t^{B;t_0}$.
(In the statement of \cite[Proposition~8.20]{universal}, $\Sigma$ is multiplied on the right, because there $\g$-vectors are row vectors rather than column vectors.)
Thus we have the following fact.
\begin{lemma}\label{B or -BT}
The $k\th$ entries of vectors in $\Cone^{B;t_0}_t$ have the same sign as the $k\th$ entries of vectors in $\Cone^{-B^T;t_0}_t$.
\end{lemma}

For $k\in\set{1,\ldots,n}$, let $J_k$ be the $n\times n$ matrix that agrees with the identity matrix except that $J_k$ has $-1$ in position $kk$.
For an $n\times n$ matrix $M$ and $k\in\set{1,\ldots,n}$, let $M^{\bullet k}$ be the matrix that agrees with $M$ in column $k$ and has zeros everywhere outside of column $k$.
Let $M^{k\bullet}$ be the matrix that agrees with $M$ in row $k$ and has zeros everywhere outside of row $k$.

Given a real number $a$, let $[a]_+$ denote $\max(a,0)$.
Given a matrix $M=[m_{ij}]$, define $[M]_+$ to be the matrix whose $ij$-entry is $[m_{ij}]_+$.
Given an exchange matrix~$B$, an index $k\in\set{1,\ldots,n}$ and a sign $\ep\in\set{\pm1}$, define matrices
\begin{align*}
E_{\ep,k}^B&=J_k+[\ep B]_+^{\bullet k}\\
F_{\ep,k}^B&=J_k+[-\ep B]_+^{k\bullet}.
\end{align*}
Each matrix $E_{\ep,k}^B$ is its own inverse, and each $F_{\ep,k}^B$ is its own inverse.
The following is essentially a result of \cite{NZ12}, although it is not stated there in this form.  \margin{Do I have this attribution right?}
\begin{lemma}\label{EBF trick}
For $k\in\set{1,\ldots,n}$ and either choice of $\ep\in\set{\pm1}$, the mutation of $B$ at $k$ is $\mu_k(B)=E_{\ep,k}^BBF_{\ep,k}^B$.
\end{lemma}
\begin{proof}
We expand the product $(J_k+[\ep B]_+^{\bullet k})B(J_k+[-\ep B]_+^{k\bullet})$ to four terms.
The term $[\ep B]_+^{\bullet k}B[-\ep B]_+^{k\bullet}$ is zero because $b_{kk}=0$.
The term $[\ep B]_+^{\bullet k}BJ_k$ is $[\ep B]_+^{\bullet k}B^{k\bullet}J_k$, which equals $[\ep B]_+^{\bullet k}B^{k\bullet}$.
Similarly, the term $J_kB[-\ep B]_+^{k\bullet}$ equals $B^{\bullet k}[-\ep B]_+^{k\bullet}$
%\begin{align*}
%(J_k+[\ep B]_+^{\bullet k})B(J_k+[-\ep B]_+^{k\bullet})
%&=J_kBJ_k+J_kB[-\ep B]_+^{k\bullet}+[\ep B]_+^{\bullet k}BJ_k+[\ep B]_+^{\bullet k}B[-\ep B]_+^{k\bullet}.
%\end{align*}
Both 
Thus the $ij$-entry of $E_{\ep,k}^BBF_{\ep,k}^B$ is 
\[
\left\{\begin{aligned}
-b_{ij}&\quad\text{if }k\in\set{i,j}\\
b_{ij}&\quad\text{otherwise}
\end{aligned}\right\}
+
\left\{\begin{aligned}
|b_{ik}|b_{kj}&\quad\text{if }\sgn b_{ik}=\ep\\
0&\quad\text{otherwise}
\end{aligned}\right\}
+
\left\{\begin{aligned}
b_{ik}|b_{kj}|&\quad\text{if }\sgn b_{kj}=-\ep\\
0&\quad\text{otherwise}
\end{aligned}\right\}.
\]
This coincides with the $ij$-entry of $\mu_k(B)$.
\end{proof}

Given a matrix $M$, write $M_{\col(i)}$ for the $i\th$ column of $M$.
We observe that $(MN)_{\col i}=M(N)_{\col i}$.
\begin{lemma}\label{columns lem}
Suppose $B=[b_{ij}]$ is an exchange matrix, let $k\in\set{1,\ldots,n}$, and choose a sign $\ep\in\set{\pm1}$.
\begin{enumerate}[\quad\bf1.]
\item \label{col i}
$(E_{\ep,k}^BB)_{\col i}=J_k(B)_{\col i}+b_{ki}([\ep B]_+)_{\col k}$.
\item \label{col k}
$(E_{\ep,k}^BB)_{\col k}=(E_{-\ep,k}^BB)_{\col k}=B_{\col k}$.
\item \label{cols k}
$(E_{-\ep,k}^BB)_{\col i}=(E_{\ep,k}^BB)_{\col i}-\ep b_{ki}B_{\col k}$.
\end{enumerate}
\end{lemma}
\begin{proof}
The first two assertions follow immediately from the fact that $(MN)_{\col i}=M(N)_{\col i}$ and the fact that $b_{kk}=0$.
The first assertion (for $\ep$ and $-\ep$) implies that $(E_{-\ep,k}^BB)_{\col i}=(E_{\ep,k}^BB)_{\col i}-b_{ki}([\ep B]_+-[-\ep B]_+)_{\col k}$.  
The third assertion follows.
\end{proof}

We will also need the following simple fact about nonnegative linear spans.
Given a set $S$ of vectors, let $\posspan(S)$ denote the nonnegative linear span of $S$.
For $k\in\set{1,\ldots,n}$ and $\ep\in\set{\pm1}$, let $S_{k,\ep}$ be the set of vectors in $S$ whose $k\th$ entry has sign strictly agreeing with $\ep$.

\begin{lemma}\label{ps lemma}
Suppose $\lambda$ is a vector in $\reals^n$ whose $k\th$ $\lambda_k$ has $\ep\lambda_k\le0$.
Then %any vector in $\set{\lambda+\posspan(S)}\cap\set{x\in\reals^n:\sgn x_k=\ep}$ can be written as a vector in $\set{\lambda+\posspan(S)}\cap\set{x\in\reals^n:x_k=0}$ plus a vector in $posspan(S_{k,\ep})$.
\begin{multline*}
\set{\lambda+\posspan(S)}\cap\set{x\in\reals^n:\ep x_k\ge0}\\
=\set{\lambda+\posspan(S)}\cap\set{x\in\reals^n:x_k=0}+\posspan(S_{k,\ep}).
\end{multline*}
\end{lemma}
\begin{proof}
The set on the right side is certainly contained in the set on the right side.
If $x$ is an element of the left side, then $x$ is $\lambda$ plus a nonzero element $y$ of $\posspan(S_{k,\ep})$ plus an element $z$ of $\posspan(S\setminus S_{k,\ep})$.
Since the sign of $\ep x\ge0$ and $\ep\lambda\le0$, there exists~$t$ with $0\le t\le1$ such that $\lambda+ty+z$ has $k\th$ entry $0$.
We see that ${x=(\lambda+ty+z)+(1-t)y}$ is an element of the right side.
\end{proof}



\section{First main result}
Let $B_0$ be an exchange matrix.
For a sequence $\kk=k_m\cdots k_1$ of indices, define seeds $t_1,\ldots,t_m=t$ by $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
We will prove the following theorem.
%
%\noindent
%START ALTERNATIVE WORDING
%
%There may be more than one sequence connecting $t_0$ to $t$.
%
%\noindent
%NOT TRUE:\\
%The mutation map $\eta_\kk^{B_0^T}$ depends only on $t$, not on the choice of $\kk$.
%
%\noindent
%HOWEVER, we can rescue the following (by showing that different $\kk$s with the same $t$ are related by a global permutation of rows/coumns):\\
%AFTER MORE THOUGHT:  I think that's probably true, but there are some issues.
%It would come down to showing that there is only one automorphism of the mutation fan that fixes the positive cone pointwise, and that's probably true, but would take some thought, and it's not really necessary for what we're trying to accomplish this spring.
%(And actually, it's not true as I just stated it... Think about Markov.  
%But for piecewise linear automorphisms of the mutation fan with the pieces separated by codim-1 cones that contain codim-1 faces of the fan, it's probable true.
%That's the kind of complication we would need to deal with.
%
%Thus we define $\P^{B_0}_{\lambda,t}$ to be $\P^{B_0}_{\lambda,\kk}$ for any sequence $\kk=k_m\cdots k_1$ with $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
%Our first main result is about $\P^{B_0}_{\lambda,t}$ in the case where $\lambda$ is in $\Cone^{B_0;t_0}_t$.
%
%Our first main result is about $\P^{B_0}_{\lambda,\kk}$ in the case where $\lambda$ is in $\Cone^{B_0;t_0}_t$ for some sequence $\kk$.
%
%
%\begin{theorem}\label{P in B0C}
%Fix an exchange pattern with $B_0$ at $t_0$.
%For some vertex $t$, suppose there exists a red sequence for $B_t$ that ends at $t_0$.
%Then for $\lambda\in\Cone^{B_0;t_0}_t$,
%\[\P^{B_0}_{\lambda,t}\subseteq\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}.\]
%\end{theorem}
%
%\noindent
%END ALTERNATIVE WORDING
%

\begin{theorem}\label{P in B0C}
Suppose $\kk=k_m\cdots k_1$ and $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
If $\kk^{-1}=k_1\cdots k_m$ is a red sequence for $B_t$, then for any~$\lambda$ in the domain of definition of $\eta_\kk^{B_0^T}$ that contains $\Cone^{B_0;t_0}_t$,
\[\P^{B_0}_{\lambda,\kk}\subseteq\set{\lambda+G_t^{B_0;t_0}B_t\alpha:\alpha\in\reals^n,\alpha\ge0}=\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}.\]
\end{theorem}

Since $\left(\eta_{\kk}^{B_0^T}\right)^{-1}=\eta_{\kk^{-1}}^{B_t^T}$, we have $\P^{B_0}_{\lambda,\kk}=\eta_{\kk^{-1}}^{B_t^T}\set{\eta_\kk^{B_0^T}(\lambda)+B_t\alpha:\alpha\ge0}$.
Let $D$ be the domain of definition of $\eta_{\kk}^{B_0^T}$  that contains $\Cone^{B_0;t_0}_t$.
Then $\eta_{\kk^{-1}}^{B_t^T}$ is linear on $\eta_{\kk}^{B_0^T}(D)$.
Let~$\L_{\kk^{-1}}^{B_t^T}$ be the linear map that agrees with $\eta_{\kk^{-1}}^{B_t^T}$ on~$\eta_{\kk}^{B_0^T}(D)$.

\begin{proposition}\label{L mat}
The matrix for $\L_{\kk^{-1}}^{B_t^T}$, acting on column vectors, is $G_t^{B_0;t_0}$.
\end{proposition}
\begin{proof}
By \cite[Proposition~8.13]{universal}, $\Cone^{B_0;t_0}_t=\eta_{\kk^{-1}}^{B_t^T}\left(\left(\reals_{\ge0}\right)^n\right)$, and therefore also ${\eta_\kk^{B_0^T}\left(\Cone^{B_0;t_0}_t\right)=\left(\reals_{\ge0}\right)^n}$.
The proof of \cite[Proposition~8.13]{universal} shows not only an equality of cones, but also that $\eta_{\kk^{-1}}^{B_t^T}$ takes the extreme ray of $\left(\reals_{\ge0}\right)^n$ spanned by $e_i$ to the extreme ray of $\Cone^{B_0;t_0}_t$ spanned by the $i\th$ $\g$-vector at $t$ relative to $B_0;t_0$, where the total order on these $\g$-vectors at $t$ is obtained from the order $e_1,\ldots,e_n$ on $\g$-vectors at $t_0$ by the sequence $\kk$ of mutations.
\end{proof}

We now apply a result of \cite{NZ12}, namely that $G_t^{B_0;t_0}B_t=B_0C_t^{B_0;t_0}$.
This fact follows from the proof of \cite[Proposition~1.3]{NZ12}, or from \cite[(6.14)]{FZ07}, as explained in \cite[Remark~2.1]{NZ12}.
Since $G_t^{B_0;t_0}$ is the matrix for~$\L_{\kk^{-1}}^{B_t^T}$ and since $\L_{\kk^{-1}}^{B_t^T}\eta_\kk^{B_0^T}(\lambda)=\lambda$, we have the following \lcnamecref{B0C}.

\begin{proposition}\label{B0C}
\begin{align*}
\L_{\kk^{-1}}^{B_t^T}\set{\eta_\kk^{B_0^T}(\lambda)+B_t\alpha:\alpha\in\reals^n,\alpha\ge0}
&=\set{\lambda+G_t^{B_0;t_0}B_t\alpha:\alpha\in\reals^n,\alpha\ge0}\\
&=\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}.
\end{align*}
\end{proposition}

In light of \cref{B0C}, the conclusion of \cref{P in B0C} is equivalent to
\[\P^{B_0}_{\lambda,\kk}\subseteq\L_{\kk^{-1}}^{B_t^T}\set{\eta_\kk^{B_0^T}(\lambda)+B_t\alpha:\alpha\ge0}.\]

%% Don't really care about this any more.  Also, if we keep it, the meaning of $D$ has changed, so we would have to rephrase it.
%\cref{B0C} also immediately implies the following statement that is weaker than \cref{P in B0C}.
%
%\begin{proposition}\label{on domain}
%$\P^{B_0}_{\lambda,\kk}\cap D=\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}\cap D$.
%\end{proposition}


\begin{proof}[Proof of \cref{P in B0C}]
We will prove that $P^{B_0}_{\lambda,\kk}\subseteq\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}$, by induction on $m$ (the length of $\kk$).
The base case, where $\kk=\emptyset$, is true because $C_{t_0}^{B_0;t_0}$ is the identity matrix and $\P_{\lambda,\emptyset}=\set{\lambda+B_0\alpha:\alpha\ge0}$.
 
\cite[Proposition~1.4]{NZ12} says that $C_t^{B_0;t_0}=F^{B_1}_{\ep,k_1}C_t^{B_1;t_1}$, where $\ep$ is the sign of the $k_1$-column of $C_{t_1}^{-B_t;t}$.  
(The hypothesis that $\kk^{-1}$ is a red sequence for $B_t$ determines $\ep$, but we leave $\ep$ unspecified for now in order to highlight later where this hypothesis is relevant.)
By \cref{EBF trick} and because $E^{B_1}_{\ep,k_1}$ and $F^{B_1}_{\ep,k_1}$ are their own inverses,
\begin{equation}\label{ind B0C}\begin{aligned}
\set{\lambda+B_0C_t^{B_0;t_0}\alpha:\alpha\ge0}
&=\set{\lambda+B_0F^{B_1}_{\ep,k_1}C_t^{B_1;t_1}\alpha:\alpha\ge0}\\
&=\set{\lambda+E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}\\
&=E^{B_1}_{\ep,k_1}\set{E^{B_1}_{\ep,k_1}\lambda+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.
\end{aligned}\end{equation}

The map $\eta_{\kk}^{B_0^T}$ is linear on $\Cone_t^{B_0;t_0}$.  
This map is $\eta_{\kk}^{B_0^T}={\eta_{k_m}^{B_{m-1}^T}\circ\cdots\circ\eta_{k_2}^{B_1^T}\circ\eta_{k_1}^{B_0^T}}$.
The map $\eta_{k_1}^{B_0^T}$ restricts to a linear map from $\Cone_t^{B_0;t_0}$ to $\Cone_t^{B_1;t_1}$.
The inverse of $\eta_{k_1}^{B_0^T}$ is $\eta_{k_1}^{B_1^T}$.
We claim that $E^{B_1}_{\ep,k_1}$ is the matrix for the linear map on column vectors that agrees with $\eta_{k_1}^{B_1^T}$ on $\Cone_t^{B_1;t_1}$.
Since $E^{B_1}_{\ep,k_1}$ is its own inverse, the claim is equivalent to saying that implies that $E^{B_1}_{\ep,k_1}$ is the linear map that agrees with $\eta_{k_1}^{B_0^T}$ on $\Cone_t^{B_0;t_0}$.

By \cite[(1.13)]{NZ12}, $\ep$ is the sign of the $k_1$-column of $\bigl(G_t^{-B_1^T;t_1}\bigr)^T$.
That is, $\ep$ is the sign of the $k_1$-row of $G_t^{-B_1^T;t_1}$, or in other words, the sign of the $k_1$-entry of vectors in $\Cone_t^{-B^T_1;t_1}$.
By \cref{B or -BT}, $\ep$ is the sign of the $k_1$-entry of vectors in $\Cone_t^{B_1;t_1}$, which is the sign that determines how $\eta_{k_1}^{B_1^T}$ acts on $\Cone_t^{B_1;t_1}$.
One easily checks that the action of $\eta_{k_1}^{B_1^T}$ on vectors whose $k_1$-entry has sign $\ep$ is precisely the action of $E^{B_1}_{\ep,k_1}$.
%It negates the $k_1$ entry.
%The $i\th$ ($i\neq k$) entry gets sent to itself plus the $k_1$ entry times $(B_1)_{ik}$ times $\ep$, if $(B_1)_{ik}$ also has sign $\ep$.

Let $\lambda'=\eta_{k_1}^{B_0^T}(\lambda)$, so that $\lambda'$ is in the same domain of definition of $\eta_{k_m\cdots k_2}^{B_1^T}$ as $\Cone_t^{B_1;t_1}$ and so that $\lambda'=E^{B_1}_{\ep,k_1}\lambda$.
By induction on $m$, 
\[\eta_{k_2\cdots k_m}^{B_t^T}\set{\eta_{k_m\cdots k_2}^{B_1^T}(\lambda')+B_t\alpha:\alpha\ge0}\subseteq\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.\]
Applying the homeomorphism $\eta_{k_1}^{B_1^T}$ to both sides, we obtain
\[\eta_{\kk^{-1}}^{B_t^T}\set{\eta_\kk^{B_0^T}(\lambda')+B_t\alpha:\alpha\ge0}\subseteq\eta_{k_1}^{B_1^T}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.\]
In light of \eqref{ind B0C}, we can complete the proof by showing that
\[\eta_{k_1}^{B_1^T}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}\subseteq E^{B_1}_{\ep,k_1}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}.\]

We have seen that $E^{B_1}_{\ep,k_1}$ is the linear map that agrees with $\eta_{k_1}^{B_1^T}$ on the set $\set{x\in\reals^n:\sgn x_{k_1}=\ep}$.
We can similarly check that $E^{B_1}_{-\ep,k_1}$ is the linear map that agrees with $\eta_{k_1}^{B_1^T}$ on $\set{x\in\reals^n:\sgn x_{k_1}=-\ep}$.
Thus $\eta_{k_1}^{B_1^T}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}$ is
\[(U\cap\set{x\in\reals^n:\sgn x_{k_1}=-\ep})\cup(V\cap\set{x\in\reals^n:\sgn x_{k_1}=\ep}),\]
where 
{\small
\begin{align*}
U&=E^{B_1}_{\ep,k_1}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}=E^{B_1}_{\ep,k_1}\lambda'+\posspan\set{\left(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}}_{i=1}^n\\
V&=E^{B_1}_{-\ep,k_1}\set{\lambda'+B_1C_t^{B_1;t_1}\alpha:\alpha\ge0}=E^{B_1}_{-\ep,k_1}\lambda'+\posspan\set{\left(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}}_{i=1}^n,
\end{align*}
}
where $\posspan$ denotes the nonnegative linear span of a set of vectors.

We need to show that $V\cap\set{x\in\reals^n:\sgn x_{k_1}=\ep}\subseteq U$.
Since $\eta_{k_1}^{B_1^T}$ is a homeomorphism, $U\cap\set{x\in\reals^n:x_{k_1}=0}=V\cap\set{x\in\reals^n:x_{k_1}=0}$.
By \cref{ps lemma}, any vector in $V\cap\set{x\in\reals^n:\sgn x_{k_1}=\ep}$ equals a vector in $V\cap\set{x\in\reals^n:x_{k_1}=0}$ plus a positive combination of vectors $\left(E^{B_1}_{-\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}$ whose $k_1$-entry has sign $\ep$.
Therefore, it suffices to show that every vector $\left(E^{B_1}_{-\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}$ whose $k_1$-entry has sign~$\ep$ is in $\posspan\set{\left(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}}_{i=1}^n$.

As a temporary shorthand, write $b_{ij}$ for the entries of $B_1$ and write $k$ for $k_1$.
Suppose $v_i=\left(E^{B_1}_{-\ep,k}B_1C_t^{B_1;t_1}\right)_{\col i}$ for some~$i$ and suppose the $k$-entry of $v_i$ has sign $\ep$.
Write $M$ for $E^{B_1}_{-\ep,k}B_1$ and write $N$ for $E^{B_1}_{\ep,k}B_1$.
\cref{columns lem}.\ref{col i} implies that $M_{kj}=-b_{kj}$ for all~$j$.
\cref{columns lem}.\ref{cols k} implies that if $\ep M_{kj}\ge0$, then $M_{\col j}=N_{\col j}+|b_{kj}|N_{\col k}$.
Similarly, if $\ep M_{kj}\le0$, then $M_{\col j}=N_{\col j}-|b_{kj}|N_{\col k}$.

Now $v_i=E^{B_1}_{-\ep,k}B_1\left(C_t^{B_1;t_1}\right)_{\col i}$, and $\left(C_t^{B_1;t_1}\right)_{\col i}$ has a sign $\delta\in\set{\pm1}$, meaning that it is not zero and all of its nonzero entries have sign $\delta$.
(This is ``sign-coherence of $C$-vectors''.  
See Remark~\ref{conditional}.)
Thus there are nonnegative numbers $\gamma_j$ such that $v_i=\delta\sum_{j=1}^n\gamma_jM_{\col j}$.
Write $\set{1,\ldots,n}=S\cup T$ with $S\cup T=\emptyset$ such that $\ep M_{kj}\ge0$ for all $j\in S$ and $\ep M_{kj}\le0$ for all $j\in T$.
Then
\begin{align*}
v_i
&=\delta\sum_{j\in S}\gamma_jM_{\col j}+\delta\sum_{j\in T}\gamma_jM_{\col j}\\
&=\delta\sum_{j\in S}\gamma_j(N_{\col j}+|b_{kj}|N_{\col k})+\delta\sum_{j\in T}\gamma_j(N_{\col j}-|b_{kj}|N_{\col k})\\
&=\delta\sum_{j=1}^n\gamma_jN_{\col j}-\delta\sum_{j=1}^n\ep\gamma_jb_{kj}N_{\col k}\\
&=N\left(C_t^{B_1;t_1}\right)_{\col i}+\delta\sum_{j=1}^n\ep\gamma_jM_{kj}N_{\col k}\\
&=N\left(C_t^{B_1;t_1}\right)_{\col i}+\sigma N_{\col k}.\\
\end{align*}
where $\sigma=\ep\delta\sum_{j=1}^n\gamma_jM_{kj}$ is a positive scalar, because $\delta\sum_{j=1}^n\gamma_jM_{kj}$ is the $k$-entry of $v_i$, which has sign $\ep$.

As noted above, $\ep$ is the sign of the $k_1$-entry of vectors in $\Cone_t^{-B_1^T;t_1}$.
Since $\Cone^{-B_1^T;t_1}_t=\set{x\in\reals^n:x^TC_t^{B_1;t_1}\ge0}$, the rows of $\left(C_t^{B_1;t_1}\right)^{-1}$ span the extreme rays of $\Cone_t^{-B_1^T;t_1}$.
In particular $\left(C_t^{B_1;t_1}\right)^{-1}(\ep e_k)$ has nonnegative entries.
Thus $C_t^{B_1;t_1}\left(C_t^{B_1;t_1}\right)^{-1}(\ep e_k)=\ep e_k$ is a nonnegative linear combination of columns of~$C_t^{B_1;t_1}$.

Now, the hypothesis that $\kk^{-1}$ is a red sequence for $B_t$, or equivalently a green sequence for $-B_t$, says that $\ep=+1$, so that $e_k$ is a nonnegative linear combination of columns of~$C_t^{B_1;t_1}$.
Thus $N_{\col k}=Ne_k$ is a nonnegative linear combination of columns of~$NC_t^{B_1;t_1}$.
We have shown that $v_i=N\left(C_t^{B_1;t_1}\right)_{\col i}+\sigma N_{\col k}$ is a nonnegative linear combination of columns of~$NC_t^{B_1;t_1}$.
In other words, $v_i$ is in $\posspan\set{\left(E^{B_1}_{\ep,k_1}B_1C_t^{B_1;t_1}\right)_{\col i}}_{i=1}^n$, as desired.
\end{proof}

\section{Extending to extended exchange matrices}
We follow \cite{FZ07} in considering $m\times n$ extended exchange matrices $\tB$ that are ``tall'', in the sense that $m\ge n$.
We will also consider $m\times m$ matrices related to $\tB$:
Writing $\tB$ in block form $\begin{bsmallmatrix}B\\E\end{bsmallmatrix}$, let $\BB$ be the matrix with block form $\begin{bsmallmatrix}B&-E^T\\E&0\end{bsmallmatrix}$.
Most importantly, $\BB$ is skew-symmetrizable and agrees with $\tB$ in columns $1$ to $n$.
Throughout, if we have defined an extended exchange matrix $\tB$, without comment we will take $B$ to be the underlying exchange matrix and $\BB$ to be the associated $m\times m$ matrix.

The matrix $\BB$ defines mutation maps $\eta_\kk^{\BB^T}$ that act on $\reals^m$ rather than $\reals^n$, but without exception we will only consider mutations in positions $1,\ldots,n$.
Also, given $\BB$, a sequence $\kk=k_m\cdots k_1$ of indices in $\set{1,\ldots,n}$, and seeds $t_1,\ldots,t_m$ by $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$, there are associated matrices of $\g$-vectors and $C$-vectors, which we write as $\GG_t^{\BB;t_0}$ and $\CC_t^{\BB;t_0}$.
Since $\kk$ only contains indices in $\set{1,\ldots,n}$, these matrices have block forms
\[
\GG_t^{\BB;t_0}=\begin{bsmallmatrix}G_t^{B;t_0}&0\\H_t^{\tB;t_0}&I_{m-n}\end{bsmallmatrix}
\quad\text{ and }\quad
\CC_t^{\BB;t_0}=\begin{bsmallmatrix}C_t^{B;t_0}&D_t^{\tB;t_0}\\0&I_{m-n}\end{bsmallmatrix},
\]
where $H_t^{\tB;t_0}$ is an $(m-n)\times n$ matrix, $D_t^{\tB;t_0}$ is an $n\times(m-n)$ matrix, and $I_{m-n}$ is the identity matrix.

Given a vector $\lambda\in\reals^m$, define $\P^\tB_{\lambda,\kk}=\left(\eta_{\kk}^{\BB^T}\right)^{-1}\!\!\set{\eta_\kk^{\BB^T}(\lambda)+\tB_t\alpha:\alpha\in\reals^n,\alpha\ge0}$.
Define the \newword{dominance region} $\P^\tB_\lambda$ of $\lambda$ with respect to $\tB$ to be the intersection $\bigcap_\kk\P^B_{\lambda,\kk}$ all sequences~$\kk$ of indices in $\set{1,\ldots,n}$.

Since $\kk$ consists only of indices in $\set{1,\ldots,n}$, the domains of definition of $\eta_\kk^{\BB^T}$ are determined by the domains of definition of $\eta_\kk^{B^T}$.
Specifically, each domain of definition of $\eta_\kk^{\BB^T}$ is the set of vectors whose projection to $\reals^n$ (ignoring the last $m-n$ entries) is a domain of definition of $\eta_\kk^{B^T}$.
Accordingly, we define $\Cone_t^{\tB;t_0}$ to be %the nonnegative span of the columns of $\GG_t^{\BB;t_0}$ and the columns of $\begin{bsmallmatrix}0\\-I_{m-n}\end{bsmallmatrix}$.
the set of vectors in $\reals^m$ whose projection to $\reals^n$ is in $\Cone_t^{B;t_0}$.
Since $\Cone_t^{B_m;t_m}=\eta_\kk^{B^T}\left(\Cone_t^{B;t_0}\right)$ for every seed~$t$, also $\Cone_t^{\tB_m;t_m}=\eta_\kk^{\BB^T}\left(\Cone_t^{\tB;t_0}\right)$ for every seed~$t$


To understand dominance regions $\P^\tB_\lambda$, it is enough to consider the case where $\lambda$ has nonzero entries only in positions $1,\ldots,n$.
Other dominance regions are obtained by translation, as explained in the following lemma.
The lemma is an immediate consequence of the fact that domains of definition of $\eta_\kk^{\BB^T}$ depend only on the first $n$ coordinates.
\begin{lemma}\label{after all coefficients are just coefficients}
If $\lambda$ and $\lambda'$ are vectors in $\reals^m$ that agree in the first $n$ coordinates, then $\P^\tB_{\lambda'}=\P^\tB_\lambda-\lambda+\lambda'$.
\end{lemma}

\cref{shift} immediately implies the following lemma.
\begin{lemma}\label{shift extended}
If $\lambda'=\eta^{\BB^T}_\kk$ and $\tB'=\mu_\kk(\tB)$, then 
\begin{enumerate}[\quad\bf1.]
\item \label{shift all}
$\eta^{\BB^T}_\kk\!\!(\P^\tB_\lambda)=\P^{\tB'}_{\lambda'}$.
\item \label{shift one}
$\eta^{\BB^T}_\kk\!\!(\P^\tB_{\lambda,\ll})=\P^{\tB'}_{\lambda',\ll\kk^{-1}}$ for any $\ll$.
\end{enumerate}
\end{lemma}



We will prove the following extension of \cref{P in B0C} and an important corollary.

\begin{theorem}\label{P in B0C extended}
Suppose $\kk=k_m\cdots k_1$ is a sequence of indices in $\set{1,\ldots, n}$ and $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
If $\kk^{-1}=k_1\cdots k_m$ is a red sequence for $B_t$, then for any~$\lambda$ in the domain of definition of $\eta_\kk^{\BB_0^T}$ that contains $\Cone^{B_0;t_0}_t$,
\[\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda+\GG_t^{\BB_0;t_0}\tB_t\alpha:\alpha\in\reals^n,\alpha\ge0}=\set{\lambda+\tB_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}.\]
\end{theorem}
\begin{proof}
First, we notice that $\kk^{-1}=k_1\cdots k_m$ is a red sequence for $\BB_t$, or in other words, $\kk$ is a green sequence for $-\BB_t$.
Indeed, since $\CC_{t_{\ell-1}}^{-\BB;t_0}=\begin{bsmallmatrix}C_{t_{\ell-1}}^{-\BB;t_0}&*\\0&I_{m-n}\end{bsmallmatrix}$, the sign of column $k_\ell$ of $\CC_{t_{\ell-1}}^{-\BB;t_0}$ equals the sign of column $k_\ell$ of $C_{t_{\ell-1}}^{-\BB;t_0}$ whenever $1\le\ell<k$.
Thus \cref{P in B0C} says that
\[\P^{\BB_0}_{\lambda,\kk}\subseteq\set{\lambda+\GG_t^{\BB_0;t_0}\BB_t\alpha:\alpha\in\reals^m,\alpha\ge0}=\set{\lambda+\BB_0\CC_t^{\BB_0;t_0}\alpha:\alpha\in\reals^m,\alpha\ge0}.\]
The assertion of \cref{P in B0C extended} is that the same holds even when, in each term, the conditions $\alpha\in\reals^m,\alpha\ge0$ are strengthened by requiring that $\alpha$ is zero in coordinates $n+1,\ldots,m$.

Thus we run through the proof of \cref{P in B0C} with $\BB$ replacing $B$ and $m$ replacing $n$ throughout and these additional conditions on $\alpha$ in all relevant expressions.
There is no effect on the argument until the point of showing that $V\cap\set{x\in\reals^m:\sgn x_{k_1}=\ep}\subseteq U$.
Here, we need to show that every vector $v_i=\left(E^{\BB_1}_{-\ep,k_1}\BB_1\CC_t^{\BB_1;t_1}\right)_{\col i}$ with $i\in\set{1,\ldots,n}$ whose $k_1$-entry has sign~$\ep$ is contained in $\posspan\set{\left(E^{\BB_1}_{\ep,k_1}\BB_1\CC_t^{\BB_1;t_1}\right)_{\col i}}_{i=1}^n$.
We argue as in the proof of \cref{P in B0C} that $v_i=N\left(\CC_t^{\BB_1;t_1}\right)_{\col i}+\sigma N_{\col k}$ and that $\ep e_k$ is a nonnegative linear combination of columns of~$\CC_t^{\BB_1;t_1}$.
Since $\CC_t^{\BB;t_0}=\begin{bsmallmatrix}C_t^{B;t_0}&*\\0&I_{m-n}\end{bsmallmatrix}$, we conclude that $\ep e_k$ is a nonnegative linear combination of columns $1$ through $n$ of~$\CC_t^{\BB_1;t_1}$.
Thus $v_i$ is a nonnegative linear combination of columns $1$ through $n$ of~$N\CC_t^{\BB_1;t_1}$ as desired.
\end{proof}

\begin{corollary}\label{P point}
Suppose $\tB_0$ is an extended exchange matrix with linearly independent columns.
Suppose $t$ is a seed in the exchange graph for $\tB_0;t_0$ and take $\lambda\in\Cone^{\tB_0;t_0}_t$.
If there exists a maximal red sequence for $B_t$, then $\P^{\tB_0}_\lambda=\set{\lambda}$.
\end{corollary}

\begin{proof}%[Proof of \cref{P point}]
Let $t'$ be the seed at the end of the maximal red sequence for $B_t$.
There exists $\ll=\ell_q\ell_{q-1}\cdots\ell_1$ with $t_0=t'_0\overset{\ell_1}{\edge}t'_1\overset{\ell_2}{\edge}\,\cdots\,\overset{\ell_q}{\edge}t'_q=t'$.
Let $\lambda'=\eta^{\BB_0^T}_\ll\!(\lambda)$.
\cref{shift extended} says $\eta^{\BB_0^T}_\ll\!(\P^{\tB_0}_\lambda)=\P^{\tB_{t'}}_{\lambda'}$.
Thus it is enough to prove that $\P^{\tB_{t'}}_{\lambda'}=\set{\lambda'}$.
Since $\eta_\ll^{\BB_0^T}\left(\Cone_t^{\tB_0;t_0}\right)=\Cone_t^{\tB_{t'};t'}$, we have reduced the proof to the case where there is a maximal red sequence for $B_t$ starting from $t$ and ending at $t_0$.

Working in that reduction, let $\kk=k_m\cdots k_1$ be the reverse of the maximal red sequence and define seeds $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
Then \cref{P in B0C extended} says that $\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda+\tB_0C_t^{B_0;t_0}\alpha:\alpha\in\reals^n,\alpha\ge0}$.

Since $\kk^{-1}$ is a maximal red sequence for $B_t$, or in other words a maximal green sequence for $-B_t$, every column of $C_{t_0}^{-B_t;t}$ has negative sign, so $\Cone_{t_0}^{B_t^T;t}=\set{x\in\reals^n:x^TC_{t_0}^{-B_t;t}\ge0}$ consists of vectors with nonpositive entries.
Since $\left(\reals_{\le0}\right)^n$ is a cone in the mutation fan $\F_{-B_t}$ (for example, combining \mbox{\cite[Proposition~7.1]{universal}}, \mbox{\cite[Proposition~8.9]{universal}}, and sign-coherence of $C$-vectors) and also $\Cone_{t_0}^{B_t^T;t}$ is a cone in $\F_{-B_t}$, we see that $\Cone_{t_0}^{B_t^T;t}=\left(\reals_{\le0}\right)^n$.
Thus, up to permuting columns, $C_{t_0}^{-B_t;t}$ is the negative of the identity matrix.
We see that $\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda-\tB_0\alpha:\alpha\in\reals^n,\alpha\ge0}$.

Since also $\P^{\tB_0}_{\lambda,\emptyset}\set{\lambda+\tB_0\alpha:\alpha\ge0}$, and since the columns of $\tB_0$ are linearly independent, we conclude that $\P^{\tB_0}_\lambda=\set{\lambda}$.
\end{proof}

\section{Affine type}
Let $B_0$ be acyclic of affine type, indexed so that entries above the diagonal are nonnegative.
%Or is this backwards?:
%Then $n(n-1)\cdots1$ is a maximal green sequence for $B_0$ and $12\cdots n$ is a maximal red sequence for $B_0$.
Take $\lambda$ in the imaginary cone.
Let $t$ be any seed such that $\Cone_t^{B_0;t_0}$ has $n-2$ rays on the boundary of the imaginary wall $\d_\infty$ such that $\lambda$ is in the imaginary cone spanned by those $n-2$ rays and the imaginary ray.
Let $\kk=k_m\cdots k_1$ be a sequence such that $t_0\overset{k_1}{\edge}t_1\overset{k_2}{\edge}\,\cdots\,\overset{k_m}{\edge}t_m=t$.
%(Probably we need to address greenness or redness of this sequence, which we can presumably do pretty easily with the sortable elements stuff.)

Let $\tB_0$ be an extension of $B_0$ that has linearly independent columns.
Computations show that  \margin{Probably need more specific notation about $\d_\infty$.}

\[\P_{\lambda,\kk}^{\tB_0}\cap\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0}\cap\d_\infty=\set{\lambda+x\tB_0\delta:x\in\reals}\cap\d_\infty.\]

Let $u$ be the seed reached from $t_0$ by the sequence $n(n-1)\cdots1$ and let $\lambda_u$ be $\eta_{n(n-1)\cdots1}^{\BB_0^T}(\lambda)$.
\cref{shift extended} says that $\eta_{n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk}^{\tB_0})=\P_{\lambda_u,\kk12\cdots n}^{\tB_u}$ and
\[\eta_{n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0})=\P_{\lambda_u,\kk n\cdots11\cdots n}^{\tB_u}=\P_{\lambda_u,\kk}^{\tB_u}.\]
The map $\eta_{n(n-1)\cdots1}^{\BB_0^T}$ is linear on $\d_\infty$ and maps $-\tB_0\delta$ to $-\tB_u\delta$, so it maps the set $\set{\lambda+x\tB_0\delta:x\in\reals}\cap\d_\infty$ to $\set{\lambda_u+x\tB_u\delta:x\in\reals}\cap\d_\infty$.
Thus we will show that 
\[\P_{\lambda_u,\kk}^{\tB_u}\cap\P_{\lambda_u,\kk12\cdots n}^{\tB_u}\cap\d_\infty=\set{\lambda_u+x\tB_u\delta:x\in\reals}\cap\d_\infty.\]

Let $t'$ be the seed reached from $t_0$ by the sequence $\kk n(n-1)\cdots1$ or in other words, the seed reached from $u$ by the sequence $\kk$.
Leaving out repetitions of ``$\alpha\ge0$'' for reasons of space,
\begin{multline*}
\P_{\lambda_u,\kk}^{\tB_u}
\cap
\P_{\lambda_u,\kk12\cdots n}^{\tB_u}
\cap
\d_\infty\\
\begin{aligned}
&=
\left(\eta_{\kk}^{\BB_u^T}\right)^{-1}\!\!\set{\eta_{\kk}^{\BB_u^T}(\lambda_u)+\tB_{t'}\alpha}
\cap
\left(\eta_{\kk1\cdots n}^{\BB_u^T}\right)^{-1}\!\!\set{\eta_{\kk1\cdots n}^{\BB_u^T}(\lambda)+\tB_t\alpha}
\cap
\d_\infty\\&=
\eta_{\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk}^{\BB_u^T}(\lambda_u)+\tB_{t'}\alpha}
\cap
\eta_{n\cdots1\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk1\cdots n}^{\BB_u^T}(\lambda_u)+\tB_t\alpha}
\cap
\d_\infty\\&=
\eta_{\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk}^{\BB_u^T}(\lambda_u)+\tB_{t'}\alpha}
\cap
\eta_{n\cdots1}^{\BB_0^T}\left(\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk1\cdots n}^{\BB_u^T}(\lambda_u)+\tB_t\alpha}\right)
\cap
\d_\infty %\\&=
\end{aligned}
\end{multline*}





Now, writing $\tB_u=\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}$, we have 
\margin{If we use this, we need to put the $\GG B C$ thing in the background.}
\begin{align*}
\tB_t=\mu_\kk(\mu_{1\cdots n}(\tB_u))=\mu_\kk(\tB_0)
&=\mu_\kk\left((\GG_{t_0}^{B_u;u})^{-1}\tB_uC_{t_0}^{B_u;u}\right)\\
&=\mu_\kk\left(\begin{bsmallmatrix}G_{t_0}^{B_u;i}&0\\ H_{t_0}^{\tB_u;u}&I_{m-n}\end{bsmallmatrix}^{-1}\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}C_{t_0}^{B_u;u}\right)\\
&=\mu_\kk\left(\begin{bsmallmatrix}(G_{t_0}^{B_u;u})^{-1}&0\\-H_{t_0}^{\tB_u;u}(G_{t_0}^{B_u;u})^{-1}&I_{m-n}\end{bsmallmatrix}\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}C_{t_0}^{B_u;u}\right)\\
&=\mu_\kk\left(\begin{bsmallmatrix}B_0\\-H_{t_0}^{\tB_u;u}B_0+E_uC_{t_0}^{B_u;u}\end{bsmallmatrix}\right)\\
\end{align*}
We compute that $B_0=B_u$ and that $C_{t_0}^{B_u;u}=-I_n$.
So $\tB_t=\mu_\kk\left(\begin{bsmallmatrix}B_u\\-H_{t_0}^{\tB_u;u}B_u-E_u\end{bsmallmatrix}\right)$.
On the other hand, $\tB_{t'}=\mu_\kk(\tB_u)=\mu_\kk\left(\begin{bsmallmatrix}B_u\\E_u\end{bsmallmatrix}\right)$.


KEY POINT:  On $\d_\infty$, the map $\eta_{n\cdots1}^{\BB_0^T}$ is linear, and agrees with $c$ or $c^{-1}$ or something.
So there is just a chance that we know something.



\subsection{Other ideas}

%Let $t'$ be the seed reached from $t$ by the sequence $\kk n(n-1)\cdots1$ and let $u$ be the seed reached from $t_0$ by the sequence $n(n-1)\cdots1$.
%Leaving out repetitions of ``$\alpha\ge0$'' for reasons of space,
%\begin{multline*}
%\P_{\lambda,\kk}^{\tB_0}
%\cap
%\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0}\\
%\begin{aligned}
%&=
%\left(\eta_{\kk}^{\BB_0^T}\right)^{-1}\!\!\set{\eta_{\kk}^{\BB_0^T}(\lambda)+\tB_t\alpha}
%\cap
%\left(\eta_{\kk n\cdots1}^{\BB_0^T}\right)^{-1}\!\!\set{\eta_{\kk n\cdots1}^{\BB_0^T}(\lambda)+\tB_{t'}\alpha}
%\\&=
%\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk}^{\BB_0^T}(\lambda)+\tB_t\alpha}
%\cap
%\eta_{1\cdots n\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1}^{\BB_0^T}(\lambda)+\tB_{t'}\alpha}
%\\&=
%\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk}^{\BB_0^T}(\lambda)+\tB_t\alpha}
%\cap
%\eta_{1\cdots n\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1}^{\BB_0^T}(\lambda)+\tB_{t'}\alpha}
%\end{aligned}
%\end{multline*}
%
%Now, writing $\tB_0=\begin{bsmallmatrix}B_0\\E_0\end{bsmallmatrix}$, we have 
%\margin{If we use this, we need to put the $\GG B C$ thing in the background.}
%\begin{align*}
%\tB_{t'}=\mu_\kk(\mu_{n\cdots1}(\tB_0))=\mu_\kk(\tB_u)
%&=\mu_\kk\left((\GG_u^{B_0;t_0})^{-1}\tB_0C_u^{B_0;t_0}\right)\\
%&=\mu_\kk\left(\begin{bsmallmatrix}G_u^{B_0;t_0}&0\\ H_u^{\tB;t_0}&I_{m-n}\end{bsmallmatrix}^{-1}\begin{bsmallmatrix}B_0\\E_0\end{bsmallmatrix}C_u^{B_0;t_0}\right)\\
%&=\mu_\kk\left(\begin{bsmallmatrix}(G_u^{B_0;t_0})^{-1}&0\\-H_u^{\tB;t_0}(G_u^{B;t_0})^{-1}&I_{m-n}\end{bsmallmatrix}\begin{bsmallmatrix}B_0\\E_0\end{bsmallmatrix}C_u^{B_0;t_0}\right)\\
%&=\mu_\kk\left(\begin{bsmallmatrix}B_u\\-H_u^{\tB;t_0}B_u+E_0C_u^{B_0;t_0}\end{bsmallmatrix}\right)\\
%\end{align*}
%We compute that $C $






%What does \cref{P in B0C extended} say?
%Assuming the appropriate sequences are red, and that $\lambda$ is in the right domain of definition, 
%\[\P^{\tB_0}_{\lambda,\kk}\subseteq\set{\lambda+\GG_t^{\BB_0;t_0}\tB_t\alpha:\alpha\ge0}=\set{\lambda+\tB_0C_t^{B_0;t_0}\alpha:\alpha\ge0}\]
%\[\P^{\tB_0}_{\lambda,\kk n(n-1)\cdots1}\subseteq\set{\lambda+\GG_{t'}^{\BB_0;t_0}\tB_{t'}\alpha:\alpha\ge0}=\set{\lambda+\tB_0C_{t'}^{B_0;t_0}\alpha:\alpha\ge0},\]
%where $t'$ is the seed obtained from $t_0$ by $\kk n(n-1)\cdots1$.

%Let $\lambda'=\eta_{\kk n(n-1)\cdots1}^{\BB_0^T}$.
%\cref{shift extended} says that $\eta_{\kk n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk}^{\tB_0})=\P_{\lambda',\kk12\cdots n\kk^{-1}}^{\tB_{t'}}$ and
%\[\eta_{\kk n(n-1)\cdots1}^{\BB_0^T}(\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0})=\P_{\lambda',\kk n\cdots11\cdots n \kk^{-1}}^{\tB_{t'}}=\P_{\lambda',\emptyset}^{\tB_{t'}}=\set{\lambda'+\tB_{t'}\alpha:\alpha\ge0}.\]

Let $\lambda_0=\eta_\kk^{\BB_0^T}$.
\cref{shift extended} says that  
\[\eta_\kk^{\BB_0^T}(\P_{\lambda,\kk n(n-1)\cdots1}^{\tB_0})=\P_{\lambda_0,\kk n(n-1)\cdots1\kk^{-1}}^{\tB_t}=\left(\eta^{\BB_t}_{\kk n\cdots1\kk^{-1}}\right)^{-1}\set{\eta^{\BB_t}_{\kk n\cdots1\kk^{-1}}(\lambda_0)+\tB_{t'}\alpha:\alpha\ge0}.\]
Also, 
\[\eta_\kk^{\BB_0^T}(\P_{\lambda,\kk}^{\tB_0})=\P_{\lambda_0,\kk\kk^{-1}}^{\tB_t}=\P_{\lambda_0,\emptyset}^{\tB_t}=\set{\lambda_0+\tB_t\alpha:\alpha\ge0}.\]

We believe we could prove that $\P_{\lambda,\kk}$ only depends on the seed that $\kk$ leads to, not the specific $\kk$.
So does it make sense to consider the sequence (maximal red or maximal green) that connects $t$ and $t'$.


%Let $t'$ be the seed reached from $t$ by the sequence $\kk n(n-1)\cdots1\kk^{-1}$ and let $u$ be the seed reached from $t_0$ by the sequence $n(n-1)\cdots1$.
%Now, leaving out repetitions of ``$\alpha\ge0$'' for reasons of space,
%\begin{multline*}
%\P_{\lambda_0,\kk n(n-1)\cdots1\kk^{-1}}^{\tB_t}\cap\P_{\lambda_0,\kk\kk^{-1}}^{\tB_t}\\
%\begin{aligned}
%&=\left(\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}\right)^{-1}\!\!\set{\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_{t'}\alpha}
%\cap
%\left(\eta_{\kk\kk^{-1}}^{\BB_t^T}\right)^{-1}\!\!\set{\eta_{\kk\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_t\alpha}\\
%&=\eta_{\kk1\cdots n\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_{t'}\alpha}
%\cap
%\eta_{\kk\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_t\alpha}\\
%&=\eta_{\kk1\cdots n}^{\BB_{u}^T}\left(\eta_{\kk^{-1}}^{\BB_{t'}^T}\set{\eta_{\kk n\cdots1\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_{t'}\alpha}\right)
%\cap
%\eta_{\kk}^{\BB_0^T}\left(\eta_{\kk^{-1}}^{\BB_t^T}\set{\eta_{\kk\kk^{-1}}^{\BB_t^T}(\lambda_0)+\tB_t\alpha}\right)\\
%\end{aligned}
%\end{multline*}



\section{An outline of a plan}
We have a characterization of the exchange matrices of \newword{neighboring seeds} (seeds that have $n-2$ $\g$-vectors in the imaginary wall.
I \emph{think} we know that this characterization is ``if and only if'', i.e.\ an exchange matrix in an affine exchange pattern has that form if and only if it belongs to a neighboring seed.
So we'll call these exchange matrices \newword{neighboring exchange matrices}.

\begin{enumerate}[\bf1.]
\item
To each neighboring exchange matrix, we need to associate an exchange matrix of rank $n-2$ and show that it is a product of finite type-C exchange matrices of the same sizes as the type-A blocks in the neighboring exchange matrix.
So we'll call it the \newword{type-C companion}.
\item
We need to show that the intersection of the nonnegative span of $B$ intersected with the hyperplane containing the imaginary wall is contained in the nonnegative span of $\pm$ the imaginary ray and the columns of the type-C companion.
\item
Call the indices of the last column/row of each $B_{ii}$ for $i=1,2,3$ (if they exist) the \newword{special indices} and call the two indices of $B_{44}$ the \newword{affine indices}.
(But really, we won't assume that we re-indexed everything to separate the blocks and put the affine indices last.
Even so, we can identify special and affine indices.)
We need to show that mutating $B$ in non-special indices commutes with mutating the type-C companion in the corresponding indices.
(We think of the type-C companion as being indexed by the non-affine indices of $B$.)
I think this should just be straightforward.
\item \label{not finally}
We then need to show that for each special index $i$, there is a sequence of mutations of $B$ in $i$ and the affine indices that yields a neighboring seed, and that the type-C companions of these two neighboring seeds are related by mutation at $i$.
We probably need to show this case-by-case in the sense of the table in affine\_dominance, but this seems actually doable and possibly not very long.  
(Exhibiting one sequence and checking will do.)
I think that this will change which of the three is special and which are affine.
\item \label{finally}
Finally, we need to check that the mutation maps for all of these mutations (in non-affine indices), as they act on the imaginary wall, are the same as the mutation maps for the type-C companion.
That is, the mutation maps for B send the imaginary ray to the new imaginary ray and act on the boundary of the imaginary wall just as the mutation maps for the type-C companion act on an $(n-2)$-dimensional plane.
Again, we should be able to do this by checking case-by-case in the table.
\end{enumerate}
If we have all that, then I think we're done.  
Because we would know that the type-C companion has mutation sequences that reduce its dominance regions to points, and the corresponding mutation sequences for $B$ would reduce dominance regions in the imaginary wall to the appropriate line segment.

Idea what the type-C companion might be:
Simply scale the special columns by $2$ and delete the affine columns and rows.
(Or maybe it should be the rows? Or maybe either would work?  Or maybe it depends on the type in the table?)
Maybe it can't be that simple, but I kinda think it has to be.

Somehow \cref{finally} is really just the same as \cref{not finally}.  
In either case, the point is to see what happens to a column that is not $i$ and not affine.

\subsection{This works for blocks $A_2^{(1)}$}
Starting with the block in the table of size $A_2^{(1)}$, consider these mutations:
\[
\begin{bsmallmatrix*}[r]
0&1&-1\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,1\\
1&0&1\\
-1&-1&0
\end{bsmallmatrix*}
\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&\,\,\,1&1\\
-1&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&2&-1\\
-2&0&1\\
1&-1&0
\end{bsmallmatrix*}.
\]
\sayD{We could use the mutations sequence $1,3,1,2,1$ like in the $C_2^{(1)}$ case to have the last two columns remain the affine entries.}
\[
\begin{bsmallmatrix*}[r]
0&1&-1\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,1\\
1&0&1\\
-1&-1&0
\end{bsmallmatrix*}
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,-1\\
1&0&-1\\
1&1&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&1&\,\,\,1\\
-1&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-1&\,\,\,1\\
1&0&1\\
-1&-1&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&1&\,\,\,-1\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}.
\]
We will apply the corresponding mutations to the matrix $B$ for the neighboring seed.
It suffices to consider only one of the blocks $B_{11}$, $B_{22}$ or $B_{33}$, without loss of generality $B_{33}$.
To compute the mutations, we take representative entries of $B$ as follows:
\begin{itemize}
\item $a_ij$ is an entry in $B_{33}$ with $i$ and $j$ \emph{non-special} or an entry (again, with $i$ nonspecial) in a column adjoined to $B$ in computing a mutation map $\eta^{B^T}$.
\item
$x_i$ is the entry in row $i$ (again non-special) of the special column of $B_{33}$.
\item
$y_j$ is the entry in column $j$ (again non-special) of the special row of $B_{33}$ or the special-index-entry in a column adjoined to $B$.
\end{itemize}
We use the notation 
\[m(a,b)=\begin{cases}
ab&\text{if }\sgn(a)=\sgn(b)=+\\
-ab&\text{if }\sgn(a)=\sgn(b)=-\\
0&\text{otherwise}.
\end{cases}\]
Writing $1$ for the special index in $B_{33}$ and $2$ and $3$ for the affine indices, the mutations proceed as follows.
We use the identity $m(a,b)+m(a,-b)=ab$ for $b>0$ several times.
\begin{align*}
\begin{bsmallmatrix*}[r]
a_{ij}&x_i&0&0\\
y_j&0&1&-1\\
0&-1&0&2\\
0&1&-2&0
\end{bsmallmatrix*}
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j)&-x_i&m(x_i,1)&m(x_i,-1)\\
-y_j&0&-1&1\\
m(y_j,-1)&1&0&1\\
m(y_j,1)&-1&-1&0
\end{bsmallmatrix*}\\
&\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j)&-x_i+m(x_i,1)&-m(x_i,1)&x_i\\
-y_j+m(y_j,-1)&0&\,\,\,1&1\\
-m(y_j,-1)&-1&0&-1\\
y_j&-1&1&0
\end{bsmallmatrix*}\\
&\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+2m(x_i,y_j)&0&0&-x_i\\
0&0&2&-1\\
0&-2&0&1\\
-y_j&1&-1&0
\end{bsmallmatrix*}.
\end{align*}
\begin{align*}
\begin{bsmallmatrix*}[r]
a_{ij}&x_i&0&0\\
y_j&0&1&-1\\
0&-1&0&2\\
0&1&-2&0
\end{bsmallmatrix*}
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j) & -x_i & m(x_i,1) & m(x_i,-1)\\
-y_j & 0 & -1 & 1\\
m(y_j,-1) & 1 & 0 & 1\\
m(y_j,1) & -1 & -1 & 0
\end{bsmallmatrix*}\\
&\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j) & -x_i+m(x_i,-1) & x_i & -m(x_i,-1)\\
-y_j+m(y_j,1) & 0 & -1 & -1\\
y_j & 1 & 0 & -1\\
-m(y_j,1) & 1 & 1 & 0
\end{bsmallmatrix*}\\
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j) & x_i-m(x_i,-1) & m(x_i,-1) & -x_i+m(x_i,-1)-m(x_i,-1)\\
y_j-m(y_j,1) & 0 & 1 & 1\\
m(y_j,1) & -1 & 0 & -1\\
-y_j+m(y_j,1)-m(y_j,1) & -1 & 1 & 0
\end{bsmallmatrix*}\\
&\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j) & x_i & -m(x_i,-1) & -x_i+m(x_i,-1)\\
y_j & 0 & -1 & 1\\
-m(y_j,1) & 1 & 0 & 1\\
-y_j+m(y_j,1) & -1 & -1 & 0
\end{bsmallmatrix*}\\
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+2m(x_i,y_j) & -x_i & 0 & 0\\
-y_j & 0 & 1 & -1\\
0 & -1 & 0 & 2\\
0 & 1 & -2 & 0
\end{bsmallmatrix*}\\
\end{align*}
where we observe in the middle mutation in direction 1 that $-x_i+m(x_i,-1)$ is always non-positive and $-y_j+m(y_j,1)$ is always non-negative.

The type-C companion of $B$ is obtained by deleting the affine rows and columns and scaling each special column by $2$.
OK, I actually don't know whether this is type-C, but Salvatore (and Tomoki) know, right?
There are probably weaker things we could use if wee don't have this.

Each non-affine column of $B$ agrees with the corresponding column of the type-C companion except that (for the special columns) there are multiples of the direction of the imaginary ray.
In particular, these non-affine columns of $B$ are already in the hyperplane containing the imaginary wall.  Furthermore, while the affine columns are not in that hyperplane, there is positive linear combination of them that is (either their sum or a two-to-one ratio), and this positive linear combination is in the direction of the imaginary ray.

If we mutate the type-C companion in the special index, we have
\[
\begin{bsmallmatrix*}
a_{ij}&2x_i\\
y_j&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(2x_i,y_j)&-2x_i\\
-y_j&0
\end{bsmallmatrix*}
\]

That works!  

If we mutate in a non-special index in $B_{33}$, we don't have to worry about anything outside $B_{33}$, and also, this mutation certainly commutes with scaling the special column by $2$.

Seems like everything works in this case.
The other cases should be similar.

\subsection{This works for blocks $C_2^{(1)}$}

Since $\mu_k(DBD^{-1})=D\mu_k(B)D^{-1}$ for any positive diagonal matrix $D$, it suffices to only consider the cases $C_2^{(1)}$, which implies both $A_4^{(2)}$ cases and $D_3^{(2)}$, and $G_2^{(1)}$, which implies $D_4^{(3)}$.
(Let $B'=DBD^{-1}$, then $E'=DED^{-1}$ and $F'=DFD^{-1}$ so that $D(EBF)D^{-1}=E'B'F'$.)

Starting with the block in the table of size $C_2^{(1)}$, consider these mutations:
\[
\begin{bsmallmatrix*}[r]
0&2&-2\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-2&\,\,\,2\\
1&0&0\\
-1&0&0
\end{bsmallmatrix*}
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-2&\,\,\,-2\\
1&0&0\\
1&0&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&2&\,\,\,2\\
-1&0&0\\
-1&0&0
\end{bsmallmatrix*}
\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-2&\,\,\,2\\
1&0&0\\
-1&0&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&2&-2\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}.
\]

Then
\begin{align*}
\begin{bsmallmatrix*}[r]
a_{ij}&x_i&0&0\\
y_j&0&2&-2\\
0&-1&0&2\\
0&1&-2&0
\end{bsmallmatrix*}
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j)&-x_i&m(x_i,2)&m(x_i,-2)\\
-y_j&0&-2&2\\
m(y_j,-1)&1&0&0\\
m(y_j,1)&-1&0&0
\end{bsmallmatrix*}\\
&\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j)&-x_i+m(x_i,-2)&m(x_i,2)&-m(x_i,-2)\\
-y_j+2m(y_j,1)&0&-2&-2\\
m(y_j,-1)&1&0&0\\
-m(y_j,1)&1&0&0
\end{bsmallmatrix*}\\
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j)&x_i-m(x_i,-2)&m(x_i,-2)&-2x_i+m(x_i,-2)\\
y_j-2m(y_j,1)&0&2&2\\
m(y_j,1)&-1&0&0\\
-y_j+m(y_j,1)&-1&0&0
\end{bsmallmatrix*}\\
&\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j)&x_i&-m(x_i,-2)&-2x_i+m(x_i,-2)\\
y_j&0&-2&2\\
-m(y_j,1)&1&0&0\\
-y_j+m(y_j,1)&-1&0&0
\end{bsmallmatrix*}\\
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+2m(x_i,y_j)&-x_i&0&0\\
-y_j&0&2&-2\\
0&-1&0&2\\
0&1&-2&0
\end{bsmallmatrix*}
\end{align*}
where we observe in the middle mutation in direction 1 that $-x_i+m(x_i,-2)$ is always non-positive and $-y_j+2m(y_j,1)$ is always non-negative.

\subsection{This works for blocks $G_2^{(1)}$}

Starting with the block in the table of size $G_2^{(1)}$, consider these mutations:
\[
\begin{bsmallmatrix*}[r]
0&3&-3\\
-1&0&2\\
1&-2&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-3&3\\
1&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&-3\\
0&0&1\\
1&-1&0
\end{bsmallmatrix*}
\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&-3\\
0&0&-1\\
1&1&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&0&3\\
0&0&-1\\
-1&1&0
\end{bsmallmatrix*}
\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&3&-3\\
-1&0&1\\
1&-1&0
\end{bsmallmatrix*}
\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}[r]
0&-3&3\\
1&0&-2\\
-1&2&0
\end{bsmallmatrix*}.
\]

Then
\begin{align*}
\begin{bsmallmatrix*}[r]
a_{ij}&x_i&0&0\\
y_j&0&3&-3\\
0&-1&0&2\\
0&1&-2&0
\end{bsmallmatrix*}
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j) & -x_i & m(x_i,3) & m(x_i,-3)\\
-y_j & 0 & -3 & 3\\
m(y_j,-1) & 1 & 0 & -1\\
m(y_j,1) & -1 & 1 & 0
\end{bsmallmatrix*}\\
&\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j) & -x_i+m(x_i,-3) & m(x_i,3) & -m(x_i,-3)\\
-y_j+3m(y_j,1) & 0 & 0 & -3\\
m(y_j,-1) & 0 & 0 & 1\\
-m(y_j,1) & 1 & -1 & 0
\end{bsmallmatrix*}\\
&\overset{2}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j) & -x_i+m(x_i,-3) & -m(x_i,3) & m(x_i,3)-m(x_i,-3)\\
-y_j+3m(y_j,1) & 0 & 0 & -3\\
-m(y_j,-1) & 0 & 0 & -1\\
m(y_j,-1)-m(y_j,1) & 1 & 1 & 0
\end{bsmallmatrix*}\\
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j) & x_i-m(x_i,-3) & -m(x_i,3) & m(x_i,-3)\\
y_j-3m(y_j,1) & 0 & 0 & 3\\
-m(y_j,-1) & 0 & 0 & -1\\
m(y_j,1) & -1 & 1 & 0
\end{bsmallmatrix*}\\
&\overset{3}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+m(x_i,y_j) & x_i & -m(x_i,3) & -m(x_i,-3)\\
y_j & 0 & 3 & -3\\
-m(y_j,-1) & -1 & 0 & 1\\
-m(y_j,1) & 1 & -1 & 0
\end{bsmallmatrix*}\\
&\overset{1}{\longrightarrow}
\begin{bsmallmatrix*}
a_{ij}+2m(x_i,y_j) & -x_i & 0 & 0\\
-y_j & 0 & -3 & 3\\
0 & 1 & 0 & -2\\
0 & -1 & 2 & 0
\end{bsmallmatrix*}
\end{align*}
where we observe in the middle mutation in direction 1 that $-x_i+m(x_i,-3)$ is always non-positive and $-y_j+3m(y_j,1)$ is always non-negative.
 


% bibliography
\bibliographystyle{plain}
\bibliography{bibliography}
\vspace{-0.175 em}


\end{document}

